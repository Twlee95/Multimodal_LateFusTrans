{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import import_ipynb\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "def train(Transformer, Transformer_optimizer,args,partition):\n",
    "    train_loader = DataLoader(partition[\"train\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Transformer.train()\n",
    "    train_loss = 0.0\n",
    "    for (x,y) in train_loader:\n",
    "        Transformer.zero_grad()\n",
    "        Transformer_optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(args.device) # 64,10 40\n",
    "        y = y.float().squeeze().to(args.device)\n",
    "\n",
    "        Transf_out,AP1,AP2,AP3  = Transformer(x[:,:,0:17],x[:,:,17:34],x[:,:,34:40])\n",
    "\n",
    "        loss = args.loss_fn(Transf_out, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        Transformer_optimizer.step() ## parameter 갱신\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    return Transformer, train_loss\n",
    "\n",
    "def validation(Transformer, args, partition):\n",
    "    val_loader = DataLoader(partition[\"val\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "\n",
    "    Transformer.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in val_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Transf_out, AP1,AP2,AP3 = Transformer(x[:,:,0:17],x[:,:,17:34],x[:,:,34:40])\n",
    "                        \n",
    "            loss = args.loss_fn(Transf_out, y)      \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    return Transformer, val_loss\n",
    "\n",
    "def test(Transformer, args, partition):\n",
    "    test_loader = DataLoader(partition[\"test\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Transformer.eval()\n",
    "    ACC_metric = 0.0\n",
    "    AP1_list= []\n",
    "    AP2_list= []\n",
    "    AP3_list= []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in test_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Transf_out, AP1,AP2,AP3 = Transformer(x[:,:,0:17],x[:,:,17:34],x[:,:,34:40])\n",
    "            \n",
    "            AP1_list.append(AP1)\n",
    "            AP2_list.append(AP2)\n",
    "            AP3_list.append(AP3)\n",
    "            \n",
    "            output_ = torch.where(Transf_out >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            perc_y_pred = output_.cpu().detach().numpy()     \n",
    "            perc_y_true =  y.cpu().detach().numpy()\n",
    "            acc = accuracy_score(perc_y_true, perc_y_pred)\n",
    "\n",
    "            ACC_metric += acc\n",
    "\n",
    "    ACC_metric = ACC_metric / len(test_loader)\n",
    "     \n",
    "    return ACC_metric, AP1_list, AP2_list, AP3_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== Argument initializtion ======#\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "#=============== Device ===============#\n",
    "args.device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#================ Path ================#\n",
    "\n",
    "args.save_file_path = \"D:\\\\MM_late_results_map\"\n",
    "\n",
    "#========= Base Hyperparameter =========#\n",
    "args.batch_size = 32\n",
    "args.lr = 0.00005\n",
    "args.L2 = 0.00001\n",
    "args.epoch = 100\n",
    "args.dropout = 0.15\n",
    "args.loss_fn = nn.BCELoss()\n",
    "\n",
    "#===== Transformer Hyperparameter =====#\n",
    "from Transformer_Encoder import Transformer\n",
    "\n",
    "args.Transformer = Transformer\n",
    "args.m1_input_feature_size = 17\n",
    "args.m2_input_feature_size = 17\n",
    "args.m3_input_feature_size = 6\n",
    "\n",
    "args.Transformer_feature_size = 32\n",
    "\n",
    "args.nhead = 6\n",
    "\n",
    "args.nlayer = 1\n",
    "args.ts_len = 10\n",
    "args.target_len = 1\n",
    "# trans_feature_size / trans_nhead => int 필수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "Epoch 0, Loss(train/val) 0.68956/0.69289. Took 0.74 sec\n",
      "Epoch 1, Loss(train/val) 0.68897/0.69277. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.68873/0.69268. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68870/0.69260. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.68823/0.69256. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68821/0.69253. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68803/0.69252. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68795/0.69250. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68780/0.69251. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68761/0.69252. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68727/0.69254. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68734/0.69256. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68745/0.69259. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68741/0.69264. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68715/0.69268. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68741/0.69273. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68676/0.69280. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68685/0.69286. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68698/0.69291. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68691/0.69297. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68680/0.69303. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68675/0.69308. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68644/0.69314. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68655/0.69321. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68644/0.69328. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68612/0.69335. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68647/0.69342. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68643/0.69348. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.68595/0.69355. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68589/0.69362. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68593/0.69367. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68601/0.69374. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68593/0.69381. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68587/0.69388. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68576/0.69394. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68565/0.69401. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68569/0.69407. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68559/0.69414. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68573/0.69420. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68543/0.69427. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68525/0.69435. Took 0.22 sec\n",
      "Epoch 41, Loss(train/val) 0.68515/0.69442. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68497/0.69449. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68506/0.69453. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 0.68518/0.69461. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68485/0.69468. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.68463/0.69476. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68436/0.69482. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 0.68447/0.69486. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.68447/0.69493. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.68412/0.69503. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.68386/0.69510. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.68409/0.69518. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.68403/0.69528. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.68357/0.69538. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.68362/0.69546. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.68309/0.69552. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.68323/0.69562. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.68297/0.69568. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.68303/0.69576. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.68285/0.69584. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.68312/0.69592. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.68270/0.69599. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.68269/0.69608. Took 0.22 sec\n",
      "Epoch 64, Loss(train/val) 0.68254/0.69615. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.68225/0.69622. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.68208/0.69629. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.68200/0.69636. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.68171/0.69641. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.68209/0.69648. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.68116/0.69652. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.68143/0.69659. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 0.68133/0.69667. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.68103/0.69672. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.68091/0.69677. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.68078/0.69686. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.68043/0.69695. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.68007/0.69703. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.68009/0.69709. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.67980/0.69718. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.67950/0.69728. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.67944/0.69740. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.67947/0.69746. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.67916/0.69755. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.67932/0.69765. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.67884/0.69773. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.67885/0.69783. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.67798/0.69789. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.67846/0.69798. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.67819/0.69809. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.67795/0.69819. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.67760/0.69829. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.67747/0.69839. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.67761/0.69850. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.67672/0.69864. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.67742/0.69876. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.67684/0.69890. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.67655/0.69902. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.67626/0.69913. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.67611/0.69923. Took 0.20 sec\n",
      "ACC: 0.5625\n"
     ]
    }
   ],
   "source": [
    "## 실행파일\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from Stock_dataloader_csv_ti import stock_csv_read\n",
    "from Stock_Dataset import StockDataset\n",
    "\n",
    "\n",
    "args.data_list = os.listdir(r\"C:\\Users\\lab\\Desktop\\Multimodal_Transformer_map\\data\\kdd17\\price_long_50_\")\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'Multimodal_transformer_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        est = time.time()\n",
    "\n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"Multimodal_transformer_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        csv_read = stock_csv_read(data, args.ts_len,args.target_len)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "\n",
    "        with open(args.new_file_path + '\\\\'+ str(args.symbol)+'test_acc_list' +'.csv', 'w',newline='') as alist:\n",
    "            www = csv.writer(alist)\n",
    "            www.writerow([\"acc_list\"])\n",
    "        \n",
    "            ACC_cv = []\n",
    "            for i, data in enumerate(split_data_list):\n",
    "                \n",
    "                args.sp_ith = i\n",
    "\n",
    "                args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "                os.makedirs(args.split_file_path)\n",
    "                ## Model\n",
    "                Transformer = args.Transformer(args.m1_input_feature_size,args.m2_input_feature_size,args.m3_input_feature_size,\n",
    "                                            args.Transformer_feature_size, args.nhead, args,\n",
    "                                            args.nlayer, args.dropout, args.ts_len)\n",
    "                Transformer.to(args.device)\n",
    "\n",
    "                ##Optimizer\n",
    "                Transformer_optimizer = optim.Adam(Transformer.parameters(), lr=args.lr, weight_decay=args.L2)\n",
    "\n",
    "                ## training\n",
    "                Train_losses = []\n",
    "                Validation_losses = []\n",
    "                for epoch in range(args.epoch):\n",
    "                    ts = time.time()\n",
    "\n",
    "                    trainset = StockDataset(data[0])\n",
    "                    valset = StockDataset(data[1])\n",
    "                    testset = StockDataset(data[2])\n",
    "\n",
    "\n",
    "                    partition = {'train': trainset, 'val': valset, 'test': testset}     \n",
    "\n",
    "                    Transformer, train_loss = train(Transformer,Transformer_optimizer, args, partition)\n",
    "                    Transformer, validation_loss = validation(Transformer, args, partition)\n",
    "\n",
    "\n",
    "                    ## .state_dict() : model의 parameter(W)만을 저장하는것임 => 다시 불러올 때 모델의 파라미터를 알고있어야함\n",
    "                    if len(Validation_losses) == 0:\n",
    "                        torch.save(Transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Transformer' +'.pt')\n",
    "                    elif min(Validation_losses) > validation_loss:\n",
    "                        torch.save(Transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Transformer' +'.pt')\n",
    "                    \n",
    "                    Train_losses.append(train_loss)\n",
    "                    Validation_losses.append(validation_loss)\n",
    "                    \n",
    "                    te = time.time()\n",
    "\n",
    "                    print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "                    .format(epoch, train_loss, validation_loss, te - ts))\n",
    "\n",
    "                ## Test\n",
    "                # state_dict로 저장했기 때문에 model의 hyperparameter를 불러와야함\n",
    "                Transformer = args.Transformer(args.m1_input_feature_size,args.m2_input_feature_size,args.m3_input_feature_size,\n",
    "                                            args.Transformer_feature_size, args.nhead, args,\n",
    "                                            args.nlayer, args.dropout,args.ts_len)\n",
    "                Transformer.to(args.device)\n",
    "\n",
    "                # Model_selection\n",
    "                min_val_losses = Validation_losses.index(min(Validation_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "\n",
    "                Transformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(min_val_losses) +'_transformer' + '.pt'))\n",
    "\n",
    "                ACC, AP1_list, AP2_list, AP3_list = test(Transformer, args, partition)\n",
    "                print('ACC: {}'.format(ACC))\n",
    "                www.writerow([ACC])\n",
    "\n",
    "                args.attention_map_path1 = args.split_file_path  +'\\\\attention_map1'\n",
    "                args.attention_map_path2 = args.split_file_path  +'\\\\attention_map2'\n",
    "                args.attention_map_path3 = args.split_file_path  +'\\\\attention_map3'\n",
    "                os.makedirs(args.attention_map_path1)\n",
    "                os.makedirs(args.attention_map_path2)\n",
    "                os.makedirs(args.attention_map_path3)\n",
    "\n",
    "\n",
    "                j=0\n",
    "                for b_att_map1, b_att_map2, b_att_map3 in zip(AP1_list, AP2_list, AP3_list):\n",
    "                    for h_att_map1, h_att_map2, h_att_map3 in zip(b_att_map1, b_att_map2, b_att_map3):\n",
    "                        for att_map1, att_map2, att_map3  in zip(h_att_map1, h_att_map2, h_att_map3):\n",
    "                            j += 1\n",
    "\n",
    "                            att_map1 = att_map1.cpu().detach().numpy()\n",
    "                            att_map2 = att_map2.cpu().detach().numpy()\n",
    "                            att_map3 = att_map3.cpu().detach().numpy()\n",
    "            \n",
    "                            scaler1  = preprocessing.MinMaxScaler().fit(att_map1)\n",
    "                            scaler2  = preprocessing.MinMaxScaler().fit(att_map2)\n",
    "                            scaler3  = preprocessing.MinMaxScaler().fit(att_map3)\n",
    "\n",
    "                            att_map1 = scaler1.transform(att_map1)\n",
    "                            att_map2 = scaler2.transform(att_map2)\n",
    "                            att_map3 = scaler3.transform(att_map3)\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            sns.heatmap(att_map1, cmap='crest')\n",
    "                            plt.savefig(args.attention_map_path1 + '\\\\' + str(j)+ '_attmap' + '.png', dpi = 300)\n",
    "                            plt.clf()\n",
    "                            sns.heatmap(att_map2, cmap='crest')\n",
    "                            plt.savefig(args.attention_map_path2 + '\\\\' + str(j)+ '_attmap' + '.png', dpi = 300)\n",
    "                            plt.clf()\n",
    "                            sns.heatmap(att_map3, cmap='crest')\n",
    "                            plt.savefig(args.attention_map_path3 + '\\\\' + str(j)+ '_attmap' + '.png', dpi = 300)\n",
    "                            plt.clf()\n",
    "\n",
    "                with open(args.split_file_path + '\\\\'+ str(min_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "                    print('ACC: {}'.format(ACC), file=fd)\n",
    "\n",
    "                result = {}\n",
    "\n",
    "                result['train_losses'] = Train_losses\n",
    "                result['val_losses'] = Validation_losses\n",
    "                result['ACC'] = ACC\n",
    "\n",
    "                eet = time.time()\n",
    "                entire_exp_time = eet - est\n",
    "\n",
    "                ## draw loss curve\n",
    "                fig = plt.figure()\n",
    "                plt.plot(result['train_losses'])\n",
    "                plt.plot(result['val_losses'])\n",
    "                plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "                plt.xlabel('epoch', fontsize=15)\n",
    "                plt.ylabel('loss', fontsize=15)\n",
    "                plt.grid()\n",
    "                plt.savefig(args.split_file_path + '\\\\' + 'fig' + '.png')\n",
    "                plt.close(fig)\n",
    "                ACC_cv.append(result['ACC'])\n",
    "\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"MM_Transformer\", args.symbol, entire_exp_time, acc_avg, acc_std])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
