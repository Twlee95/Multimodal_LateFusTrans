{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('C:\\\\taewon_project\\\\Multimodal_Transformer')\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(Transformer, Transformer_optimizer,args,partition):\n",
    "    train_loader = DataLoader(partition[\"train\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Transformer.train()\n",
    "    train_loss = 0.0\n",
    "    for (x,y) in train_loader:\n",
    "        Transformer.zero_grad()\n",
    "        Transformer_optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(args.device) # 64,10 40\n",
    "        y = y.float().squeeze().to(args.device)\n",
    "\n",
    "        Transf_out = Transformer(x[:,:,0:17],x[:,:,17:34],x[:,:,34:40])\n",
    "\n",
    "        loss = args.loss_fn(Transf_out, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        Transformer_optimizer.step() ## parameter 갱신\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    return Transformer, train_loss\n",
    "\n",
    "def validation(Transformer, args, partition):\n",
    "    val_loader = DataLoader(partition[\"val\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "\n",
    "    Transformer.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in val_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Transf_out = Transformer(x[:,:,0:17],x[:,:,17:34],x[:,:,34:40])\n",
    "                        \n",
    "            loss = args.loss_fn(Transf_out, y)      \n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    return Transformer, val_loss\n",
    "\n",
    "def test(Transformer, args, partition):\n",
    "    test_loader = DataLoader(partition[\"test\"],batch_size= args.batch_size,shuffle=False,drop_last=True)\n",
    "    \n",
    "    Transformer.eval()\n",
    "    ACC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in test_loader:\n",
    "\n",
    "            x = x.float().to(args.device) # 64,10 40\n",
    "            y = y.float().squeeze().to(args.device)\n",
    "\n",
    "            Transf_out = Transformer(x[:,:,0:17],x[:,:,17:34],x[:,:,34:40])\n",
    "            \n",
    "\n",
    "            output_ = torch.where(Transf_out >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            perc_y_pred = output_.cpu().detach().numpy()     \n",
    "            perc_y_true =  y.cpu().detach().numpy()\n",
    "            acc = accuracy_score(perc_y_true, perc_y_pred)\n",
    "\n",
    "            ACC_metric += acc\n",
    "\n",
    "    ACC_metric = ACC_metric / len(test_loader)\n",
    "     \n",
    "    return ACC_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== Argument initializtion ======#\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "#=============== Device ===============#\n",
    "args.device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#================ Path ================#\n",
    "\n",
    "args.save_file_path = \"D:\\\\MM_late_results\"\n",
    "\n",
    "#========= Base Hyperparameter =========#\n",
    "args.batch_size = 32\n",
    "args.lr = 0.00005\n",
    "args.L2 = 0.00001\n",
    "args.epoch = 100\n",
    "args.dropout = 0.15\n",
    "args.loss_fn = nn.BCELoss()\n",
    "\n",
    "#===== Transformer Hyperparameter =====#\n",
    "from Transformer_Encoder import Transformer\n",
    "\n",
    "args.Transformer = Transformer\n",
    "args.m1_input_feature_size = 17\n",
    "args.m2_input_feature_size = 17\n",
    "args.m3_input_feature_size = 6\n",
    "\n",
    "args.Transformer_feature_size = 32\n",
    "\n",
    "args.nhead = 4\n",
    "\n",
    "args.nlayer = 1\n",
    "args.ts_len = 10\n",
    "args.target_len = 1\n",
    "# trans_feature_size / trans_nhead => int 필수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "Epoch 0, Loss(train/val) 0.68983/0.70188. Took 3.00 sec\n",
      "Epoch 1, Loss(train/val) 0.68754/0.70098. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.68670/0.69990. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.68630/0.69925. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.68530/0.69867. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68490/0.69835. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68506/0.69801. Took 0.22 sec\n",
      "Epoch 7, Loss(train/val) 0.68471/0.69769. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68430/0.69759. Took 0.22 sec\n",
      "Epoch 9, Loss(train/val) 0.68419/0.69737. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68359/0.69731. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68388/0.69723. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.68298/0.69740. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68301/0.69724. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.68194/0.69720. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68198/0.69746. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68114/0.69751. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68094/0.69736. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.67905/0.69780. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.67983/0.69797. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.67993/0.69788. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67894/0.69801. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 0.67854/0.69816. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.67785/0.69826. Took 0.22 sec\n",
      "Epoch 24, Loss(train/val) 0.67611/0.69869. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.67766/0.69898. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 0.67731/0.69908. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67525/0.69936. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.67429/0.69968. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.67498/0.69991. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.67275/0.70035. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67248/0.70082. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67157/0.70117. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67071/0.70137. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67012/0.70198. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66793/0.70269. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.66728/0.70355. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.66828/0.70332. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.66582/0.70367. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66444/0.70480. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.66362/0.70545. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66234/0.70736. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.66224/0.70784. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66267/0.70797. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.65992/0.70987. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.65565/0.71082. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.65755/0.71209. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65552/0.71219. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.65424/0.71499. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.65235/0.71657. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 0.65177/0.71779. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.65068/0.71986. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.65137/0.72067. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.65017/0.72153. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.64776/0.72110. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.64590/0.72731. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.64543/0.72591. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64375/0.72899. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64130/0.73183. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.64263/0.73303. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.64200/0.73432. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.63812/0.73319. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.64149/0.73757. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.63543/0.74073. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63680/0.74393. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.63319/0.74544. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63295/0.74534. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.63037/0.74885. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63112/0.74790. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.62959/0.74820. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.62659/0.75145. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.62496/0.75337. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62600/0.75813. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62743/0.76162. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62461/0.76090. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.61975/0.76138. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62244/0.76096. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.61668/0.76534. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.61885/0.77180. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.62003/0.76781. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.61446/0.77197. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.61560/0.77125. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61021/0.77584. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.60717/0.78124. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.61253/0.77955. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.60956/0.78315. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.60909/0.78288. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.60984/0.78472. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.60674/0.78825. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.60970/0.78815. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60551/0.78679. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.60027/0.79311. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.60532/0.79530. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.60500/0.79631. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.60029/0.79644. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.60111/0.79893. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59839/0.79964. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.59244/0.79835. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.59672/0.80342. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.59345/0.81427. Took 0.19 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69136/0.68794. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68955/0.68734. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.68863/0.68708. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.68862/0.68694. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.68879/0.68686. Took 0.22 sec\n",
      "Epoch 5, Loss(train/val) 0.68751/0.68674. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68731/0.68663. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68740/0.68661. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68731/0.68674. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68673/0.68687. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68545/0.68704. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68591/0.68713. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68576/0.68712. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68536/0.68728. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68580/0.68745. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68507/0.68765. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68418/0.68775. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68368/0.68780. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68420/0.68788. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68330/0.68816. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68380/0.68814. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68332/0.68840. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68308/0.68854. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68160/0.68854. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68116/0.68870. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68081/0.68864. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68265/0.68883. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68160/0.68914. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68179/0.68918. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68057/0.68944. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68037/0.68967. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68063/0.68983. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67969/0.69031. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67878/0.69053. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67846/0.69070. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67802/0.69120. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67861/0.69128. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67859/0.69098. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67717/0.69197. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67591/0.69244. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67601/0.69232. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67522/0.69228. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67367/0.69294. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67149/0.69332. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67440/0.69320. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67498/0.69426. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67103/0.69530. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67104/0.69564. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67111/0.69639. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67032/0.69660. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67062/0.69654. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66968/0.69746. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66770/0.69750. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.66984/0.69832. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66724/0.69872. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66524/0.69913. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66609/0.69995. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66545/0.70028. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66226/0.70109. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66494/0.70158. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66154/0.70202. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66121/0.70323. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66065/0.70333. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66083/0.70424. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66081/0.70520. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65678/0.70441. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65565/0.70635. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65905/0.70591. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65410/0.70641. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65572/0.70732. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65376/0.70681. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65191/0.70848. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65155/0.71075. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65097/0.71265. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65184/0.71095. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64847/0.70914. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.64979/0.71311. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64523/0.71403. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64589/0.71463. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64591/0.71665. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64813/0.71540. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64532/0.71564. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64048/0.71584. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64221/0.71261. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.63886/0.71628. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63661/0.71623. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63653/0.71588. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63872/0.71634. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63543/0.71607. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63509/0.72062. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63288/0.72309. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63201/0.72576. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63491/0.72706. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63224/0.72656. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62871/0.72666. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62563/0.72616. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62784/0.72891. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62769/0.72781. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62453/0.73035. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62662/0.73049. Took 0.20 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69172/0.69301. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68876/0.69313. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68788/0.69330. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68736/0.69326. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68737/0.69301. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68795/0.69266. Took 0.22 sec\n",
      "Epoch 6, Loss(train/val) 0.68711/0.69240. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68729/0.69203. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68731/0.69166. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68637/0.69126. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68710/0.69095. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68644/0.69059. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.68624/0.69002. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68584/0.68974. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68623/0.68919. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68598/0.68877. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68533/0.68839. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68546/0.68784. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68438/0.68735. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68429/0.68681. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68359/0.68649. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68344/0.68576. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68357/0.68558. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68219/0.68502. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68164/0.68453. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68210/0.68451. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68188/0.68368. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68272/0.68298. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68104/0.68232. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68135/0.68209. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67927/0.68168. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67846/0.68085. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67764/0.68041. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.67808/0.67996. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67580/0.67907. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67698/0.67912. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67436/0.67732. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.67397/0.67712. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67231/0.67706. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67196/0.67662. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67028/0.67709. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.66979/0.67505. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.66753/0.67395. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.66674/0.67514. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66607/0.67363. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66432/0.67338. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.66402/0.67119. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 0.65961/0.67162. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66043/0.67045. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.65734/0.67085. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65554/0.66917. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.65437/0.67029. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65113/0.66779. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.64996/0.66968. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64796/0.66797. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64904/0.66765. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.64468/0.66716. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.64184/0.66777. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.63992/0.66886. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.63865/0.66818. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.63901/0.66613. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.63660/0.66641. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.63575/0.66475. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.63477/0.66285. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.63252/0.66399. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.63277/0.66297. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.62976/0.66051. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.62905/0.65905. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.62002/0.66182. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.62111/0.66003. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.62108/0.66320. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62407/0.66221. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62263/0.65787. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.62057/0.66201. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.61662/0.66174. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.61641/0.66600. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.61885/0.66462. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.60934/0.66353. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.60828/0.66411. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.60711/0.66527. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.60779/0.66120. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.60954/0.66490. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 0.60445/0.66348. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.60420/0.66210. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.60029/0.66295. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.60035/0.66987. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.59836/0.66837. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.59941/0.66597. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.59733/0.66770. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.59210/0.66185. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.59347/0.66215. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.59075/0.66426. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.59009/0.66224. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.59085/0.66511. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.59136/0.66323. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.58708/0.66042. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.58647/0.66349. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.58485/0.65841. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.58804/0.65929. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.58440/0.66199. Took 0.20 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69299/0.70417. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68995/0.71202. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68885/0.71295. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68849/0.71304. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68840/0.71301. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68801/0.71365. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68822/0.71361. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68789/0.71408. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68744/0.71442. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68690/0.71538. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68698/0.71527. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68703/0.71541. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68636/0.71612. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68701/0.71613. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68662/0.71630. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68704/0.71692. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68606/0.71683. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68615/0.71641. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68601/0.71753. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68591/0.71752. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68553/0.71863. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68477/0.71879. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68523/0.71935. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68567/0.71927. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68410/0.71997. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68442/0.72058. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68349/0.72070. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68314/0.72188. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68249/0.72266. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68183/0.72382. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68213/0.72462. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68053/0.72506. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68191/0.72750. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68066/0.72742. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67965/0.72839. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68014/0.72780. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67874/0.72876. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67896/0.73024. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67809/0.73092. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67887/0.73264. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67853/0.73252. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67763/0.73354. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67777/0.73604. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67674/0.73677. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67536/0.73801. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67672/0.73801. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67561/0.74003. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67637/0.74151. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67492/0.74427. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67251/0.74334. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67311/0.74529. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67112/0.74437. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67316/0.74763. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67317/0.74853. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67189/0.74913. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66968/0.74963. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67122/0.75341. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66889/0.75408. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66846/0.75559. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67166/0.75778. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66897/0.75802. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66836/0.75906. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66734/0.76109. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66599/0.76177. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66510/0.76552. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66687/0.76575. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66609/0.76524. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66462/0.76770. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66282/0.77022. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66373/0.77221. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66395/0.77163. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66161/0.77169. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66221/0.77527. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66116/0.77622. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66247/0.77522. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65746/0.77868. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65843/0.78304. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65770/0.78482. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65765/0.78449. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65823/0.78536. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65522/0.78807. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65476/0.78923. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65442/0.79029. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65258/0.79306. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65280/0.79653. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65300/0.79817. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64996/0.79762. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64533/0.79921. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64420/0.80410. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64676/0.80319. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64657/0.80499. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64334/0.80801. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64553/0.80949. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64096/0.81078. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64486/0.81289. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64260/0.81129. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64002/0.81641. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63590/0.81776. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63744/0.81901. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63478/0.82357. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69511/0.69657. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69095/0.69537. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69066/0.69447. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69037/0.69384. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69014/0.69313. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69010/0.69268. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68928/0.69228. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68873/0.69162. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68886/0.69130. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68875/0.69093. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68870/0.69046. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68803/0.69004. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68792/0.68961. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68685/0.68916. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.68746/0.68863. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68714/0.68842. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.68646/0.68812. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68681/0.68782. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.68603/0.68747. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68581/0.68719. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.68547/0.68683. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68503/0.68639. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68523/0.68624. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68415/0.68580. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68485/0.68579. Took 0.23 sec\n",
      "Epoch 25, Loss(train/val) 0.68383/0.68523. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 0.68259/0.68486. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68241/0.68477. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68164/0.68446. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68172/0.68402. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68053/0.68390. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 0.68048/0.68338. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.67907/0.68325. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67964/0.68269. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67812/0.68255. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67872/0.68232. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67801/0.68220. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67587/0.68207. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67498/0.68167. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67573/0.68116. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.67365/0.68111. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.67428/0.68084. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.67314/0.68019. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67115/0.67936. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67038/0.67838. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67037/0.67882. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.66900/0.67865. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66834/0.67793. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66743/0.67761. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66632/0.67750. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 0.66395/0.67736. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66399/0.67656. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.66282/0.67552. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66118/0.67520. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66139/0.67517. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65893/0.67537. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.65832/0.67388. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.65511/0.67365. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 0.65556/0.67388. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65361/0.67462. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.65558/0.67380. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65075/0.67316. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.65015/0.67241. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.64965/0.67228. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.64921/0.67073. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65056/0.67153. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.64768/0.67169. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64538/0.67197. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.64274/0.67063. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.64560/0.67160. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.64331/0.66974. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.63871/0.66953. Took 0.22 sec\n",
      "Epoch 72, Loss(train/val) 0.63937/0.67140. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.63794/0.67058. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.63494/0.66985. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63826/0.67067. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.63531/0.67046. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63203/0.67259. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.63495/0.67103. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63341/0.67139. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.62808/0.67233. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.62586/0.67409. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.62647/0.67572. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62616/0.67323. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.62740/0.67111. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62478/0.67522. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.62272/0.67385. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62004/0.67481. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.62291/0.67664. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61786/0.67756. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.61894/0.67524. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.61666/0.67700. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.61726/0.67756. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61602/0.67464. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62008/0.67804. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.61730/0.67645. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.61241/0.68278. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.60735/0.68670. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 0.60910/0.68167. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60945/0.68452. Took 0.20 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69189/0.69143. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69188/0.69144. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69088/0.69137. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69101/0.69113. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69095/0.69094. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69004/0.69066. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68989/0.69027. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69005/0.68994. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68916/0.68952. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68923/0.68916. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68920/0.68868. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68787/0.68811. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68829/0.68751. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68828/0.68690. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68701/0.68627. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68653/0.68554. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68724/0.68475. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68595/0.68378. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68422/0.68265. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68540/0.68182. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68484/0.68085. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68455/0.68002. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68368/0.67894. Took 0.22 sec\n",
      "Epoch 23, Loss(train/val) 0.68237/0.67784. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.68012/0.67655. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68049/0.67513. Took 0.22 sec\n",
      "Epoch 26, Loss(train/val) 0.68086/0.67409. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68212/0.67371. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.67904/0.67268. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67856/0.67174. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.67664/0.67106. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67752/0.67010. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.67761/0.67008. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67801/0.66940. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 0.67711/0.66879. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67583/0.66786. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.67680/0.66767. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67433/0.66804. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67133/0.66719. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67197/0.66731. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67060/0.66734. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67272/0.66689. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.67181/0.66645. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67159/0.66711. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.66968/0.66798. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67197/0.66788. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.66894/0.66704. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66941/0.66721. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66894/0.66657. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66981/0.66699. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.66633/0.66652. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66616/0.66560. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.66596/0.66712. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66647/0.66645. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66647/0.66640. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66717/0.66910. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66605/0.66911. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66637/0.66958. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66244/0.66941. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65975/0.66879. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66296/0.67034. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66468/0.67121. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66265/0.67005. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66322/0.67048. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66226/0.67126. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66134/0.67089. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66005/0.67288. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65750/0.67378. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65822/0.67538. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65974/0.67294. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65891/0.67400. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65717/0.67268. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65321/0.67373. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.65612/0.67502. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65772/0.67655. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65348/0.67951. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65661/0.67782. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65047/0.67920. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65286/0.67823. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65253/0.68053. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 0.65475/0.68124. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.65411/0.67943. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65377/0.68022. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65128/0.68119. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65160/0.68452. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64663/0.68359. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64925/0.68486. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.65060/0.68739. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64458/0.68514. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64494/0.68889. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64751/0.68891. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64851/0.69032. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64362/0.68991. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64378/0.68761. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64110/0.69173. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64667/0.69127. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64173/0.68816. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63991/0.69198. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63889/0.69686. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64245/0.69465. Took 0.20 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69296/0.68965. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69244/0.68908. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69170/0.68917. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69100/0.68941. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69117/0.68962. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69070/0.69016. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69005/0.69072. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69004/0.69100. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68920/0.69138. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68957/0.69190. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68898/0.69237. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68756/0.69320. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68818/0.69388. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68720/0.69441. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68605/0.69523. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68787/0.69568. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68590/0.69630. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68615/0.69703. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68483/0.69755. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68397/0.69831. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68284/0.69846. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68386/0.69918. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68296/0.69999. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68109/0.70110. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68142/0.70188. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68096/0.70183. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67830/0.70303. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67654/0.70408. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67721/0.70464. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67687/0.70527. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67553/0.70539. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67301/0.70756. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67198/0.70864. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67122/0.71060. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.66997/0.71146. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67016/0.71255. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.66587/0.71319. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66695/0.71368. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.66128/0.71528. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66248/0.71652. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.65953/0.71928. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.65806/0.71915. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.65293/0.72342. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.65092/0.72475. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.65197/0.72532. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.64963/0.72676. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.64480/0.72884. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.64638/0.72945. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.64936/0.73295. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.64518/0.73166. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.63974/0.73349. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.64099/0.73667. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.63597/0.73684. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.63948/0.73527. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.63487/0.73848. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.63126/0.74218. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.63631/0.74621. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.63262/0.74590. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.62846/0.74678. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.63238/0.74541. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.63058/0.74463. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.62837/0.74641. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.62613/0.74336. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.62323/0.74828. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.62401/0.74536. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.62125/0.75127. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.62113/0.74682. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.62130/0.75297. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.61939/0.75378. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.61657/0.75815. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.61849/0.76056. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.61817/0.75963. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.61364/0.76048. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.61450/0.75896. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.61290/0.75543. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.61180/0.76214. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.60807/0.76469. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.60777/0.76200. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.60821/0.76116. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.60337/0.76342. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.60985/0.76466. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.60625/0.76342. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.60275/0.76683. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.59934/0.77130. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.60370/0.77570. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.60091/0.76965. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.59775/0.77054. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.59461/0.77374. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.59608/0.76817. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.59221/0.77705. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.59298/0.78255. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.59461/0.78010. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.59153/0.78084. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.59051/0.78317. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.58419/0.77895. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.58635/0.78159. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59068/0.78383. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.59188/0.78095. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.58424/0.78671. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.58205/0.79115. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69468/0.69352. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69228/0.69389. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69256/0.69417. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69148/0.69442. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69173/0.69460. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69100/0.69479. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68993/0.69507. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68980/0.69537. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68931/0.69581. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69001/0.69624. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68886/0.69674. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68733/0.69722. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68894/0.69772. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68791/0.69831. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68747/0.69887. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68693/0.69953. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68701/0.70021. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68655/0.70080. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68451/0.70156. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68532/0.70228. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68554/0.70268. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68542/0.70299. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68514/0.70333. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68281/0.70411. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68437/0.70460. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68250/0.70530. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68327/0.70569. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68266/0.70599. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68234/0.70655. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68153/0.70720. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68144/0.70755. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68109/0.70820. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68077/0.70883. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68044/0.70917. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67978/0.70953. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67908/0.71027. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68027/0.71058. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67836/0.71089. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67695/0.71130. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67691/0.71184. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67549/0.71219. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67540/0.71261. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67671/0.71327. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67595/0.71371. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67427/0.71400. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67242/0.71486. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67214/0.71479. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67355/0.71481. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67206/0.71510. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67209/0.71534. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67086/0.71526. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67031/0.71654. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67049/0.71692. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67008/0.71682. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66821/0.71739. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66704/0.71769. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66925/0.71815. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66711/0.71706. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66823/0.71724. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66586/0.71826. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66570/0.71793. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66574/0.71719. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66344/0.71643. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66074/0.71774. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66066/0.71789. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66253/0.71900. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65973/0.71989. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66147/0.72028. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65809/0.72025. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65832/0.71847. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65576/0.71788. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65716/0.71775. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65211/0.71700. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65367/0.71870. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65185/0.71782. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65180/0.71877. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65054/0.71860. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64603/0.71848. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64680/0.72014. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64368/0.72023. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64614/0.71820. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64430/0.71788. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64175/0.71602. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.64255/0.71415. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.63899/0.71310. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64008/0.71237. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63327/0.71410. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63529/0.71450. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63439/0.71359. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63753/0.71243. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63499/0.71294. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62915/0.71639. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.62693/0.71613. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62905/0.71438. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62711/0.71365. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62555/0.71517. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.62514/0.71520. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62348/0.71506. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.61527/0.71403. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61828/0.71488. Took 0.20 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69250/0.70115. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69154/0.70330. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69111/0.70438. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69021/0.70528. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69016/0.70627. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68971/0.70675. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69008/0.70833. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68912/0.70960. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68891/0.71010. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68857/0.71055. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68795/0.71143. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68766/0.71235. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68769/0.71333. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68715/0.71381. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68730/0.71448. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68674/0.71522. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68631/0.71545. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68684/0.71556. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68623/0.71554. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68596/0.71547. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68506/0.71596. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68477/0.71686. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68425/0.71660. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68501/0.71653. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68466/0.71764. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68310/0.71770. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68327/0.71741. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68303/0.71689. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68259/0.71697. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68248/0.71669. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68222/0.71673. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68129/0.71685. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68179/0.71798. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68194/0.71750. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67924/0.71751. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68011/0.71671. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67783/0.71739. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67835/0.71793. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67831/0.71882. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67718/0.71839. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67674/0.71886. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67740/0.71900. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67647/0.71945. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67488/0.72017. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67446/0.72044. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67434/0.72055. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67340/0.72162. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67295/0.72180. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67278/0.72281. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67094/0.72257. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67125/0.72454. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67003/0.72615. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.66995/0.72508. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67218/0.72480. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67181/0.72781. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67033/0.72595. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66931/0.72837. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66797/0.72985. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66702/0.73016. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66608/0.73172. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66990/0.73028. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66477/0.73245. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66410/0.73520. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66522/0.73480. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66579/0.73475. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66354/0.73358. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66489/0.73508. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66319/0.73549. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66356/0.73852. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66387/0.73748. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66160/0.73689. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 0.66139/0.73833. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66134/0.73893. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66024/0.74044. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66206/0.73975. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65680/0.74124. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66015/0.74299. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65712/0.74356. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65670/0.74235. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65980/0.74316. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65538/0.74357. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65617/0.74379. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65700/0.74527. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65596/0.74685. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65580/0.74772. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65446/0.75149. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65467/0.74810. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65183/0.74825. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65325/0.74986. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65339/0.75102. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65189/0.75392. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65107/0.75129. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64837/0.75322. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64714/0.75673. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64694/0.75631. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64893/0.75453. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64811/0.75716. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64868/0.75850. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64611/0.75790. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64464/0.75998. Took 0.20 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69417/0.69048. Took 0.27 sec\n",
      "Epoch 1, Loss(train/val) 0.69303/0.69063. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69205/0.69096. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69204/0.69114. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69202/0.69129. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69106/0.69144. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69082/0.69179. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69043/0.69205. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69089/0.69208. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68901/0.69234. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68968/0.69253. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68975/0.69275. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68802/0.69299. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68787/0.69357. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68840/0.69394. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68793/0.69416. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68695/0.69449. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68676/0.69490. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68738/0.69512. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68631/0.69562. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68695/0.69602. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68506/0.69664. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68355/0.69746. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68335/0.69822. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68466/0.69876. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68274/0.69946. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68115/0.70057. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68104/0.70187. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67952/0.70340. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67952/0.70442. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67947/0.70590. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67768/0.70768. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67771/0.70899. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67486/0.71079. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67572/0.71258. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67469/0.71371. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67362/0.71528. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67268/0.71720. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67353/0.71792. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67227/0.72021. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67514/0.72243. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67038/0.72356. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66740/0.72500. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66864/0.72677. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66686/0.73122. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66809/0.73218. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66702/0.73312. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66560/0.73425. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66626/0.73691. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66624/0.73828. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66533/0.73861. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66298/0.74175. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.66176/0.74164. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66347/0.74528. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66242/0.74620. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66131/0.74871. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66169/0.74966. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66117/0.74886. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65723/0.75377. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66035/0.75220. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65821/0.75515. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65726/0.75653. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65777/0.76003. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65762/0.76086. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65337/0.76128. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65348/0.76269. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65754/0.76370. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65125/0.76625. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65250/0.76614. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65042/0.76904. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65619/0.77072. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65622/0.77072. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65144/0.76923. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65033/0.77356. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65260/0.77235. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64738/0.77480. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65156/0.77579. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64775/0.77376. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64631/0.77736. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64614/0.77706. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64969/0.77940. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64415/0.78161. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64775/0.78226. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64328/0.78560. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64884/0.78395. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64278/0.78490. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64489/0.78522. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64060/0.78808. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63978/0.78879. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63911/0.79063. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64063/0.79229. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63757/0.79446. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63888/0.79471. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63803/0.79762. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63655/0.79154. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63388/0.79715. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63969/0.79607. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63788/0.79767. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63495/0.79400. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63276/0.79767. Took 0.19 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69351/0.69046. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69195/0.69065. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69143/0.69030. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69184/0.69002. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69071/0.68974. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68977/0.68952. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68985/0.68920. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68936/0.68899. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68912/0.68889. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68923/0.68882. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68811/0.68856. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68887/0.68861. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68831/0.68851. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68766/0.68826. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68838/0.68842. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68724/0.68866. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68777/0.68875. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68666/0.68902. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68555/0.68913. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68676/0.68938. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68580/0.68969. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68569/0.68991. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68583/0.68945. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68395/0.69006. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68507/0.69014. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68409/0.69022. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68321/0.69011. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68374/0.69141. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68182/0.69218. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68238/0.69252. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68191/0.69321. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68263/0.69413. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68097/0.69408. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68188/0.69441. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67987/0.69436. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68121/0.69477. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67889/0.69521. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67833/0.69561. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67862/0.69629. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67779/0.69670. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67876/0.69717. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67662/0.69780. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67530/0.69897. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67897/0.69974. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67576/0.70023. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67426/0.70056. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67396/0.70171. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67500/0.70162. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67426/0.70178. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67274/0.70299. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67186/0.70472. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67146/0.70518. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67316/0.70538. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66856/0.70566. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66880/0.70820. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67043/0.70761. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66711/0.70933. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66697/0.70990. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66660/0.71032. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66357/0.71273. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66552/0.71265. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66476/0.71519. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66119/0.71505. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66539/0.71635. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66074/0.71775. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65907/0.72078. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66157/0.72226. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66086/0.72485. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66228/0.72407. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65945/0.72499. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65904/0.72580. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65841/0.72551. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65720/0.72747. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65688/0.72892. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65631/0.73255. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65694/0.73208. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65553/0.73422. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.65505/0.73512. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65446/0.73404. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65105/0.73613. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65487/0.73853. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65397/0.73897. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65101/0.73772. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64716/0.73887. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64741/0.74122. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65186/0.74237. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64716/0.74388. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64976/0.74529. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64441/0.74561. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64614/0.74800. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64546/0.75021. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64908/0.74834. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64671/0.75005. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64240/0.75258. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64169/0.75436. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64299/0.75554. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64178/0.75752. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64373/0.75630. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64217/0.75791. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64018/0.76146. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69404/0.69235. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69300/0.69210. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69190/0.69151. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69184/0.69096. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69157/0.69060. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69197/0.69036. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69088/0.69030. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69189/0.69029. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69015/0.69027. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69087/0.69032. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69027/0.69029. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68810/0.69032. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69048/0.69000. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68917/0.68987. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68986/0.68982. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68930/0.68978. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.68842/0.68957. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68834/0.68954. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68880/0.68942. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68777/0.68918. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68807/0.68899. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68733/0.68888. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68675/0.68867. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68703/0.68862. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.68617/0.68856. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68809/0.68786. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68703/0.68759. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68513/0.68752. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68586/0.68701. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.68395/0.68638. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68462/0.68596. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68508/0.68563. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68577/0.68494. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68351/0.68474. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68335/0.68392. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.68381/0.68372. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68388/0.68382. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68222/0.68397. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68280/0.68358. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68289/0.68251. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68303/0.68231. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.68214/0.68207. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68207/0.68180. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68019/0.68152. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67943/0.68090. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.68108/0.68044. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67919/0.68024. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67854/0.67977. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67962/0.67934. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67828/0.67935. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67843/0.67897. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67754/0.67911. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67827/0.67934. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67746/0.67914. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67663/0.67920. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67467/0.67899. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67516/0.67861. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67599/0.67837. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67486/0.67906. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67431/0.67855. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67229/0.67838. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67240/0.67833. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.67206/0.67885. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67235/0.67880. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67453/0.67889. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67226/0.67901. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66792/0.67863. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66971/0.67870. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66944/0.67889. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66937/0.67886. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66819/0.67880. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66877/0.67895. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66886/0.67817. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66749/0.67822. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66721/0.67794. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66401/0.67856. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66487/0.68003. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66456/0.67965. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66396/0.67939. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66301/0.67895. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.66374/0.67946. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66111/0.67960. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65948/0.68014. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66116/0.67906. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.66000/0.67977. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65932/0.67962. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65827/0.67997. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65742/0.68011. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65994/0.68053. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65763/0.68105. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.65795/0.68055. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65337/0.68071. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.65616/0.67929. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65426/0.68011. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65488/0.68097. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65267/0.68061. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.65387/0.68222. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65134/0.68215. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65075/0.68391. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65053/0.68363. Took 0.20 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69461/0.69565. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69294/0.69537. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69324/0.69436. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69228/0.69363. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69225/0.69276. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69220/0.69228. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69108/0.69208. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69064/0.69151. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69129/0.69133. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69066/0.69093. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69059/0.69053. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.69014/0.69015. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.69022/0.68992. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68945/0.68988. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.68974/0.68955. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68970/0.68933. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68976/0.68960. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68914/0.68952. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68852/0.68955. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68833/0.68957. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68897/0.68969. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68847/0.68977. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68817/0.68994. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68736/0.68982. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68794/0.69023. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68665/0.69023. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68738/0.69007. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68649/0.69094. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68729/0.69065. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68689/0.69106. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68517/0.69108. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68527/0.69184. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68620/0.69242. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68716/0.69241. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68438/0.69268. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68383/0.69305. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68450/0.69374. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68340/0.69430. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68350/0.69497. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68120/0.69521. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68053/0.69589. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68301/0.69631. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68059/0.69704. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67940/0.69775. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67898/0.69861. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67916/0.69896. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67774/0.70033. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67655/0.70042. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67722/0.70184. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67565/0.70294. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67577/0.70427. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67612/0.70424. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67397/0.70453. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67163/0.70527. Took 0.23 sec\n",
      "Epoch 54, Loss(train/val) 0.67196/0.70644. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67355/0.70597. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67196/0.70686. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66922/0.70829. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67172/0.70894. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66888/0.71060. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66800/0.71030. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66619/0.71078. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66504/0.71262. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66607/0.71302. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66733/0.71330. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66426/0.71437. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66523/0.71583. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66538/0.71399. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66233/0.71702. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66366/0.71719. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65786/0.71750. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65931/0.71793. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65898/0.71925. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65763/0.72051. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65751/0.72237. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65542/0.72077. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65421/0.71991. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65539/0.72157. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65168/0.72262. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65330/0.72576. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65203/0.72493. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65452/0.72622. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65151/0.72779. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64933/0.73005. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65089/0.72990. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64917/0.73002. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64918/0.73109. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64853/0.73211. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 0.64858/0.73274. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64390/0.73578. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64430/0.73393. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64722/0.73736. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64237/0.73726. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63972/0.73800. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.63670/0.74136. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63885/0.73968. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63689/0.74087. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63629/0.74258. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63594/0.74327. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63302/0.74507. Took 0.20 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69307/0.69255. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69282/0.69268. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69271/0.69296. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69264/0.69336. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69292/0.69356. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69284/0.69370. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69217/0.69387. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69197/0.69414. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69141/0.69442. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69165/0.69478. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69230/0.69506. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69168/0.69532. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69126/0.69579. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69089/0.69618. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.69170/0.69647. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.69017/0.69673. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.69109/0.69719. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.69049/0.69776. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.69059/0.69837. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68987/0.69900. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.69057/0.69921. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69059/0.69932. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68802/0.70008. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68968/0.70048. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68811/0.70088. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68693/0.70114. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68785/0.70164. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68735/0.70160. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68650/0.70209. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68668/0.70235. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68628/0.70248. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68494/0.70273. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68574/0.70247. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68626/0.70287. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68555/0.70258. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68483/0.70275. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68341/0.70224. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68231/0.70248. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68240/0.70330. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68154/0.70313. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68224/0.70242. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68080/0.70197. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67935/0.70254. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67891/0.70371. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67811/0.70321. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67890/0.70293. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67792/0.70271. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67484/0.70228. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67602/0.70309. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67507/0.70356. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67561/0.70466. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.67473/0.70433. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67235/0.70452. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67387/0.70416. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67256/0.70546. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67221/0.70501. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66888/0.70408. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67180/0.70426. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67112/0.70434. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66885/0.70411. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66889/0.70478. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66575/0.70652. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66455/0.70656. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66613/0.70629. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66596/0.70592. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66475/0.70437. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66492/0.70669. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66213/0.70630. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66117/0.70685. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66076/0.70759. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66056/0.70703. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66213/0.70834. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66130/0.70765. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65817/0.70860. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65963/0.71024. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66015/0.70963. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65749/0.70920. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65739/0.70647. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65443/0.70990. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65617/0.71008. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65068/0.70906. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65376/0.70966. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65336/0.71142. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65255/0.71161. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65340/0.71144. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.64927/0.71264. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64908/0.71291. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65038/0.71360. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64674/0.71493. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64731/0.71401. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64561/0.71568. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64707/0.71309. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64022/0.71813. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65022/0.71493. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64301/0.71776. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64299/0.71606. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63922/0.71662. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64129/0.71900. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63960/0.71793. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63870/0.72094. Took 0.19 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69469/0.68730. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69316/0.68854. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69284/0.68946. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69211/0.69049. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69177/0.69137. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69186/0.69222. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69116/0.69295. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69085/0.69335. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69055/0.69387. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68973/0.69459. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69030/0.69510. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68912/0.69567. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68956/0.69614. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68868/0.69657. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68934/0.69650. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68910/0.69675. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68842/0.69711. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68767/0.69765. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68763/0.69809. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68740/0.69798. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68725/0.69829. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68668/0.69905. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68703/0.69940. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68636/0.69953. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68578/0.70003. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68491/0.70039. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68450/0.70123. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68432/0.70140. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68398/0.70216. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68442/0.70206. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68416/0.70206. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68278/0.70254. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68138/0.70384. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68227/0.70458. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68162/0.70462. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68083/0.70510. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68169/0.70496. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68136/0.70464. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68050/0.70462. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67969/0.70514. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67947/0.70603. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67847/0.70632. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67794/0.70730. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67852/0.70742. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67763/0.70715. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67636/0.70782. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67655/0.70835. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67608/0.70803. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67446/0.70898. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67483/0.70944. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67464/0.70848. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67378/0.70814. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67564/0.70835. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67255/0.70883. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67166/0.71082. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67000/0.71138. Took 0.22 sec\n",
      "Epoch 56, Loss(train/val) 0.67145/0.71180. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66979/0.71353. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66825/0.71357. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66846/0.71304. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66865/0.71480. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66705/0.71676. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66851/0.71790. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66802/0.71691. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66455/0.71724. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66633/0.71802. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66444/0.71967. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66399/0.71935. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66499/0.71937. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66338/0.72068. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66177/0.72226. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66170/0.72301. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65934/0.72355. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66043/0.72478. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65877/0.72532. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66028/0.72612. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65798/0.72508. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65779/0.72676. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65828/0.72977. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65909/0.72775. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65376/0.72862. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65771/0.73255. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65038/0.73322. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65270/0.73180. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65232/0.73320. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65249/0.73479. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65189/0.73785. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65270/0.73695. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65142/0.74053. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65076/0.74027. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64987/0.74025. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65021/0.74052. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64969/0.74059. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65249/0.73904. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64813/0.74443. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64774/0.74497. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64883/0.74393. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64692/0.74332. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64553/0.74634. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64554/0.74831. Took 0.20 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69419/0.69192. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69342/0.69336. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69293/0.69451. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69274/0.69541. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69189/0.69623. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69135/0.69717. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69167/0.69805. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69082/0.69901. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69049/0.69995. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69020/0.70080. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69029/0.70165. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69073/0.70236. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68994/0.70331. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68992/0.70409. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68916/0.70490. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68904/0.70587. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68814/0.70677. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68870/0.70740. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68893/0.70806. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68814/0.70906. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.68754/0.70980. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68761/0.71033. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68729/0.71101. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68744/0.71135. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68723/0.71190. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68630/0.71259. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68575/0.71283. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68598/0.71292. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68543/0.71338. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68554/0.71354. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68544/0.71366. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68591/0.71363. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68426/0.71346. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68344/0.71367. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68243/0.71422. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68210/0.71507. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68353/0.71515. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68152/0.71498. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68273/0.71549. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68078/0.71567. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68063/0.71540. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68134/0.71542. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68173/0.71517. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67835/0.71489. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67999/0.71524. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67684/0.71516. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67650/0.71531. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67681/0.71505. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67737/0.71530. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67421/0.71485. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67635/0.71401. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67527/0.71468. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67441/0.71529. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67423/0.71496. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67451/0.71408. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67353/0.71344. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67429/0.71355. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67278/0.71299. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67115/0.71211. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66991/0.71302. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66843/0.71360. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66868/0.71294. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66724/0.71318. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66837/0.71400. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66759/0.71427. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66749/0.71408. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66736/0.71416. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66793/0.71408. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66784/0.71289. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66474/0.71226. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66434/0.71236. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66340/0.71254. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66619/0.71144. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66131/0.71220. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66350/0.71225. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66467/0.71210. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66117/0.71204. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66100/0.71209. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66495/0.71253. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66048/0.71291. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 0.66072/0.71187. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65959/0.71117. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65920/0.71109. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65774/0.71151. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65779/0.71122. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65613/0.71190. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65708/0.71245. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65591/0.71232. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65303/0.71312. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65520/0.71238. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65359/0.71265. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65157/0.71161. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.65303/0.71230. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65352/0.71253. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65163/0.71242. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64939/0.71182. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.65068/0.71143. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65280/0.71094. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64786/0.71045. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64854/0.71099. Took 0.20 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69333/0.69413. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69165/0.69356. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69092/0.69366. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69113/0.69383. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69047/0.69418. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69000/0.69461. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68878/0.69540. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68925/0.69609. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68730/0.69695. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68802/0.69793. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68743/0.69888. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68684/0.69992. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68599/0.70102. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68496/0.70219. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68577/0.70335. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68477/0.70407. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68420/0.70549. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68319/0.70658. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68439/0.70743. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68487/0.70798. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68247/0.70873. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68425/0.70926. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68234/0.70992. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68193/0.71030. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68102/0.71104. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68199/0.71159. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68070/0.71247. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68192/0.71262. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68094/0.71339. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67881/0.71395. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67930/0.71421. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67898/0.71476. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67823/0.71530. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.67854/0.71502. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67776/0.71547. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67742/0.71627. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67747/0.71649. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67622/0.71644. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67660/0.71703. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67580/0.71740. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.67496/0.71774. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67647/0.71725. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67622/0.71680. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67367/0.71722. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67470/0.71697. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67357/0.71734. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67300/0.71790. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67233/0.71746. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67204/0.71851. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66939/0.71913. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67046/0.71881. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67022/0.72033. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66940/0.71958. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66818/0.72088. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66945/0.72065. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66847/0.72066. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66770/0.72206. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66903/0.72225. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66519/0.72303. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66528/0.72290. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66728/0.72381. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66522/0.72337. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66431/0.72409. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 0.66569/0.72450. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66187/0.72346. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66232/0.72429. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66088/0.72526. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66210/0.72597. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66038/0.72657. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66028/0.72664. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65900/0.72910. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65994/0.72985. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66003/0.73057. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65562/0.73120. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65613/0.73275. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65702/0.73437. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65637/0.73380. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65417/0.73308. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65399/0.73523. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65440/0.73627. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65129/0.73688. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65345/0.73779. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65238/0.73931. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65042/0.74045. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65190/0.74247. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64848/0.74224. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65132/0.74514. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64796/0.74678. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64734/0.74840. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64626/0.74699. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64185/0.74980. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64241/0.74934. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64539/0.75072. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64146/0.75056. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64146/0.75390. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64481/0.75445. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64352/0.75663. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64017/0.75936. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63781/0.76017. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64436/0.76033. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69396/0.69332. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69206/0.69282. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69113/0.69203. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69019/0.69144. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68940/0.69152. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68934/0.69107. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68736/0.69106. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68723/0.69115. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68647/0.69140. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68668/0.69112. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68637/0.69074. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68539/0.69119. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68573/0.69120. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68570/0.69105. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68442/0.69117. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68421/0.69145. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68411/0.69183. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68370/0.69171. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68359/0.69147. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68206/0.69169. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68218/0.69172. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68185/0.69189. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68222/0.69200. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68106/0.69231. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68045/0.69196. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68080/0.69203. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68009/0.69210. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67871/0.69168. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.67844/0.69169. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67806/0.69195. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67766/0.69224. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67651/0.69171. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67727/0.69219. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67581/0.69168. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67373/0.69166. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67489/0.69144. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67346/0.69198. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67384/0.69149. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67212/0.69168. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67118/0.69185. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66943/0.69219. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66881/0.69218. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.66884/0.69268. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66728/0.69316. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.66731/0.69260. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66549/0.69260. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.66732/0.69283. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66521/0.69256. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66479/0.69315. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66276/0.69259. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.66100/0.69194. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66177/0.69230. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66027/0.69385. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65972/0.69228. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.65898/0.69428. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65781/0.69425. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.65362/0.69356. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65482/0.69532. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.65564/0.69369. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65766/0.69426. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.65504/0.69455. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65001/0.69536. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.64914/0.69503. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64941/0.69639. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64929/0.69701. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64725/0.69759. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64640/0.69872. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64450/0.69746. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64309/0.69827. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64244/0.70042. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64207/0.69858. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64159/0.69984. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63901/0.69974. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.63965/0.70102. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.63599/0.70311. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.63651/0.70315. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63551/0.70238. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63528/0.70194. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63270/0.70194. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63252/0.70301. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.63318/0.70750. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62897/0.70766. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.62962/0.70671. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62651/0.71185. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62706/0.70944. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62225/0.71223. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.62628/0.71274. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62195/0.71379. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.61873/0.71270. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61616/0.71994. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.61738/0.71970. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61700/0.71934. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.61735/0.72381. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61800/0.72623. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.61305/0.72360. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60946/0.72426. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.61418/0.72848. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61364/0.72733. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.60866/0.73053. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60454/0.73405. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69396/0.69036. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69315/0.68943. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69184/0.68864. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69179/0.68795. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69145/0.68735. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69184/0.68672. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.69083/0.68631. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68978/0.68566. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69009/0.68511. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68918/0.68467. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68858/0.68413. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68763/0.68359. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68805/0.68289. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68703/0.68267. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68714/0.68277. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68624/0.68231. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68474/0.68220. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68605/0.68234. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68464/0.68236. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68475/0.68216. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68338/0.68211. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68318/0.68231. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68184/0.68260. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68259/0.68219. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68243/0.68194. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68259/0.68281. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68134/0.68314. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67942/0.68354. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68072/0.68299. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67941/0.68283. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.67995/0.68285. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67872/0.68337. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67910/0.68362. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68005/0.68428. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67814/0.68390. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67679/0.68427. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67677/0.68459. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67816/0.68427. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67483/0.68549. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67561/0.68625. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67492/0.68643. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67523/0.68614. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67373/0.68690. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67205/0.68748. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67400/0.68709. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67198/0.68725. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.67166/0.68735. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67210/0.68741. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66976/0.68856. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66861/0.68669. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67094/0.68877. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67106/0.68992. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.66680/0.68955. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66796/0.69031. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66651/0.69041. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66702/0.69114. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66687/0.69115. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66408/0.69159. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66406/0.69283. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66443/0.69242. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66211/0.69262. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66082/0.69289. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.65988/0.69312. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66093/0.69375. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.65814/0.69475. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65788/0.69563. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65741/0.69577. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65621/0.69672. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65662/0.69780. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65362/0.69837. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65187/0.69903. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65301/0.69814. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65063/0.69860. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64977/0.70110. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64980/0.70151. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65114/0.70214. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64772/0.70186. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64760/0.70555. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64551/0.70396. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64375/0.70368. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64350/0.70534. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64180/0.70230. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.64051/0.70214. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63732/0.70402. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64140/0.70371. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63736/0.70579. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.63409/0.70577. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63277/0.70790. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63042/0.70638. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63461/0.70477. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63327/0.70087. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63075/0.70329. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.62594/0.70403. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62802/0.70432. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62777/0.70346. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62453/0.70563. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.62165/0.70674. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62443/0.70768. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.62034/0.70979. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62076/0.71130. Took 0.20 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69445/0.69643. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69230/0.69523. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69223/0.69456. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69216/0.69407. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69176/0.69385. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69215/0.69349. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69213/0.69325. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69136/0.69299. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69137/0.69282. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69034/0.69260. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69085/0.69238. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68865/0.69205. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68916/0.69174. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68915/0.69150. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68851/0.69146. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68808/0.69144. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68657/0.69125. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68745/0.69120. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.68747/0.69114. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68753/0.69129. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68593/0.69111. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68529/0.69067. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68554/0.69084. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68526/0.69153. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68590/0.69172. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68471/0.69180. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68444/0.69189. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68378/0.69247. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68484/0.69263. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68281/0.69274. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68445/0.69282. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68288/0.69349. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68128/0.69413. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68227/0.69444. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68133/0.69542. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68234/0.69594. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68098/0.69668. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68235/0.69680. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68031/0.69762. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67924/0.69826. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67922/0.69874. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68040/0.69913. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67948/0.69887. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67847/0.70036. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67992/0.70105. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67934/0.70110. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67609/0.70233. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67807/0.70272. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67649/0.70323. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67638/0.70355. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67590/0.70410. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67647/0.70413. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67370/0.70578. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67457/0.70551. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67372/0.70729. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67307/0.70777. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67354/0.70754. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67173/0.70903. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67108/0.70991. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67220/0.71003. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66804/0.71038. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66988/0.71055. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66966/0.71160. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66795/0.71239. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66581/0.71282. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66452/0.71429. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66483/0.71544. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66496/0.71610. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66385/0.71740. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66568/0.71790. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66366/0.71706. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66445/0.71761. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66093/0.72015. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66324/0.72142. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65993/0.72217. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66051/0.72399. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66253/0.72394. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65632/0.72517. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65679/0.72556. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65700/0.72642. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65270/0.72931. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65405/0.73155. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65133/0.73163. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65318/0.73304. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64903/0.73327. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64928/0.73348. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64672/0.73590. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64602/0.73458. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64484/0.73545. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64323/0.73752. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64611/0.73700. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64345/0.73495. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.63821/0.73825. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63957/0.73958. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63550/0.74047. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63442/0.74410. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63729/0.74302. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63184/0.74120. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63478/0.74294. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63218/0.74379. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69395/0.69250. Took 0.31 sec\n",
      "Epoch 1, Loss(train/val) 0.69285/0.69214. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69249/0.69184. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69115/0.69179. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69176/0.69159. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69145/0.69161. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69078/0.69151. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69130/0.69150. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69074/0.69133. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69065/0.69138. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.69043/0.69143. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68997/0.69147. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68991/0.69158. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69073/0.69154. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68984/0.69178. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.69010/0.69194. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68961/0.69207. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68910/0.69228. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68931/0.69248. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68888/0.69280. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68871/0.69291. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68834/0.69295. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68902/0.69298. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68899/0.69323. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68803/0.69359. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68918/0.69347. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68855/0.69354. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68784/0.69377. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68773/0.69399. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68859/0.69409. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68774/0.69439. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68726/0.69452. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68854/0.69481. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68805/0.69520. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68635/0.69541. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68761/0.69579. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68726/0.69616. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68766/0.69629. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68673/0.69680. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68502/0.69720. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68610/0.69760. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68611/0.69786. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68628/0.69813. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68559/0.69830. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68472/0.69882. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68432/0.69932. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68523/0.70000. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68478/0.69998. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68433/0.70060. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.68379/0.70073. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68335/0.70130. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68457/0.70141. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68371/0.70172. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68427/0.70212. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.68342/0.70264. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68312/0.70278. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.68319/0.70343. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.68257/0.70386. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.68178/0.70420. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.68278/0.70466. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.68209/0.70483. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.68130/0.70525. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.68287/0.70567. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.68107/0.70596. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.68187/0.70632. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.68023/0.70665. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67927/0.70733. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.68082/0.70767. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.68097/0.70771. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.68001/0.70866. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67918/0.70842. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67835/0.70900. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.67875/0.70890. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67916/0.70991. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67824/0.70965. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67897/0.70954. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.67878/0.70918. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.67784/0.70918. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.67629/0.70969. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.67555/0.71006. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.67462/0.71135. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.67688/0.71106. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.67689/0.71145. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67497/0.71172. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.67557/0.71343. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.67291/0.71307. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.67445/0.71379. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.67427/0.71392. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.67361/0.71489. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.67196/0.71503. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.67239/0.71566. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.67310/0.71604. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.67210/0.71648. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.67113/0.71713. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.67217/0.71712. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.67064/0.71773. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.66920/0.71861. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.66806/0.71881. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.66959/0.71917. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66906/0.71961. Took 0.19 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69607/0.69209. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69357/0.69205. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69318/0.69254. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69314/0.69322. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69249/0.69377. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69199/0.69443. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69196/0.69473. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69192/0.69514. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69123/0.69538. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69073/0.69547. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69170/0.69569. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69143/0.69572. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.69105/0.69594. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69123/0.69618. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69032/0.69634. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69024/0.69626. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68997/0.69622. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.69017/0.69608. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68923/0.69622. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68922/0.69631. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.69014/0.69579. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68961/0.69587. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68890/0.69580. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68866/0.69551. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68867/0.69550. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68775/0.69561. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68716/0.69516. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68791/0.69486. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68717/0.69476. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68639/0.69458. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68500/0.69413. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68710/0.69321. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68499/0.69358. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68524/0.69319. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68315/0.69314. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68352/0.69232. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68217/0.69201. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68333/0.69167. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68218/0.69076. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67971/0.69017. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.68119/0.68960. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.67942/0.68906. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.67914/0.68916. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67899/0.68797. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67685/0.68856. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67604/0.68722. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.67439/0.68690. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67436/0.68600. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 0.67268/0.68507. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67203/0.68410. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 0.67129/0.68332. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.66969/0.68323. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.66916/0.68238. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.66966/0.68306. Took 0.21 sec\n",
      "Epoch 54, Loss(train/val) 0.66791/0.68220. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.66902/0.68054. Took 0.26 sec\n",
      "Epoch 56, Loss(train/val) 0.66571/0.67968. Took 0.22 sec\n",
      "Epoch 57, Loss(train/val) 0.66594/0.68039. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 0.66134/0.67938. Took 0.22 sec\n",
      "Epoch 59, Loss(train/val) 0.66269/0.68048. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66240/0.68192. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66231/0.68045. Took 0.23 sec\n",
      "Epoch 62, Loss(train/val) 0.66158/0.68337. Took 0.24 sec\n",
      "Epoch 63, Loss(train/val) 0.65962/0.68026. Took 0.22 sec\n",
      "Epoch 64, Loss(train/val) 0.65805/0.68227. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.65727/0.68150. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 0.65784/0.68240. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.65233/0.68295. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65248/0.68308. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 0.65281/0.68169. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65317/0.68239. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 0.65162/0.67991. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65365/0.68122. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64869/0.68425. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.64645/0.68171. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64465/0.68602. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.64801/0.68059. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64544/0.68214. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64776/0.68106. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64043/0.68444. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64423/0.68321. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63950/0.68220. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.64225/0.68637. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64076/0.68514. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64012/0.68485. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.63632/0.68536. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 0.63453/0.68602. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63801/0.68885. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63050/0.68779. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63009/0.68815. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.62598/0.69028. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62534/0.68915. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.62526/0.68841. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62642/0.68970. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62497/0.69151. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62192/0.68919. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.62542/0.68983. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62346/0.68824. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.62012/0.69107. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.61398/0.69362. Took 0.20 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69523/0.69011. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69340/0.68954. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69328/0.68908. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69124/0.68914. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69176/0.68940. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69169/0.68917. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.69078/0.68912. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69094/0.68925. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.69057/0.68953. Took 0.22 sec\n",
      "Epoch 9, Loss(train/val) 0.68972/0.68956. Took 0.22 sec\n",
      "Epoch 10, Loss(train/val) 0.69037/0.68996. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.69030/0.68993. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.68847/0.69013. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68912/0.69035. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68822/0.69073. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68832/0.69109. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68789/0.69120. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68684/0.69194. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68692/0.69246. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68601/0.69318. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68662/0.69385. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68564/0.69429. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68547/0.69496. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68361/0.69604. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68361/0.69649. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68268/0.69794. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68254/0.69864. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68235/0.69955. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68075/0.69991. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68053/0.70098. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.67901/0.70247. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67974/0.70283. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67904/0.70357. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67988/0.70537. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67680/0.70528. Took 0.22 sec\n",
      "Epoch 35, Loss(train/val) 0.67710/0.70670. Took 0.22 sec\n",
      "Epoch 36, Loss(train/val) 0.67563/0.70733. Took 0.23 sec\n",
      "Epoch 37, Loss(train/val) 0.67249/0.70841. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.67264/0.70904. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67260/0.70988. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67304/0.70970. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67110/0.71130. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.66959/0.71294. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67009/0.71309. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 0.66827/0.71400. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66750/0.71516. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.66632/0.71630. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 0.66609/0.71882. Took 0.23 sec\n",
      "Epoch 48, Loss(train/val) 0.66407/0.72000. Took 0.22 sec\n",
      "Epoch 49, Loss(train/val) 0.66004/0.72195. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.66604/0.72219. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66451/0.72153. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.66478/0.72372. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.66350/0.72554. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66034/0.72777. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65759/0.73048. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.65849/0.73060. Took 0.22 sec\n",
      "Epoch 57, Loss(train/val) 0.65463/0.73207. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.65780/0.73400. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.65404/0.73675. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 0.65440/0.73738. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.65559/0.73746. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 0.65273/0.73833. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65058/0.73891. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.64903/0.74303. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.64986/0.74380. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.64911/0.74393. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65033/0.74607. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 0.64755/0.74868. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.64677/0.74818. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.64455/0.75089. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64125/0.75766. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.64646/0.75475. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64246/0.75955. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.64066/0.75920. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64083/0.76047. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.64047/0.76350. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64090/0.76390. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.63856/0.76514. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63864/0.77052. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.63819/0.77307. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.62900/0.77161. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.63432/0.77691. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.63191/0.77510. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.62911/0.77825. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63026/0.78183. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 0.62853/0.78242. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.62639/0.78357. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63049/0.78833. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.62650/0.78806. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.62511/0.79205. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62522/0.79021. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.62378/0.79393. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.62203/0.79243. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.62333/0.79355. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 0.62035/0.79860. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.61998/0.79876. Took 0.22 sec\n",
      "Epoch 97, Loss(train/val) 0.62199/0.80208. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.61626/0.80223. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.61636/0.80760. Took 0.20 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69334/0.69055. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69313/0.69025. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69294/0.69012. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69311/0.69014. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69265/0.68983. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69225/0.68967. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.69221/0.68960. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69231/0.68955. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.69355/0.68964. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69247/0.68952. Took 0.23 sec\n",
      "Epoch 10, Loss(train/val) 0.69208/0.68973. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69228/0.69029. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.69228/0.69056. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69266/0.69072. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.69136/0.69052. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.69170/0.69075. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.69081/0.69063. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.69201/0.69061. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.69163/0.69116. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.69149/0.69130. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.69120/0.69133. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.69053/0.69150. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.69071/0.69118. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.69103/0.69170. Took 0.22 sec\n",
      "Epoch 24, Loss(train/val) 0.69030/0.69149. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68953/0.69180. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68998/0.69218. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68795/0.69228. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68854/0.69300. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.69016/0.69346. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68806/0.69324. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68882/0.69361. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68783/0.69389. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68727/0.69376. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68708/0.69473. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68616/0.69646. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68594/0.69582. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68562/0.69519. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68431/0.69592. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68436/0.69681. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68324/0.69665. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68397/0.69665. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68298/0.69840. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68119/0.69915. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68212/0.69799. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68104/0.69765. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67953/0.69797. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67892/0.69857. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67723/0.69944. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67810/0.69799. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67647/0.69824. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67526/0.69742. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67385/0.69780. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67303/0.69804. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67061/0.69677. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67380/0.69736. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66939/0.69895. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66898/0.69865. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66649/0.69815. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66673/0.69877. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66470/0.69717. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66534/0.69647. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66574/0.69796. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66379/0.69970. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66415/0.69671. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66385/0.69999. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65951/0.69676. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66062/0.69782. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66023/0.69910. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65936/0.69940. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65832/0.70029. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65662/0.69944. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65548/0.70371. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65434/0.70274. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65548/0.70290. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65494/0.70174. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65358/0.70315. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64933/0.70528. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65316/0.69817. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64663/0.70855. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65087/0.70794. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64719/0.70468. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64657/0.70462. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64670/0.70814. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64867/0.70424. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64288/0.70598. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64586/0.70932. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.64607/0.70904. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64434/0.70803. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.64266/0.70728. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64083/0.70658. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63962/0.71003. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.63804/0.71156. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64113/0.70500. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.63614/0.70860. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63763/0.70649. Took 0.21 sec\n",
      "Epoch 96, Loss(train/val) 0.63847/0.70722. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63517/0.70856. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63372/0.71411. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63372/0.70657. Took 0.20 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69655/0.68972. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69345/0.68994. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69221/0.69142. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69074/0.69226. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69052/0.69283. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68959/0.69349. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68851/0.69414. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68869/0.69481. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68802/0.69530. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68615/0.69603. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68609/0.69652. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68618/0.69756. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68480/0.69797. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68359/0.69882. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68400/0.70023. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68420/0.70073. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68216/0.70098. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68068/0.70220. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68095/0.70319. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68013/0.70466. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.67841/0.70484. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68015/0.70496. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 0.67920/0.70556. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67774/0.70698. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.67621/0.70780. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67677/0.70815. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 0.67601/0.70931. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67668/0.70931. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.67487/0.71079. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67390/0.71124. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67506/0.71226. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67225/0.71168. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67186/0.71303. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67312/0.71227. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67082/0.71375. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67125/0.71437. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.66959/0.71302. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.66940/0.71521. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.66905/0.71495. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66710/0.71607. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.66790/0.71575. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.66552/0.71522. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.66430/0.71546. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66295/0.71583. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.66247/0.71440. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.65956/0.71772. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.66053/0.71679. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65779/0.71821. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.65853/0.71753. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.65811/0.71754. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.65658/0.71635. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65492/0.71633. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.65510/0.71871. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.65160/0.72019. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.65220/0.71876. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65033/0.72121. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.65086/0.71775. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65005/0.71647. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.64964/0.71903. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.64834/0.71774. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.64680/0.71579. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.64504/0.71920. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.64523/0.71629. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.64373/0.71629. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.64167/0.71728. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.64106/0.71669. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.64106/0.71712. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64313/0.71844. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.64314/0.71812. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.63872/0.71517. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63640/0.71686. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.63452/0.71856. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.63652/0.71656. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.63687/0.71689. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.63369/0.71640. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.63100/0.71818. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.62872/0.71687. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.62843/0.72006. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.62765/0.71755. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.62967/0.71560. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.62786/0.71801. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.62737/0.71821. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.62688/0.71900. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.62849/0.71462. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.61886/0.71880. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.61853/0.71833. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.61710/0.71867. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.61967/0.71924. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.62046/0.71711. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.61454/0.71683. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.61422/0.71645. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.61638/0.71347. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.61418/0.71543. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.61449/0.71599. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.60939/0.71767. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61080/0.71547. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61261/0.71292. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.60760/0.71567. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 0.61030/0.71708. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.60240/0.71554. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69346/0.69259. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69145/0.69187. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69121/0.69105. Took 0.23 sec\n",
      "Epoch 3, Loss(train/val) 0.69097/0.69016. Took 0.22 sec\n",
      "Epoch 4, Loss(train/val) 0.69057/0.68950. Took 0.22 sec\n",
      "Epoch 5, Loss(train/val) 0.69023/0.68885. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.69016/0.68833. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69029/0.68802. Took 0.22 sec\n",
      "Epoch 8, Loss(train/val) 0.68985/0.68767. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68884/0.68720. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68838/0.68699. Took 0.22 sec\n",
      "Epoch 11, Loss(train/val) 0.68883/0.68710. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68871/0.68722. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68878/0.68727. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68763/0.68737. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68768/0.68735. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68690/0.68742. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68692/0.68741. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.68662/0.68789. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68682/0.68839. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.68692/0.68837. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68650/0.68891. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68567/0.68949. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68499/0.68994. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68481/0.69047. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68381/0.69118. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68538/0.69197. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68280/0.69248. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68354/0.69317. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68445/0.69362. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68400/0.69474. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68284/0.69535. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68270/0.69550. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68190/0.69653. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68119/0.69740. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68116/0.69829. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67945/0.69912. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67986/0.69991. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68024/0.70131. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67889/0.70188. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67835/0.70296. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67757/0.70334. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67734/0.70479. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67666/0.70628. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67734/0.70649. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67655/0.70801. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67514/0.70902. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67327/0.71009. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67352/0.71141. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67187/0.71259. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67317/0.71341. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67079/0.71516. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.66869/0.71605. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67000/0.71829. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66936/0.71905. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66682/0.71975. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66748/0.72285. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66500/0.72232. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66332/0.72374. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66041/0.72512. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66078/0.72689. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66081/0.73064. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 0.66124/0.73155. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65727/0.73394. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.65522/0.73740. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65348/0.73801. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65052/0.74236. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65247/0.74036. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65106/0.74319. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.64849/0.74379. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.64855/0.74249. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64343/0.74778. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 0.64430/0.74840. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64215/0.75032. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64290/0.75487. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64246/0.74931. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63729/0.75502. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.63639/0.76011. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.63550/0.75691. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63448/0.76500. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62764/0.76377. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63036/0.76763. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.63172/0.76448. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63097/0.76567. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.62923/0.76862. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62383/0.77142. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62057/0.77158. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62410/0.77251. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62182/0.77467. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.62494/0.77326. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62166/0.77338. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62183/0.77151. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.61866/0.77761. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.61942/0.76660. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.61436/0.77856. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.61378/0.77893. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61605/0.76923. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61085/0.77739. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.61278/0.78162. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.61104/0.78579. Took 0.20 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69138/0.69266. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69092/0.69341. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69192/0.69385. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69139/0.69409. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69023/0.69439. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69015/0.69467. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69011/0.69495. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69001/0.69523. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68976/0.69563. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68921/0.69600. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68937/0.69633. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68908/0.69660. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68918/0.69695. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68993/0.69732. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68850/0.69780. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68837/0.69795. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68875/0.69844. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68858/0.69881. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68763/0.69943. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68767/0.69986. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68676/0.70021. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68715/0.70058. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68683/0.70067. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68734/0.70105. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68588/0.70140. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68595/0.70194. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68587/0.70262. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68435/0.70329. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68541/0.70356. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68484/0.70396. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68383/0.70450. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68414/0.70523. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68418/0.70535. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68400/0.70561. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68292/0.70634. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68243/0.70712. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68254/0.70767. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68077/0.70858. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68169/0.70894. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68101/0.70932. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68027/0.71000. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67962/0.71071. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67900/0.71137. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68084/0.71182. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67618/0.71334. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67549/0.71434. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67592/0.71514. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67429/0.71627. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67507/0.71739. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67459/0.71759. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67348/0.71817. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67128/0.71934. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67070/0.72053. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67071/0.72182. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66995/0.72288. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.66821/0.72372. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66799/0.72467. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66917/0.72527. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66739/0.72636. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66528/0.72800. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66393/0.72933. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.66479/0.72990. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66527/0.73191. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 0.66251/0.73217. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.65963/0.73370. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65993/0.73492. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65844/0.73471. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65893/0.73693. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65522/0.73965. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65809/0.74231. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65360/0.74470. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65295/0.74565. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65388/0.74598. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64918/0.74953. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.64903/0.75191. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64662/0.75181. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.64770/0.75291. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64517/0.75559. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64547/0.75936. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64549/0.76029. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64535/0.76271. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63780/0.76794. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 0.64193/0.76893. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64022/0.77149. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.63484/0.77293. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63122/0.77466. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.63569/0.77514. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63477/0.77794. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63250/0.78029. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63350/0.78096. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.63004/0.78138. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62912/0.78315. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.63026/0.78444. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62580/0.78822. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62656/0.79126. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62432/0.79233. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.62471/0.79418. Took 0.22 sec\n",
      "Epoch 97, Loss(train/val) 0.62355/0.79331. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62264/0.79713. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62483/0.79829. Took 0.21 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69265/0.69877. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69121/0.69814. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69094/0.69700. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69041/0.69601. Took 0.25 sec\n",
      "Epoch 4, Loss(train/val) 0.68992/0.69517. Took 0.24 sec\n",
      "Epoch 5, Loss(train/val) 0.68997/0.69437. Took 0.22 sec\n",
      "Epoch 6, Loss(train/val) 0.68900/0.69365. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68960/0.69314. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68844/0.69245. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68858/0.69198. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68811/0.69149. Took 0.22 sec\n",
      "Epoch 11, Loss(train/val) 0.68743/0.69114. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.68773/0.69057. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68790/0.69057. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68757/0.69039. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68680/0.69041. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68662/0.69020. Took 0.22 sec\n",
      "Epoch 17, Loss(train/val) 0.68644/0.69037. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68591/0.69047. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68610/0.69053. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68559/0.69079. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68548/0.69075. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68462/0.69111. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68432/0.69116. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.68471/0.69093. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68494/0.69108. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68448/0.69154. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68375/0.69189. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68420/0.69245. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68268/0.69273. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68280/0.69289. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68283/0.69309. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68180/0.69316. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68124/0.69324. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68143/0.69341. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68198/0.69363. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68051/0.69429. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68110/0.69406. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68216/0.69394. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68013/0.69407. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.67877/0.69425. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67991/0.69445. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67945/0.69485. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67914/0.69495. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67920/0.69497. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67763/0.69535. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67760/0.69601. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67725/0.69647. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67686/0.69633. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67672/0.69617. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67498/0.69654. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67676/0.69670. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67532/0.69744. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67655/0.69847. Took 0.23 sec\n",
      "Epoch 54, Loss(train/val) 0.67544/0.69832. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.67541/0.69874. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67544/0.69844. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.67230/0.69964. Took 0.22 sec\n",
      "Epoch 58, Loss(train/val) 0.67315/0.69996. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.67360/0.70056. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67274/0.70185. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.67250/0.70232. Took 0.23 sec\n",
      "Epoch 62, Loss(train/val) 0.67282/0.70192. Took 0.22 sec\n",
      "Epoch 63, Loss(train/val) 0.67152/0.70190. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 0.66988/0.70297. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66973/0.70305. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.67159/0.70398. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67022/0.70455. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66929/0.70429. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66801/0.70518. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66785/0.70441. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67015/0.70544. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66777/0.70592. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66827/0.70527. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66647/0.70670. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66611/0.70780. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66585/0.70925. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66571/0.70769. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66640/0.70890. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66680/0.70808. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66298/0.70881. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66515/0.70874. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66362/0.70972. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66314/0.71043. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.66585/0.71068. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66486/0.71093. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65966/0.71130. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.66094/0.71144. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.66200/0.71193. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66032/0.71400. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.65785/0.71488. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65923/0.71484. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65801/0.71455. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66108/0.71281. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65807/0.71339. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65802/0.71384. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.65508/0.71657. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65902/0.71686. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65915/0.71737. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65685/0.71616. Took 0.20 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69319/0.69585. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69200/0.69681. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69093/0.69738. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69015/0.69822. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68965/0.69871. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68919/0.69913. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68900/0.69977. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68878/0.70021. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68787/0.70098. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68777/0.70115. Took 0.22 sec\n",
      "Epoch 10, Loss(train/val) 0.68772/0.70198. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68702/0.70228. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68628/0.70285. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68638/0.70355. Took 0.23 sec\n",
      "Epoch 14, Loss(train/val) 0.68595/0.70416. Took 0.22 sec\n",
      "Epoch 15, Loss(train/val) 0.68625/0.70465. Took 0.23 sec\n",
      "Epoch 16, Loss(train/val) 0.68530/0.70538. Took 0.23 sec\n",
      "Epoch 17, Loss(train/val) 0.68495/0.70636. Took 0.22 sec\n",
      "Epoch 18, Loss(train/val) 0.68495/0.70661. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68417/0.70692. Took 0.22 sec\n",
      "Epoch 20, Loss(train/val) 0.68453/0.70775. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68495/0.70818. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68336/0.70871. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68316/0.70902. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68206/0.71008. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68177/0.71109. Took 0.22 sec\n",
      "Epoch 26, Loss(train/val) 0.68025/0.71178. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68210/0.71220. Took 0.22 sec\n",
      "Epoch 28, Loss(train/val) 0.68154/0.71264. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.68083/0.71300. Took 0.22 sec\n",
      "Epoch 30, Loss(train/val) 0.68034/0.71351. Took 0.23 sec\n",
      "Epoch 31, Loss(train/val) 0.67948/0.71391. Took 0.22 sec\n",
      "Epoch 32, Loss(train/val) 0.67996/0.71483. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68121/0.71466. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68089/0.71557. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67884/0.71583. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67997/0.71620. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68061/0.71679. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67900/0.71732. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67778/0.71712. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.67843/0.71742. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.67725/0.71718. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67817/0.71737. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67701/0.71797. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67754/0.71883. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67725/0.71883. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67678/0.71862. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67588/0.71859. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67619/0.72003. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67713/0.72000. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67553/0.72004. Took 0.23 sec\n",
      "Epoch 51, Loss(train/val) 0.67615/0.72104. Took 0.23 sec\n",
      "Epoch 52, Loss(train/val) 0.67562/0.72117. Took 0.22 sec\n",
      "Epoch 53, Loss(train/val) 0.67501/0.72122. Took 0.22 sec\n",
      "Epoch 54, Loss(train/val) 0.67398/0.72172. Took 0.23 sec\n",
      "Epoch 55, Loss(train/val) 0.67470/0.72247. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67531/0.72260. Took 0.22 sec\n",
      "Epoch 57, Loss(train/val) 0.67328/0.72258. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67336/0.72391. Took 0.23 sec\n",
      "Epoch 59, Loss(train/val) 0.67382/0.72303. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 0.67301/0.72423. Took 0.22 sec\n",
      "Epoch 61, Loss(train/val) 0.67352/0.72380. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.67105/0.72400. Took 0.23 sec\n",
      "Epoch 63, Loss(train/val) 0.67278/0.72480. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 0.67278/0.72397. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.66990/0.72472. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.67071/0.72549. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66912/0.72556. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 0.67070/0.72683. Took 0.23 sec\n",
      "Epoch 69, Loss(train/val) 0.66911/0.72552. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66906/0.72607. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66823/0.72691. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66910/0.72743. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66774/0.72676. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66652/0.72799. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66789/0.73011. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66610/0.72877. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66857/0.73008. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66564/0.72949. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66538/0.73107. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66514/0.73027. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66313/0.73099. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.66200/0.73086. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66131/0.73332. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.66291/0.73441. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66276/0.73397. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66135/0.73454. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66190/0.73517. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.66003/0.73674. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65937/0.73785. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66046/0.73663. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66266/0.73754. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.65608/0.73680. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65880/0.73873. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65828/0.73826. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65550/0.74057. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65499/0.74244. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65595/0.74448. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65635/0.74449. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.65402/0.74248. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69456/0.69312. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69097/0.69181. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69076/0.69231. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69085/0.69299. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68937/0.69352. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69014/0.69410. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69025/0.69470. Took 0.22 sec\n",
      "Epoch 7, Loss(train/val) 0.68969/0.69523. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68975/0.69574. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68926/0.69614. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68983/0.69647. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68943/0.69670. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68961/0.69709. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68921/0.69725. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68899/0.69758. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68906/0.69797. Took 0.25 sec\n",
      "Epoch 16, Loss(train/val) 0.68905/0.69836. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68827/0.69893. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68931/0.69929. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68816/0.69975. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68859/0.70011. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68832/0.70060. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 0.68747/0.70081. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68839/0.70132. Took 0.22 sec\n",
      "Epoch 24, Loss(train/val) 0.68750/0.70203. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68764/0.70282. Took 0.22 sec\n",
      "Epoch 26, Loss(train/val) 0.68746/0.70307. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68712/0.70357. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68683/0.70391. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68611/0.70445. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68664/0.70514. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68598/0.70548. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68567/0.70630. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68636/0.70656. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68574/0.70716. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68537/0.70784. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68535/0.70844. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68516/0.70905. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68474/0.70973. Took 0.22 sec\n",
      "Epoch 39, Loss(train/val) 0.68452/0.71077. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.68408/0.71172. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68500/0.71235. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68411/0.71290. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68346/0.71354. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68332/0.71435. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68118/0.71515. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.68135/0.71641. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68275/0.71684. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.68130/0.71761. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68211/0.71836. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68221/0.71907. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68043/0.71913. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68060/0.72050. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67930/0.72133. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68033/0.72180. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67864/0.72350. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67856/0.72365. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67939/0.72423. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67802/0.72510. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67817/0.72596. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67643/0.72606. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67691/0.72698. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67678/0.72754. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67548/0.72849. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67669/0.72940. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67546/0.73022. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 0.67587/0.73133. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67469/0.73261. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67495/0.73254. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67469/0.73314. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67374/0.73300. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67319/0.73445. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.67204/0.73472. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66914/0.73709. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67286/0.73759. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67161/0.73850. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66902/0.73924. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66805/0.74045. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.67011/0.74028. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66983/0.74062. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66916/0.74098. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66779/0.74307. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66692/0.74324. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66766/0.74319. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66507/0.74335. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66720/0.74444. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66381/0.74468. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66590/0.74666. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66447/0.74760. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66457/0.74849. Took 0.24 sec\n",
      "Epoch 90, Loss(train/val) 0.66609/0.74846. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66594/0.74797. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66290/0.74888. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66229/0.74884. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65994/0.75017. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66181/0.75042. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.66186/0.74973. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65981/0.75479. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65643/0.75384. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65881/0.75445. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69564/0.69837. Took 0.31 sec\n",
      "Epoch 1, Loss(train/val) 0.69455/0.69451. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69384/0.69180. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69354/0.68986. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69326/0.68851. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69250/0.68781. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69223/0.68713. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69179/0.68710. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69273/0.68699. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69221/0.68672. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69174/0.68633. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69137/0.68617. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69182/0.68598. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69152/0.68634. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69162/0.68654. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69199/0.68683. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69057/0.68693. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69108/0.68700. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.69048/0.68692. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.69095/0.68701. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.69053/0.68700. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.69041/0.68697. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68893/0.68712. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.69031/0.68774. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68999/0.68754. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68937/0.68777. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68941/0.68767. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68835/0.68736. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68890/0.68836. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68773/0.68886. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68818/0.68930. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68816/0.68973. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68822/0.68973. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68638/0.69015. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68556/0.69044. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68679/0.69078. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68584/0.69181. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68586/0.69280. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68744/0.69434. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68654/0.69462. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68583/0.69346. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68604/0.69484. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68465/0.69563. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68312/0.69506. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68254/0.69609. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68253/0.69652. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68203/0.69772. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68294/0.69818. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68294/0.69775. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68273/0.70012. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68239/0.70208. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68279/0.70180. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68080/0.70058. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67961/0.70065. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67998/0.70119. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67918/0.70311. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67762/0.70179. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67748/0.70215. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67862/0.70434. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67801/0.70307. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67769/0.70411. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67640/0.70392. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67796/0.70237. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67637/0.70559. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67831/0.70418. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67359/0.70384. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67382/0.70429. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67144/0.70535. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67238/0.70698. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66912/0.70640. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67204/0.70677. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66984/0.70675. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66833/0.70644. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66596/0.70836. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66792/0.70880. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66779/0.70822. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66722/0.71248. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66458/0.71051. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66197/0.70902. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66437/0.71028. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66351/0.70926. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66020/0.71213. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66075/0.71414. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66026/0.71126. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65933/0.71497. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66298/0.71081. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65777/0.71066. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65826/0.71228. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65698/0.71159. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65690/0.71273. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65663/0.71350. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65165/0.71216. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65653/0.71267. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65195/0.71465. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65198/0.71413. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65153/0.71091. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65366/0.71162. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64906/0.71349. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65306/0.71226. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64905/0.71573. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69513/0.69201. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69236/0.69574. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69150/0.69819. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69148/0.69961. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69147/0.70051. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69063/0.70119. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69111/0.70209. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69046/0.70272. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69088/0.70319. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68952/0.70377. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69020/0.70442. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69030/0.70466. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68876/0.70551. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68927/0.70603. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68883/0.70642. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68902/0.70699. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68907/0.70727. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68903/0.70768. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68840/0.70829. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68864/0.70895. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68769/0.70948. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68844/0.71002. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68808/0.71038. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68814/0.71081. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68752/0.71138. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68711/0.71173. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68795/0.71208. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68677/0.71248. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68748/0.71263. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68661/0.71269. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68612/0.71279. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68658/0.71332. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68675/0.71336. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68640/0.71335. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68631/0.71316. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68564/0.71326. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68500/0.71372. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68571/0.71414. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68549/0.71412. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68457/0.71401. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68530/0.71419. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68443/0.71400. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68468/0.71424. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68476/0.71431. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68466/0.71344. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68504/0.71341. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68484/0.71392. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68415/0.71352. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68306/0.71358. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68241/0.71374. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68384/0.71382. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68335/0.71420. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68308/0.71430. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.68221/0.71502. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68424/0.71539. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68287/0.71531. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68162/0.71493. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.68159/0.71492. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68130/0.71490. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.68060/0.71519. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67957/0.71532. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.68176/0.71509. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.68019/0.71518. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.68008/0.71510. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67884/0.71457. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.68056/0.71492. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67952/0.71466. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67860/0.71448. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67869/0.71483. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67728/0.71413. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67691/0.71477. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67851/0.71485. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67725/0.71514. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67667/0.71393. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67686/0.71292. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67549/0.71478. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67544/0.71324. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67418/0.71390. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67211/0.71438. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.67375/0.71364. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67449/0.71473. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.67326/0.71429. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67260/0.71422. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.67067/0.71406. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.67035/0.71432. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.67190/0.71363. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66985/0.71485. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.67055/0.71465. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66860/0.71625. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66818/0.71588. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66724/0.71478. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66680/0.71646. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66709/0.71681. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66703/0.71579. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66478/0.71568. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66475/0.71667. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66347/0.71662. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66376/0.71850. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66367/0.71901. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.66067/0.71930. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69421/0.69971. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69262/0.69991. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69230/0.70000. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69185/0.69997. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69149/0.70031. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69052/0.70066. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69100/0.70092. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69000/0.70104. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68912/0.70134. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68907/0.70170. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68861/0.70198. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68880/0.70216. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68798/0.70245. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68759/0.70265. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68698/0.70248. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68574/0.70249. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68559/0.70236. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68434/0.70177. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68511/0.70160. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68331/0.70139. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68286/0.70117. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68192/0.70050. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68267/0.69972. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67924/0.69955. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67903/0.69879. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.67947/0.69771. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67796/0.69734. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67573/0.69651. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67429/0.69485. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67474/0.69341. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67436/0.69292. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67428/0.69289. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67292/0.69324. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.66926/0.69225. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.66889/0.69148. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66861/0.69057. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.66669/0.69069. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.66468/0.68915. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66514/0.68842. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66454/0.68825. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66303/0.68695. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.66207/0.68465. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66209/0.68694. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.65735/0.68400. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.65735/0.68416. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65736/0.68324. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.65655/0.68135. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65675/0.67969. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65227/0.67833. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65308/0.67818. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65305/0.67705. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65213/0.67607. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.64961/0.67601. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64874/0.67488. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64923/0.67579. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64858/0.67466. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64686/0.67236. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64720/0.67121. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64459/0.67132. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64387/0.66892. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.64262/0.67006. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64068/0.66904. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64082/0.66759. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64038/0.66621. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64018/0.66665. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64085/0.66656. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64001/0.66492. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63780/0.66547. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63192/0.66418. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.63383/0.66238. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.63072/0.66209. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.63146/0.66208. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63317/0.66356. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62350/0.66237. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63079/0.66196. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62972/0.66171. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62901/0.66262. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62535/0.66521. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62982/0.66292. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62440/0.66041. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62536/0.66025. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.62489/0.66014. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.62193/0.66183. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62091/0.65969. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62338/0.66095. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61722/0.66290. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61892/0.66408. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62011/0.66120. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61588/0.66354. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61243/0.66418. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61331/0.66458. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.60935/0.66210. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61788/0.66467. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61587/0.66497. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61007/0.66961. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61077/0.67087. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61286/0.66718. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60851/0.66936. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60534/0.66821. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60741/0.67200. Took 0.18 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.69595/0.69152. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69449/0.69114. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69372/0.69063. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69290/0.69052. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69285/0.69022. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69191/0.69013. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69163/0.68994. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69160/0.68960. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69151/0.68968. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69137/0.68988. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69088/0.68947. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69077/0.68931. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.69024/0.68931. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69023/0.68901. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69019/0.68904. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68923/0.68863. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68917/0.68876. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68938/0.68867. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68904/0.68874. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68873/0.68818. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68840/0.68854. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68855/0.68824. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68816/0.68847. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68782/0.68844. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68695/0.68802. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68551/0.68825. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68643/0.68722. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68534/0.68739. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68499/0.68759. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68456/0.68682. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68454/0.68672. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68344/0.68637. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68347/0.68535. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68196/0.68502. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68062/0.68349. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67963/0.68297. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67915/0.68249. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67800/0.68077. Took 0.24 sec\n",
      "Epoch 38, Loss(train/val) 0.67781/0.67911. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67713/0.67823. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67735/0.67616. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67559/0.67539. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67502/0.67407. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67134/0.67177. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67129/0.67121. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67074/0.66874. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67173/0.66774. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66894/0.66593. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66974/0.66605. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66601/0.66439. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66737/0.66305. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66536/0.66186. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66445/0.66087. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66430/0.65854. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66201/0.65770. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66213/0.65804. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66116/0.65751. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65943/0.65624. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65927/0.65461. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.66058/0.65302. Took 0.22 sec\n",
      "Epoch 60, Loss(train/val) 0.65746/0.65276. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65716/0.65422. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65727/0.65217. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65522/0.65138. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.65732/0.65191. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65764/0.65102. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65389/0.65035. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65461/0.64965. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65232/0.65036. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65145/0.64985. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65132/0.64963. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65020/0.64964. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65051/0.64927. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64894/0.65119. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64856/0.65008. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64550/0.65075. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64758/0.65083. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64673/0.64990. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64675/0.64951. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64438/0.65020. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64335/0.64893. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64413/0.65052. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64246/0.64955. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64102/0.64866. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64071/0.64874. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63891/0.64742. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63967/0.64793. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63907/0.65013. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63967/0.64848. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63593/0.64976. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63467/0.64884. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63730/0.64871. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.63366/0.64959. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63365/0.65029. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63604/0.65052. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62967/0.64939. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63262/0.65025. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62785/0.64911. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63022/0.64915. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63168/0.64845. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69673/0.69311. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69409/0.69216. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69457/0.69134. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69483/0.69054. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69307/0.68992. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69343/0.68945. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69333/0.68912. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69315/0.68868. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69321/0.68850. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69244/0.68814. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69279/0.68758. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69279/0.68718. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69226/0.68683. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69182/0.68648. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69176/0.68612. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.69179/0.68578. Took 0.22 sec\n",
      "Epoch 16, Loss(train/val) 0.69166/0.68563. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69157/0.68525. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.69174/0.68503. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.69155/0.68488. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.69122/0.68478. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69108/0.68447. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.69147/0.68422. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.69065/0.68417. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.69099/0.68364. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.69134/0.68372. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.69046/0.68361. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.69055/0.68407. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.69048/0.68393. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68996/0.68378. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.69019/0.68390. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.69089/0.68392. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68961/0.68384. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68928/0.68346. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68965/0.68356. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68899/0.68290. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68982/0.68293. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68799/0.68286. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68848/0.68258. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68828/0.68275. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68848/0.68288. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68872/0.68324. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68780/0.68294. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68787/0.68292. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68760/0.68343. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68683/0.68358. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68857/0.68392. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68796/0.68424. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68751/0.68497. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68750/0.68482. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68632/0.68404. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68613/0.68406. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68565/0.68422. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.68635/0.68445. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68611/0.68457. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.68586/0.68522. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68468/0.68498. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.68369/0.68640. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68376/0.68555. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.68413/0.68632. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.68368/0.68588. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.68214/0.68651. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.68245/0.68669. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.68122/0.68640. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.68234/0.68692. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.68175/0.68797. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.68222/0.68827. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.68160/0.68896. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.68072/0.68909. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67936/0.68951. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67926/0.68984. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67903/0.69055. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67943/0.69276. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67687/0.69176. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67751/0.69290. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67821/0.69161. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67836/0.69197. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.67596/0.69372. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67568/0.69604. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.67709/0.69524. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67589/0.69541. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.67710/0.69784. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67284/0.69742. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67593/0.69825. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.67439/0.70011. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.67073/0.70082. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.67252/0.70122. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.67168/0.70283. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.67112/0.70249. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.67175/0.70197. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.67048/0.70436. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.67332/0.70403. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66944/0.70578. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.67044/0.70741. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66915/0.70621. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66781/0.70836. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.66661/0.71071. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66788/0.71164. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66334/0.71404. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.66660/0.71415. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69359/0.69374. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69374/0.69372. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69328/0.69329. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69295/0.69294. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69323/0.69258. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69352/0.69221. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69275/0.69195. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69284/0.69172. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69226/0.69143. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69222/0.69129. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69162/0.69116. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69277/0.69101. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69210/0.69098. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69089/0.69088. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69147/0.69056. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69124/0.69033. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.69152/0.69017. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69116/0.69020. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.69021/0.68997. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.69066/0.68982. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.69046/0.68980. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69000/0.68949. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68953/0.68924. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.69016/0.68923. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68973/0.68927. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68970/0.68927. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68885/0.68926. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68952/0.68904. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68780/0.68924. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68856/0.68939. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68800/0.68941. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68696/0.68918. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68617/0.68920. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68660/0.68937. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68668/0.68954. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68508/0.68938. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68398/0.68927. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68476/0.68950. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68402/0.69019. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68369/0.68930. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68295/0.69063. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68420/0.69047. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68312/0.69010. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68295/0.68948. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68176/0.68991. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68193/0.69085. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68292/0.69128. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68060/0.69097. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67919/0.69035. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68020/0.68893. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67848/0.69020. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67942/0.69037. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67946/0.69181. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67882/0.69084. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67739/0.68904. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67532/0.68966. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67663/0.69090. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67480/0.69048. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67463/0.69128. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67436/0.69013. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67677/0.69325. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67405/0.69168. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67163/0.69052. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67238/0.69146. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67161/0.69175. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67087/0.69072. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66782/0.69414. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66887/0.69224. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66757/0.69098. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66784/0.69234. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66846/0.69322. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66688/0.69116. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66386/0.69391. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66245/0.69291. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66353/0.69384. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66634/0.69373. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66291/0.69312. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66324/0.69540. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66213/0.69483. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66099/0.69443. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66159/0.69512. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65894/0.69441. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65863/0.69611. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65856/0.69681. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65981/0.69938. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65522/0.69751. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65122/0.69787. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65700/0.69945. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65272/0.70035. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65520/0.69969. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64958/0.70009. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64952/0.70160. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64930/0.70224. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64688/0.70559. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64909/0.70677. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65139/0.70767. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64636/0.70767. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64403/0.70711. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64672/0.70641. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64471/0.70775. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69711/0.69013. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69440/0.68909. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69385/0.68980. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69407/0.69065. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69281/0.69121. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69294/0.69163. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69369/0.69182. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69265/0.69235. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69288/0.69308. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69211/0.69331. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69163/0.69365. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69091/0.69406. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69146/0.69421. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69220/0.69439. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69119/0.69469. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.69091/0.69540. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69156/0.69571. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69078/0.69580. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69091/0.69598. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.69044/0.69597. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.69051/0.69657. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.69083/0.69673. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.69006/0.69722. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68894/0.69704. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68977/0.69740. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68912/0.69710. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68856/0.69742. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68858/0.69717. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68903/0.69742. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68821/0.69745. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68912/0.69800. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68908/0.69852. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68837/0.69814. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68741/0.69775. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68858/0.69821. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68582/0.69805. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68621/0.69794. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68747/0.69782. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68528/0.69783. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68592/0.69879. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68519/0.69917. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68418/0.69765. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68330/0.69943. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68390/0.69906. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68308/0.69802. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68386/0.69839. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68100/0.69926. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68108/0.69989. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68093/0.69985. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.68177/0.70026. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68023/0.70132. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67886/0.70088. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68229/0.69950. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67953/0.70030. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67861/0.70150. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67808/0.70324. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67840/0.70080. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67653/0.70146. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67765/0.70232. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67754/0.70343. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67617/0.70470. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67552/0.70306. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67624/0.70262. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67564/0.70256. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67389/0.70487. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67360/0.70430. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67460/0.70708. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67399/0.70455. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67229/0.70298. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67589/0.70334. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67274/0.70520. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67281/0.70421. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67079/0.70558. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67159/0.70456. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67117/0.70657. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67240/0.70536. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67143/0.70507. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67073/0.70582. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67049/0.70579. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66842/0.70439. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67035/0.70770. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66761/0.70766. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66927/0.70522. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66722/0.70616. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66717/0.70727. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66614/0.70670. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66321/0.70498. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66472/0.70789. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66527/0.70682. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66430/0.70697. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66453/0.70692. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66210/0.70677. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66119/0.70654. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66056/0.70889. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66081/0.70623. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66084/0.70740. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66241/0.70989. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65994/0.70750. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65939/0.70972. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65910/0.70949. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69695/0.69273. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69200/0.69119. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69227/0.69099. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69185/0.69085. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69139/0.69070. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69100/0.69062. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69118/0.69053. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69129/0.69055. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69025/0.69047. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69042/0.69053. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68936/0.69062. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68940/0.69070. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68824/0.69061. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68848/0.69067. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68829/0.69073. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68883/0.69088. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68754/0.69107. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68761/0.69116. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68820/0.69110. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68610/0.69108. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68670/0.69101. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68485/0.69096. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68591/0.69090. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68315/0.69094. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68350/0.69074. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68158/0.69120. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68063/0.69112. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68043/0.69125. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68059/0.69159. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67814/0.69173. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67667/0.69289. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67498/0.69341. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67349/0.69407. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67259/0.69485. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67204/0.69568. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.66873/0.69684. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67054/0.69746. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.66608/0.69906. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66870/0.70034. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.66693/0.70027. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.66418/0.70147. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66419/0.70278. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66377/0.70322. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66182/0.70455. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.65867/0.70581. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65847/0.70683. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.65911/0.70747. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.65316/0.70917. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65664/0.71038. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.65538/0.71179. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65320/0.71280. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.65223/0.71424. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.64958/0.71421. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.64713/0.71480. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64782/0.71637. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.64345/0.71894. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64701/0.72042. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64696/0.72123. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64618/0.72128. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.64207/0.72235. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64023/0.72427. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64087/0.72711. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63835/0.72751. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.63740/0.72821. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63570/0.73094. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.63521/0.73108. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63339/0.73589. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63369/0.73591. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.62927/0.73502. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.62967/0.73584. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63110/0.73866. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.62382/0.73988. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62980/0.73815. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62075/0.73880. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62389/0.74301. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.62041/0.74275. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62072/0.74473. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.61860/0.74548. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.61850/0.74943. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61040/0.75137. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61438/0.75315. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61171/0.75461. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.60404/0.75800. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61789/0.75600. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.60492/0.76341. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.60403/0.76879. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.60338/0.76359. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.60394/0.76798. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60030/0.76843. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.60096/0.77089. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.59428/0.77227. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.59468/0.77173. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.59147/0.77508. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.58888/0.77713. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.59548/0.78017. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60021/0.77992. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.59119/0.77894. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.59266/0.78044. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.58810/0.78222. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.58629/0.78999. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69676/0.69568. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69301/0.69440. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69152/0.69427. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69189/0.69421. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69227/0.69417. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69159/0.69415. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69158/0.69413. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69159/0.69424. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69205/0.69421. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69089/0.69422. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69084/0.69424. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69090/0.69418. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69084/0.69419. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69116/0.69423. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69097/0.69416. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.69118/0.69416. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69039/0.69405. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69041/0.69403. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68945/0.69401. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68947/0.69391. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68986/0.69377. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68990/0.69377. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68963/0.69366. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68931/0.69353. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68944/0.69345. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68849/0.69328. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68879/0.69342. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68762/0.69318. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68804/0.69310. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68795/0.69311. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68827/0.69295. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68734/0.69270. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68615/0.69260. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68604/0.69189. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68580/0.69173. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68565/0.69141. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68594/0.69123. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68540/0.69109. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68527/0.69085. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68417/0.69046. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68441/0.69004. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68522/0.68981. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68475/0.68917. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68259/0.68814. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68190/0.68824. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68291/0.68725. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.68094/0.68716. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68042/0.68704. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68150/0.68724. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67887/0.68713. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67815/0.68589. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67878/0.68602. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67855/0.68589. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67908/0.68588. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67731/0.68490. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67716/0.68428. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67655/0.68370. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67557/0.68312. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67471/0.68248. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67460/0.68141. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67354/0.68101. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67250/0.68042. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67256/0.68104. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67302/0.68016. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67288/0.68002. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67244/0.67869. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67075/0.67776. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66965/0.67856. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66871/0.67821. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66932/0.67777. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67082/0.67997. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66928/0.67858. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66677/0.67998. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66587/0.67996. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66832/0.67909. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66309/0.67894. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66484/0.67827. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66423/0.67965. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66259/0.67965. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66090/0.67937. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66260/0.68000. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66056/0.68026. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66127/0.67985. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66043/0.68128. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66085/0.68255. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66055/0.67939. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65518/0.68128. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65867/0.68029. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65226/0.68156. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65241/0.68042. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65551/0.68039. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65186/0.68054. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65193/0.68081. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65038/0.68196. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65077/0.68066. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64806/0.68468. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64749/0.68163. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64839/0.68115. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64877/0.67983. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64806/0.68266. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69336/0.69298. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69102/0.69358. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69010/0.69418. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69010/0.69468. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68964/0.69494. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68969/0.69523. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68902/0.69557. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68870/0.69588. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68874/0.69613. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68908/0.69629. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68880/0.69661. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68838/0.69707. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68867/0.69755. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68853/0.69791. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68765/0.69844. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68859/0.69884. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68835/0.69909. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68692/0.69958. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68676/0.70005. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68683/0.70048. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68682/0.70085. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68656/0.70107. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68656/0.70174. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68536/0.70223. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68586/0.70261. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68526/0.70313. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68541/0.70336. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68453/0.70408. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68475/0.70469. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68427/0.70476. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68574/0.70517. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68281/0.70583. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68369/0.70663. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68377/0.70682. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68233/0.70712. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68299/0.70754. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68153/0.70774. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68032/0.70800. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68047/0.70886. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68155/0.70915. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68150/0.70933. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67878/0.71005. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67865/0.71066. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67850/0.71149. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67800/0.71217. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67631/0.71224. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67552/0.71323. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67848/0.71393. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67530/0.71407. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67530/0.71434. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67284/0.71471. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67386/0.71492. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67137/0.71551. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67038/0.71658. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66907/0.71729. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67126/0.71830. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66725/0.71913. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66834/0.72040. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66768/0.72130. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66928/0.72226. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66396/0.72283. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66616/0.72306. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66543/0.72405. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66319/0.72421. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66182/0.72549. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66238/0.72622. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65932/0.72746. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65951/0.72796. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66141/0.72989. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65764/0.73079. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65821/0.73214. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65597/0.73277. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65663/0.73434. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65518/0.73397. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65451/0.73547. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65293/0.73517. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65354/0.73731. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64927/0.73897. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64976/0.73981. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64685/0.74215. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64734/0.74368. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64599/0.74435. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64615/0.74558. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64760/0.74769. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64612/0.74841. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64346/0.75010. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64381/0.75029. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64305/0.75059. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64223/0.75303. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64159/0.75430. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63921/0.75569. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63806/0.75899. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64018/0.75834. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63760/0.76135. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63415/0.76178. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63374/0.76410. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63830/0.76300. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63621/0.76525. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63578/0.76621. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63138/0.76806. Took 0.18 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69629/0.69268. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69219/0.69485. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68928/0.69678. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.68858/0.69801. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68802/0.69878. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68794/0.69921. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68699/0.69950. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68706/0.69953. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68670/0.69910. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68689/0.69870. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68569/0.69836. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68599/0.69765. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68580/0.69771. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68618/0.69710. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68430/0.69702. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68519/0.69645. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68508/0.69610. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68363/0.69569. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68350/0.69523. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68423/0.69504. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68311/0.69498. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68367/0.69489. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68320/0.69523. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68251/0.69491. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68116/0.69448. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68103/0.69494. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68025/0.69541. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68057/0.69589. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68034/0.69561. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67955/0.69565. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67947/0.69525. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67910/0.69603. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67824/0.69633. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67827/0.69650. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67815/0.69682. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67720/0.69625. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67612/0.69633. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67581/0.69626. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67433/0.69752. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67508/0.69781. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67446/0.69799. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67663/0.69763. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67453/0.69774. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67308/0.69684. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67222/0.69782. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67156/0.69696. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67085/0.69817. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67107/0.69800. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67057/0.69925. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66944/0.69646. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66908/0.69764. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66800/0.69852. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66768/0.69768. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66782/0.69786. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66628/0.69770. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66466/0.69652. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66469/0.69661. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66776/0.69844. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66347/0.69851. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66496/0.69939. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66291/0.70015. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66229/0.69887. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66123/0.69827. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65951/0.69755. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66090/0.69960. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65899/0.69737. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65738/0.69794. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65757/0.69838. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65382/0.69889. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65656/0.69497. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65509/0.70043. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65374/0.69618. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65363/0.69841. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65423/0.69851. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65229/0.69904. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64843/0.69629. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64863/0.70007. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64947/0.69870. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65096/0.70110. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64688/0.69965. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64708/0.69772. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64645/0.69657. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64346/0.70131. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64238/0.69693. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64153/0.69630. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64246/0.69768. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64155/0.69826. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64355/0.69685. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64224/0.69874. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63792/0.69788. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64185/0.69832. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63516/0.70064. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63707/0.70072. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63483/0.69786. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63354/0.69830. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63337/0.69774. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63492/0.69937. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63132/0.70386. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62876/0.69924. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62659/0.70207. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69271/0.69512. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69052/0.69572. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68934/0.69606. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68972/0.69624. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68850/0.69665. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68838/0.69695. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68761/0.69709. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68876/0.69734. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68802/0.69781. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68714/0.69793. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68756/0.69828. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68630/0.69847. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68578/0.69864. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68589/0.69889. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68602/0.69911. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68515/0.69921. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68592/0.69929. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68500/0.69923. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68384/0.69928. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68406/0.69941. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68327/0.69962. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68363/0.69971. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68339/0.70002. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68391/0.69979. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68343/0.69988. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68187/0.69979. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68029/0.69963. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68172/0.69956. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68182/0.69895. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68067/0.69905. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67974/0.69977. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67920/0.70008. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68109/0.70006. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67863/0.70079. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67968/0.70024. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67865/0.69916. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67725/0.69896. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67852/0.69874. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67726/0.69893. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67589/0.69921. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67538/0.69932. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67480/0.70062. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67471/0.69967. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67404/0.70035. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67531/0.70069. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67338/0.69920. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67396/0.69868. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67109/0.69781. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67115/0.70002. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67176/0.70095. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66948/0.70178. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66890/0.70034. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66712/0.70167. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66855/0.69993. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66661/0.69918. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66626/0.70165. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66559/0.69983. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66479/0.70099. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66646/0.70018. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66460/0.70033. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66234/0.70050. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66629/0.69927. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66377/0.70011. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66181/0.69947. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65900/0.69852. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65691/0.70092. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65783/0.69728. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65540/0.69866. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65899/0.70224. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65588/0.70024. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65548/0.69860. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65513/0.69867. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65439/0.69777. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65228/0.70161. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65074/0.69766. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65333/0.69932. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65111/0.69980. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65067/0.69982. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64932/0.70287. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64965/0.70038. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64668/0.70323. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64764/0.70183. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64623/0.70617. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64585/0.70541. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64455/0.70557. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64231/0.71056. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64418/0.70355. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63976/0.71021. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64508/0.70452. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63619/0.70972. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63744/0.71103. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63495/0.71147. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63828/0.70698. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63660/0.71198. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63544/0.70999. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63185/0.71106. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63119/0.70641. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62857/0.70836. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63025/0.71792. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62884/0.71457. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69275/0.69802. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69057/0.69570. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69087/0.69465. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69089/0.69373. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69046/0.69342. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68955/0.69256. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68941/0.69228. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68926/0.69180. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68990/0.69155. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69026/0.69122. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68889/0.69144. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68944/0.69111. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68953/0.69069. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68907/0.69122. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68856/0.69101. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68874/0.69116. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68845/0.69100. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68814/0.69159. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68821/0.69083. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68809/0.69062. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68830/0.69032. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68850/0.69048. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68836/0.69056. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68750/0.69072. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68729/0.69043. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68696/0.69066. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68736/0.69064. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68677/0.69057. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68771/0.69085. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68633/0.69035. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68682/0.69085. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68713/0.69087. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68672/0.69088. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68719/0.69025. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68720/0.69088. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68686/0.69091. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68736/0.69057. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68660/0.69043. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68661/0.69032. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68533/0.69053. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68591/0.69067. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68522/0.69062. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68670/0.69021. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68505/0.69034. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68549/0.68929. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68526/0.68980. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68558/0.69007. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68541/0.69020. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68546/0.68987. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68483/0.68975. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68416/0.68988. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68407/0.69011. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68437/0.68982. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.68480/0.68992. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68411/0.69013. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.68407/0.68968. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68424/0.68981. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.68263/0.68962. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68401/0.68878. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.68334/0.68871. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.68271/0.68866. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.68319/0.68863. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.68315/0.68915. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.68233/0.68810. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.68230/0.68820. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.68165/0.68835. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.68166/0.68784. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.68192/0.68826. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.68231/0.68805. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.68138/0.68764. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.68104/0.68737. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.68092/0.68724. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.68098/0.68648. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67943/0.68689. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67929/0.68610. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67990/0.68662. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67956/0.68596. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.67778/0.68599. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.68117/0.68566. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67833/0.68619. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67682/0.68554. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.67707/0.68572. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67834/0.68536. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67748/0.68560. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.67561/0.68517. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.67759/0.68420. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.67678/0.68459. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.67551/0.68518. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.67575/0.68366. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.67593/0.68443. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.67602/0.68492. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.67635/0.68353. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.67467/0.68443. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.67467/0.68488. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.67268/0.68439. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.67372/0.68548. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.67472/0.68483. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.67337/0.68406. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.67263/0.68406. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.67090/0.68483. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69596/0.69397. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69374/0.69314. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69248/0.69293. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69195/0.69301. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69224/0.69313. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69196/0.69334. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69118/0.69354. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69134/0.69370. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69063/0.69390. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69035/0.69410. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69034/0.69425. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69080/0.69438. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69017/0.69456. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69068/0.69474. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69001/0.69490. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69005/0.69512. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69000/0.69522. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69020/0.69535. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68973/0.69547. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68968/0.69557. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.69002/0.69573. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68930/0.69588. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68916/0.69608. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68905/0.69637. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68912/0.69662. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68903/0.69665. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68825/0.69675. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68942/0.69682. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68865/0.69699. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68777/0.69710. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68747/0.69738. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68723/0.69746. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68694/0.69770. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68697/0.69810. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68611/0.69849. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68778/0.69870. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68599/0.69901. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68624/0.69936. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68550/0.69953. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68449/0.69965. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68504/0.69988. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68487/0.70019. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68487/0.70084. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68385/0.70123. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68308/0.70120. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68375/0.70160. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68247/0.70125. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68262/0.70139. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68108/0.70197. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68258/0.70201. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68071/0.70219. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.68056/0.70225. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68018/0.70191. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67783/0.70286. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67875/0.70297. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67928/0.70313. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67744/0.70356. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67666/0.70446. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67687/0.70490. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67359/0.70543. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67507/0.70666. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67431/0.70656. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67406/0.70787. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67195/0.70829. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67228/0.70829. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67056/0.71059. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67121/0.71145. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66839/0.71164. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67007/0.71187. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66864/0.71215. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66636/0.71352. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66729/0.71460. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66595/0.71585. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66609/0.71755. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66487/0.71816. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66344/0.71953. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66211/0.72009. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66251/0.72167. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65845/0.72304. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65705/0.72440. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65695/0.72632. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65898/0.72762. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65758/0.72872. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65476/0.72970. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65635/0.72969. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65333/0.73254. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65594/0.73258. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65714/0.73378. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65123/0.73586. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65064/0.73673. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64964/0.73761. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65472/0.73850. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65032/0.73926. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65119/0.74178. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64534/0.74296. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64764/0.74567. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64613/0.74671. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64280/0.74858. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64493/0.74914. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64607/0.75124. Took 0.19 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69575/0.69306. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69262/0.69631. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69210/0.69748. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69190/0.69817. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69160/0.69884. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69164/0.69939. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69046/0.69962. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69046/0.70036. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69040/0.70105. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69099/0.70147. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69060/0.70213. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69054/0.70234. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69035/0.70298. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68949/0.70342. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69000/0.70425. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.69014/0.70425. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68980/0.70455. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68869/0.70544. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68886/0.70611. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68849/0.70696. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68894/0.70740. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68866/0.70726. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68848/0.70743. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68798/0.70824. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68815/0.70839. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68736/0.70915. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68778/0.70954. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68759/0.70987. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68716/0.71062. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68608/0.71144. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68677/0.71220. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68636/0.71289. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68552/0.71323. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68598/0.71336. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68630/0.71335. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68475/0.71498. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68477/0.71538. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68563/0.71461. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68489/0.71528. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68482/0.71566. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68333/0.71609. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68239/0.71805. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68289/0.71839. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68312/0.71930. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68230/0.71959. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68232/0.72003. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68248/0.72098. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68170/0.72052. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68152/0.72198. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67877/0.72266. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67999/0.72423. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67995/0.72374. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67906/0.72491. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67954/0.72431. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67785/0.72454. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67718/0.72538. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67802/0.72604. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67556/0.72611. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67716/0.72687. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67511/0.72682. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67556/0.72792. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67452/0.72789. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67229/0.72813. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67221/0.72944. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67245/0.72909. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67116/0.73005. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67127/0.72961. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67138/0.72913. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67007/0.73041. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67035/0.72919. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66793/0.72994. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66760/0.73011. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66605/0.72995. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66560/0.73213. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66415/0.73246. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66383/0.73198. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66384/0.73451. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66201/0.73327. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66140/0.73382. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66123/0.73205. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65924/0.73264. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66067/0.73192. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66040/0.73258. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65730/0.73292. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65693/0.73064. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65306/0.73217. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65251/0.73043. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64944/0.73164. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65185/0.73389. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65064/0.73613. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64628/0.73362. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64676/0.73234. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64466/0.73414. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64285/0.73523. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64176/0.73853. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64069/0.73456. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64180/0.73832. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63464/0.73565. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63976/0.73503. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63351/0.73418. Took 0.18 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69293/0.69874. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.69752. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69212/0.69668. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69082/0.69573. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69163/0.69524. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68975/0.69436. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68988/0.69382. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69069/0.69348. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68922/0.69305. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68971/0.69256. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68841/0.69212. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69062/0.69153. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68882/0.69115. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68955/0.69082. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68802/0.69082. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68845/0.69037. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68795/0.68983. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68824/0.68985. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68746/0.69017. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68682/0.69030. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68728/0.69035. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68963/0.68997. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68709/0.68957. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68699/0.68954. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68705/0.68888. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68716/0.68887. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 0.68468/0.68873. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68753/0.68837. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68570/0.68813. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68640/0.68826. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68561/0.68815. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68537/0.68828. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68511/0.68846. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68680/0.68878. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68671/0.68894. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68687/0.68897. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68379/0.68854. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68597/0.68878. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68476/0.68889. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68518/0.68908. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68411/0.68844. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68297/0.68895. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68491/0.68904. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68395/0.68903. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68265/0.68833. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68362/0.68825. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68116/0.68863. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68215/0.68913. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68115/0.68900. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68172/0.68962. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68243/0.69033. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68321/0.69039. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68111/0.68979. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.68143/0.68847. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68036/0.68872. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.68061/0.68862. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67971/0.68868. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67709/0.68864. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67849/0.68867. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67959/0.68951. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67813/0.69039. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67802/0.68993. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67551/0.68947. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67699/0.68947. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67665/0.68989. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67762/0.69021. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67415/0.68980. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67397/0.68936. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67673/0.68965. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67448/0.68868. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67317/0.68996. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67250/0.68923. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67226/0.69017. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66754/0.69015. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67139/0.69176. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66858/0.69174. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67041/0.69210. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.67007/0.69395. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66859/0.69385. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66749/0.69193. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66916/0.69167. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66343/0.69312. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66299/0.69282. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66360/0.69314. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66191/0.69380. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66465/0.69332. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66121/0.69297. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66114/0.69299. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65815/0.69243. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65739/0.69627. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65413/0.69531. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65697/0.69382. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65383/0.69500. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65855/0.69560. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65198/0.69509. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65606/0.69634. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65220/0.69900. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65228/0.69751. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65131/0.69876. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65076/0.69806. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69445/0.69386. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69365/0.69343. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69265/0.69329. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69272/0.69327. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69311/0.69330. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69289/0.69332. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69218/0.69336. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69282/0.69331. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69167/0.69308. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69216/0.69308. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69126/0.69303. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69191/0.69306. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69108/0.69303. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69121/0.69304. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69117/0.69315. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68998/0.69322. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.69039/0.69324. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.69046/0.69337. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69066/0.69367. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68964/0.69389. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68921/0.69428. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68945/0.69429. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68861/0.69438. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68937/0.69486. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68833/0.69516. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68836/0.69532. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68871/0.69562. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68873/0.69614. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68819/0.69638. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68753/0.69639. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68785/0.69622. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68668/0.69630. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68600/0.69638. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68727/0.69636. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68647/0.69702. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.68640/0.69705. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68664/0.69686. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68542/0.69693. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.68565/0.69727. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68515/0.69785. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68585/0.69809. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68599/0.69841. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68358/0.69842. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68414/0.69894. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68524/0.69909. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68137/0.69936. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68430/0.69924. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68267/0.69918. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68159/0.69973. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68232/0.69984. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68081/0.70064. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68072/0.70058. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68042/0.70118. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.68055/0.70132. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67766/0.70109. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67815/0.70139. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67827/0.70097. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67559/0.70185. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67961/0.70273. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67765/0.70193. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67471/0.70242. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67500/0.70230. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67679/0.70349. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67552/0.70284. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67580/0.70187. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67597/0.70237. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67275/0.70239. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67093/0.70295. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67346/0.70496. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67241/0.70428. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67236/0.70497. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67099/0.70464. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67383/0.70477. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67033/0.70518. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67063/0.70498. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66792/0.70546. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66794/0.70608. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66644/0.70572. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66789/0.70529. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66789/0.70523. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66497/0.70759. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66677/0.70823. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66424/0.70819. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66616/0.70636. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66277/0.70743. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66236/0.70823. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66172/0.70794. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66321/0.70965. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66190/0.70949. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65628/0.71250. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66129/0.71108. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65865/0.71074. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65983/0.71316. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65860/0.71400. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65611/0.71441. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65634/0.71701. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65756/0.71700. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65749/0.71564. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65809/0.71492. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65430/0.71658. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69502/0.69395. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.69572. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69175/0.69773. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69085/0.69957. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69025/0.70118. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69042/0.70233. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68993/0.70363. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68959/0.70489. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68907/0.70561. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68866/0.70636. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68836/0.70698. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68906/0.70721. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68971/0.70767. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68864/0.70762. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68861/0.70836. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68785/0.70818. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68793/0.70857. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68822/0.70817. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68735/0.70800. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68824/0.70788. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68753/0.70836. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68704/0.70856. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68795/0.70849. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68736/0.70886. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68645/0.70878. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68619/0.70863. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68654/0.70849. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68658/0.70888. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68563/0.70895. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68631/0.70863. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68510/0.70867. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68490/0.70849. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68507/0.70896. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68485/0.70899. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68364/0.70883. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68392/0.70891. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68467/0.70843. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68360/0.70853. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68417/0.70850. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68334/0.70890. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68353/0.70876. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68267/0.70926. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68217/0.70894. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68279/0.70884. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68229/0.70939. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68102/0.70933. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68091/0.71015. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68046/0.71099. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68022/0.70999. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68134/0.71083. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67827/0.71097. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68014/0.71135. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67785/0.71101. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67756/0.71091. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67817/0.71195. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67643/0.71214. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67809/0.71136. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67663/0.71159. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67492/0.71143. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67509/0.71173. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67431/0.71150. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67388/0.71266. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67264/0.71173. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67320/0.71291. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67112/0.71123. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67148/0.71100. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67215/0.71068. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66993/0.71051. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66797/0.70856. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66817/0.71146. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66604/0.70951. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66454/0.70867. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66759/0.70856. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66377/0.70732. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66322/0.70969. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66122/0.71387. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66090/0.70924. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66077/0.70957. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65713/0.71054. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65812/0.71057. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65741/0.71216. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65970/0.70905. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65680/0.70949. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65371/0.70196. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65384/0.70793. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65431/0.70783. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65123/0.70832. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65027/0.71109. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65115/0.71024. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64881/0.71299. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.65015/0.70917. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64591/0.71204. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64265/0.71273. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64382/0.71385. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64668/0.71585. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64269/0.71484. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63989/0.71214. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63976/0.71795. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 0.64150/0.71815. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63557/0.72276. Took 0.20 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69451/0.69089. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69281/0.69082. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69265/0.69089. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69199/0.69075. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69195/0.69068. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69171/0.69072. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69098/0.69076. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69152/0.69111. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69044/0.69125. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68969/0.69146. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68876/0.69172. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68975/0.69230. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68932/0.69275. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68933/0.69328. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68815/0.69393. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68754/0.69466. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68771/0.69542. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68729/0.69594. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68705/0.69673. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68645/0.69739. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68727/0.69817. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68554/0.69879. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68742/0.69968. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68651/0.70037. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68666/0.70101. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68507/0.70167. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68619/0.70215. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68597/0.70224. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68586/0.70273. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68566/0.70331. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68520/0.70353. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68460/0.70406. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68400/0.70466. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68377/0.70545. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68430/0.70611. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68513/0.70672. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68347/0.70702. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68449/0.70746. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68414/0.70800. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68383/0.70832. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68270/0.70863. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68328/0.70902. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68316/0.70920. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68277/0.70947. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68186/0.71004. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68171/0.71023. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68145/0.71090. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68080/0.71134. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68211/0.71182. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67983/0.71249. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68089/0.71279. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67987/0.71306. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68028/0.71365. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68096/0.71399. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67992/0.71455. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67888/0.71499. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67973/0.71516. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67960/0.71585. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67871/0.71616. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67802/0.71710. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67809/0.71761. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67898/0.71823. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67826/0.71878. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67800/0.71945. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67746/0.72017. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67654/0.72083. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67605/0.72112. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67684/0.72113. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67518/0.72184. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67679/0.72123. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67397/0.72277. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67384/0.72348. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67373/0.72320. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67311/0.72404. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67344/0.72498. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67390/0.72573. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67191/0.72785. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.67094/0.72859. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67141/0.72990. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.67097/0.72930. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67194/0.72988. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67196/0.72897. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66947/0.73081. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66911/0.73200. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66810/0.73262. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.67064/0.73233. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66802/0.73404. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66739/0.73400. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66725/0.73415. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66619/0.73465. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66601/0.73746. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66790/0.73735. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66740/0.73857. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66505/0.73940. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66810/0.73978. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66623/0.73918. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66242/0.74097. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66627/0.74169. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66429/0.74248. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.66333/0.74226. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69430/0.69736. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69321/0.69782. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69219/0.69782. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69154/0.69796. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69168/0.69812. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69053/0.69842. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69014/0.69848. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69002/0.69878. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68984/0.69907. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68906/0.69945. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68923/0.69964. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68921/0.70021. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68901/0.70021. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68801/0.70051. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68810/0.70092. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68776/0.70127. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68699/0.70144. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68703/0.70171. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68725/0.70184. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68690/0.70199. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68666/0.70209. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68745/0.70227. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68495/0.70237. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68566/0.70295. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68469/0.70302. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68486/0.70336. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68504/0.70353. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68370/0.70399. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68247/0.70501. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68349/0.70542. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68365/0.70621. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68227/0.70671. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68066/0.70728. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68194/0.70786. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68055/0.70906. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67996/0.70999. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67919/0.71073. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67726/0.71138. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67975/0.71145. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67851/0.71195. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67690/0.71390. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67531/0.71507. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67590/0.71585. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67525/0.71646. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67622/0.71603. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67520/0.71875. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67438/0.71906. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67405/0.71958. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67239/0.71982. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67479/0.72078. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67268/0.72090. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67145/0.72260. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67141/0.72347. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67179/0.72242. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66950/0.72365. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67175/0.72272. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66853/0.72436. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66721/0.72676. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66852/0.72557. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66661/0.72692. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66417/0.72879. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66746/0.72845. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66521/0.73027. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66670/0.73042. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66421/0.73091. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66184/0.72959. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66352/0.73163. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66121/0.73250. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66272/0.73158. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66251/0.73457. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66088/0.73201. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66147/0.73607. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66239/0.73058. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65708/0.73570. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65793/0.73758. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65726/0.73767. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65763/0.73648. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65659/0.73969. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65714/0.73520. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65396/0.74062. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65471/0.73886. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65634/0.73885. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65279/0.73981. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65350/0.73996. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65146/0.74139. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65152/0.73992. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65150/0.73671. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65003/0.74103. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64790/0.74510. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64732/0.74984. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64575/0.74475. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64832/0.74641. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64499/0.75013. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64296/0.74971. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64782/0.74888. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64334/0.75254. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64353/0.75186. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64454/0.75370. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64024/0.75336. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64175/0.75283. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69294/0.69527. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69220/0.69580. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69109/0.69634. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69061/0.69675. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68980/0.69745. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68939/0.69830. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68805/0.69900. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68736/0.70020. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68706/0.70137. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68658/0.70252. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68628/0.70342. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68532/0.70450. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68471/0.70566. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68420/0.70650. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68387/0.70767. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68307/0.70877. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68205/0.71011. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68141/0.71137. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68173/0.71199. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68061/0.71302. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68101/0.71363. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68011/0.71433. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67864/0.71565. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67916/0.71710. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67970/0.71830. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67853/0.71974. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67803/0.72073. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67739/0.72123. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67755/0.72223. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67601/0.72332. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67681/0.72409. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67591/0.72493. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67507/0.72591. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67522/0.72718. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67518/0.72860. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67355/0.72923. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67100/0.73059. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67322/0.73119. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67319/0.73211. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67145/0.73240. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67250/0.73402. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66901/0.73465. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67116/0.73519. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66739/0.73658. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66795/0.73761. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66823/0.74079. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66804/0.74078. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66756/0.74300. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66819/0.74249. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66665/0.74479. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66648/0.74288. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66562/0.74402. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66290/0.74395. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66221/0.74643. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66374/0.74909. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66284/0.74649. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66155/0.75048. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66005/0.75078. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66121/0.75074. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66062/0.74981. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65806/0.75326. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65825/0.75121. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65857/0.75170. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65843/0.75284. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.65708/0.75639. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65721/0.75933. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65452/0.75852. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65577/0.76032. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65267/0.76102. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65252/0.76270. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65427/0.76257. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64958/0.76352. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65051/0.76422. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64733/0.76521. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64959/0.76654. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64919/0.76732. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64738/0.76842. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64618/0.77049. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64608/0.76924. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64489/0.77064. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64586/0.77143. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64716/0.77347. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64225/0.77573. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64226/0.77548. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63963/0.77482. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64019/0.77334. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63579/0.77454. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63886/0.77212. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63652/0.77906. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63875/0.78023. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63317/0.78154. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63230/0.78213. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63252/0.78155. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63048/0.78086. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62673/0.78637. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 0.62861/0.78795. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62904/0.78852. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62698/0.79080. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62391/0.78799. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62235/0.79083. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69463/0.69600. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.69596. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69141/0.69581. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69033/0.69558. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69037/0.69541. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68897/0.69518. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68821/0.69517. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68657/0.69494. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68647/0.69508. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68575/0.69503. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68452/0.69519. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68454/0.69545. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68451/0.69561. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68398/0.69576. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68330/0.69635. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68308/0.69683. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68222/0.69732. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68077/0.69753. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68246/0.69782. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68147/0.69768. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68147/0.69829. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68084/0.69824. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68134/0.69838. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68083/0.69834. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67890/0.69822. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67938/0.69825. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67833/0.69832. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67860/0.69837. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68002/0.69853. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67764/0.69864. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67854/0.69826. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67744/0.69792. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67699/0.69775. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67720/0.69759. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67837/0.69747. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67840/0.69733. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67619/0.69736. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67546/0.69702. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67527/0.69700. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67493/0.69682. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67632/0.69689. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67593/0.69648. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67471/0.69643. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67562/0.69628. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67400/0.69560. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67484/0.69565. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67424/0.69551. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67294/0.69519. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67323/0.69488. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67190/0.69438. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67294/0.69386. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67363/0.69360. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67165/0.69316. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67286/0.69314. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66982/0.69277. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67011/0.69193. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67056/0.69159. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67017/0.69153. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66730/0.69145. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66862/0.69111. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66827/0.69100. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66921/0.69076. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66762/0.69024. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66616/0.68964. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66696/0.68926. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66541/0.68988. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66836/0.68952. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66527/0.69019. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66497/0.68937. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66485/0.68967. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66468/0.68894. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66124/0.68855. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66152/0.68809. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66278/0.68882. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66218/0.68889. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65908/0.68845. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66151/0.68768. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65937/0.68770. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66013/0.68697. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65862/0.68665. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65859/0.68651. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65875/0.68737. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65564/0.68685. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65447/0.68514. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65566/0.68468. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65497/0.68332. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65671/0.68301. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65389/0.68277. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 0.65015/0.68292. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65336/0.68299. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65133/0.68372. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64954/0.68495. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64839/0.68411. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65073/0.68255. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64975/0.68275. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64768/0.68369. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65026/0.68327. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64706/0.68374. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64617/0.68400. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64618/0.68301. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69106/0.69476. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68984/0.69609. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68857/0.69739. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.68746/0.69893. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68663/0.70091. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68598/0.70252. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68486/0.70408. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68444/0.70575. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68308/0.70770. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68212/0.70970. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68163/0.71106. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68110/0.71237. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68200/0.71322. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68092/0.71388. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68031/0.71502. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67934/0.71596. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68046/0.71673. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.67965/0.71716. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67799/0.71796. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.67793/0.71859. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67892/0.71916. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67773/0.71972. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67706/0.71955. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67724/0.72078. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67727/0.72126. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67690/0.72167. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67715/0.72261. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67539/0.72314. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67666/0.72346. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67580/0.72356. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67559/0.72422. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67466/0.72509. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67491/0.72598. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67424/0.72616. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67585/0.72611. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67385/0.72702. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67466/0.72769. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67382/0.72832. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67488/0.72831. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67501/0.72847. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67247/0.72897. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67220/0.72961. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67441/0.73028. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67201/0.73067. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67163/0.73066. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67153/0.73176. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67074/0.73214. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67077/0.73279. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67109/0.73376. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66879/0.73463. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67025/0.73434. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66997/0.73453. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66958/0.73520. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66848/0.73624. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66809/0.73745. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66622/0.73794. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66928/0.73883. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66840/0.73904. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66624/0.73936. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66670/0.73989. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66639/0.74019. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66556/0.74146. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66339/0.74232. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66295/0.74365. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66625/0.74405. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66336/0.74447. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66237/0.74603. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66291/0.74596. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66181/0.74699. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66159/0.74741. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66176/0.74816. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66188/0.74876. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65998/0.75004. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65940/0.75106. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65746/0.75189. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66125/0.75210. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65725/0.75150. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65824/0.75185. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65356/0.75356. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65462/0.75388. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65560/0.75480. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65731/0.75558. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65638/0.75665. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65552/0.75729. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65325/0.75922. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65082/0.76009. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64987/0.76161. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65038/0.76236. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65075/0.76235. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65001/0.76340. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64820/0.76451. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64733/0.76640. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64771/0.76720. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64504/0.76786. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64749/0.76892. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64592/0.76911. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64349/0.77022. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64333/0.76841. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64499/0.76808. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64307/0.77020. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69501/0.69597. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69310/0.69610. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69222/0.69641. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69177/0.69711. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69092/0.69769. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68980/0.69893. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68971/0.69931. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68864/0.70040. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68883/0.70072. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68812/0.70235. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68733/0.70291. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68752/0.70401. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68635/0.70458. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68612/0.70509. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68695/0.70583. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68461/0.70595. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68511/0.70725. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68562/0.70748. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68537/0.70756. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68569/0.71022. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68480/0.70990. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68377/0.71155. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68437/0.71027. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68273/0.71203. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68192/0.71045. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68491/0.71221. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68271/0.71188. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68375/0.71292. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68206/0.71352. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68161/0.71389. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68164/0.71330. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68010/0.71491. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68067/0.71600. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68000/0.71601. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68090/0.71725. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67952/0.71815. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67929/0.72008. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67866/0.71976. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67891/0.72019. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67840/0.71988. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67719/0.72083. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67696/0.72086. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67532/0.72436. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67735/0.72179. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67639/0.72198. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67436/0.72308. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67300/0.72532. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67364/0.72461. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67321/0.72694. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67172/0.72718. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67227/0.73032. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67266/0.72763. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67182/0.72975. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66846/0.73211. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66853/0.72864. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66738/0.73378. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66575/0.73274. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66728/0.73448. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66500/0.73756. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66561/0.73867. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66372/0.73727. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66152/0.74006. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66161/0.74091. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66276/0.73974. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65961/0.74049. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65987/0.74327. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66014/0.74391. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65798/0.74844. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65829/0.74788. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65549/0.75083. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65404/0.74998. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65520/0.75291. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65079/0.75261. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65270/0.75466. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65161/0.75718. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65125/0.75679. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65223/0.75994. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64979/0.76283. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64874/0.76330. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64891/0.75899. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64890/0.76648. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64625/0.76717. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64665/0.76400. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64183/0.76637. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64640/0.77516. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64139/0.76976. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64264/0.77408. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64031/0.77910. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63618/0.77662. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64030/0.77762. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64056/0.77949. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63534/0.77844. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63498/0.77844. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63826/0.78469. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63436/0.78746. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63080/0.78666. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62727/0.78937. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63432/0.78967. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62702/0.78892. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62804/0.79720. Took 0.18 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69358/0.69365. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69344/0.69343. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69330/0.69320. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69222/0.69300. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69229/0.69279. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69257/0.69258. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69320/0.69257. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69224/0.69244. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69299/0.69232. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69181/0.69211. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69211/0.69209. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69266/0.69192. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.69137/0.69162. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69174/0.69117. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.69097/0.69079. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.69084/0.69012. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.69063/0.68997. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.69074/0.68936. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68971/0.68880. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68969/0.68795. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68869/0.68695. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68887/0.68548. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68794/0.68441. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68681/0.68446. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68596/0.68310. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68724/0.68215. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68559/0.68171. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68708/0.68107. Took 0.22 sec\n",
      "Epoch 28, Loss(train/val) 0.68485/0.67987. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.68645/0.67927. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68438/0.67990. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68484/0.67918. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68331/0.67877. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68302/0.67905. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68314/0.67773. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68467/0.67865. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68071/0.67769. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68271/0.67814. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68207/0.67697. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68140/0.67749. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67995/0.67617. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67903/0.67766. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67897/0.67653. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67883/0.67747. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67748/0.67673. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67974/0.67716. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67790/0.67800. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67768/0.67792. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67814/0.67664. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67668/0.67853. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67727/0.67817. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67527/0.67978. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67545/0.67887. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67462/0.67939. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67350/0.68003. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67313/0.68075. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67536/0.68223. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67532/0.68204. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67174/0.68358. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67184/0.68410. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66907/0.68490. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67030/0.68491. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66989/0.68580. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66976/0.68749. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66816/0.68768. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66690/0.68816. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66735/0.68887. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66690/0.69114. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66734/0.69091. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66669/0.69112. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66437/0.69203. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66571/0.69332. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66534/0.69552. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66316/0.69475. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66387/0.69851. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66244/0.69801. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66165/0.69993. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66188/0.70161. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65911/0.70236. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65605/0.70376. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66030/0.70612. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65644/0.70433. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65489/0.70719. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65657/0.70855. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65598/0.70811. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65524/0.71138. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65642/0.70926. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65385/0.71341. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65015/0.71547. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65236/0.71750. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64975/0.71691. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65179/0.72059. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64938/0.72123. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65021/0.72408. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64861/0.72067. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64576/0.72201. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64517/0.72343. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64464/0.72619. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64505/0.72743. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64027/0.72899. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69400/0.69320. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69221/0.69131. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69200/0.69140. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69173/0.69166. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69159/0.69187. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69108/0.69216. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69079/0.69248. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69042/0.69286. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69034/0.69314. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69003/0.69327. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68988/0.69354. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68930/0.69379. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68988/0.69410. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68873/0.69434. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68853/0.69454. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68842/0.69489. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68826/0.69532. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68859/0.69602. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68754/0.69630. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68789/0.69695. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68741/0.69731. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68728/0.69755. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68647/0.69810. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68647/0.69843. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68567/0.69885. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68552/0.69910. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68569/0.69927. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68513/0.69919. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68394/0.69987. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68331/0.70012. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68425/0.70020. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68407/0.70068. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68345/0.70150. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68291/0.70118. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68259/0.70162. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68065/0.70189. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68036/0.70169. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68054/0.70200. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67869/0.70314. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67891/0.70348. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67758/0.70383. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67781/0.70514. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67655/0.70484. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67542/0.70459. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67426/0.70488. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67552/0.70482. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67133/0.70410. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67309/0.70431. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67003/0.70244. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66751/0.70336. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67356/0.70426. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66964/0.70512. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66644/0.70498. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66690/0.70556. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66734/0.70648. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66246/0.70460. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66340/0.70416. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66081/0.70362. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66134/0.70308. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65996/0.70412. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65937/0.70342. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65620/0.70544. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65641/0.70670. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65655/0.70665. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65512/0.70395. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65298/0.70676. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65513/0.70697. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65281/0.70768. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65055/0.70892. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64946/0.71022. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65002/0.71032. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64559/0.71029. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64579/0.71212. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64889/0.71223. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64400/0.71747. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64034/0.71583. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63974/0.71669. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64098/0.71877. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63665/0.71836. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63678/0.71936. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63452/0.72226. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63656/0.72140. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63191/0.71672. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63295/0.72153. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62775/0.72401. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63359/0.72298. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62869/0.72200. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62825/0.72359. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63021/0.72327. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.62687/0.72422. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62787/0.72000. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62064/0.72762. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62502/0.72710. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62110/0.72706. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62431/0.72956. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62245/0.73325. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61807/0.72373. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62451/0.73533. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61197/0.73373. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61223/0.73551. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69433/0.69134. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69296/0.68877. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69195/0.68824. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69156/0.68838. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69092/0.68834. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69102/0.68827. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69124/0.68850. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68921/0.68858. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68960/0.68843. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68997/0.68856. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68973/0.68849. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68807/0.68852. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68774/0.68838. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68792/0.68875. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68726/0.68854. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68681/0.68841. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68676/0.68808. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68650/0.68805. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.68653/0.68807. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68621/0.68809. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68573/0.68816. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68459/0.68739. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68386/0.68729. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68404/0.68716. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68445/0.68677. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68363/0.68668. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68342/0.68678. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68244/0.68680. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68316/0.68703. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68148/0.68668. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68124/0.68638. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68135/0.68581. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68024/0.68596. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68115/0.68602. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67905/0.68607. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67951/0.68547. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67851/0.68543. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67882/0.68525. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67616/0.68449. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67889/0.68416. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67632/0.68405. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67704/0.68512. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67297/0.68511. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67277/0.68505. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67476/0.68382. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67415/0.68425. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67373/0.68411. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67182/0.68485. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67054/0.68424. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66979/0.68356. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66906/0.68200. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66796/0.68325. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67016/0.68313. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66906/0.68347. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66870/0.68410. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66833/0.68333. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66993/0.68250. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66702/0.68395. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66573/0.68468. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66625/0.68465. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66747/0.68548. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66472/0.68374. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66478/0.68410. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66253/0.68530. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66206/0.68496. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66217/0.68454. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65974/0.68553. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66104/0.68453. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66002/0.68435. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66000/0.68278. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65939/0.68336. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66046/0.68382. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65769/0.68426. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65587/0.68207. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65696/0.68290. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65281/0.68202. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65076/0.68346. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65207/0.68360. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65472/0.68453. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65020/0.68431. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65112/0.68472. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65184/0.68517. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64989/0.68643. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64808/0.68710. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64776/0.68646. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64491/0.68752. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64501/0.68704. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64630/0.68791. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64507/0.68872. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64144/0.68913. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64287/0.69013. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64283/0.69070. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64201/0.69051. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64360/0.68913. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64354/0.69130. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63941/0.68907. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63902/0.69100. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63928/0.69215. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63529/0.69096. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63555/0.69278. Took 0.19 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69527/0.70081. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69262/0.70376. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69184/0.70382. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69037/0.70404. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69060/0.70424. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68973/0.70484. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68920/0.70548. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68877/0.70589. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68819/0.70633. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68768/0.70691. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68707/0.70741. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68698/0.70795. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68539/0.70865. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68552/0.70846. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68576/0.70940. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68463/0.71010. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68485/0.71052. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68441/0.71049. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68464/0.71133. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68414/0.71169. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68406/0.71168. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68461/0.71266. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68304/0.71327. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68317/0.71342. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68224/0.71365. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68354/0.71376. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68311/0.71389. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68173/0.71448. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68135/0.71508. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68165/0.71561. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68050/0.71679. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68265/0.71646. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68078/0.71668. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68003/0.71741. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68025/0.71820. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68066/0.71844. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68050/0.71929. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68007/0.71966. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67945/0.71997. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67746/0.72045. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67866/0.72111. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67925/0.72149. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67818/0.72162. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67805/0.72250. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67710/0.72253. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67739/0.72293. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67750/0.72403. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67832/0.72356. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67566/0.72355. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67618/0.72428. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67530/0.72452. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67446/0.72620. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67416/0.72618. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67488/0.72635. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67265/0.72624. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67371/0.72776. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67582/0.72770. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67097/0.72812. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67212/0.72840. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67126/0.72968. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67161/0.72915. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67155/0.73049. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66747/0.73083. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67088/0.73164. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66904/0.73066. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66796/0.73280. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66885/0.73260. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66699/0.73296. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66783/0.73456. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66901/0.73401. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66365/0.73377. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66628/0.73378. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66401/0.73603. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66473/0.73590. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66618/0.73485. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66260/0.73562. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66136/0.73809. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65880/0.73848. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66198/0.73934. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66036/0.73925. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66032/0.74063. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66049/0.74096. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65864/0.73986. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66062/0.74200. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65673/0.74325. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65665/0.74301. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65700/0.74425. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65461/0.74257. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65361/0.74410. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65290/0.74726. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64922/0.74678. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65366/0.74806. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65064/0.74619. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65162/0.74746. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64610/0.75022. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65011/0.75051. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64773/0.75260. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64632/0.75034. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64445/0.75034. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64436/0.75531. Took 0.18 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69147/0.70042. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69129/0.70243. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69119/0.70298. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69050/0.70342. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68974/0.70437. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68963/0.70441. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68940/0.70439. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68903/0.70495. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68836/0.70410. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68730/0.70459. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68684/0.70557. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68677/0.70551. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68692/0.70515. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68595/0.70566. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68462/0.70528. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68416/0.70556. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68505/0.70684. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68468/0.70611. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68325/0.70640. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68229/0.70757. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68222/0.70584. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67916/0.70790. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68100/0.70636. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67947/0.70687. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67811/0.70716. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67869/0.70611. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67854/0.70754. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67818/0.70854. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67639/0.70663. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67618/0.70574. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67459/0.70695. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67269/0.70607. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67249/0.70604. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67068/0.70499. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.66961/0.70516. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.66833/0.70496. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66668/0.70596. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.66534/0.70579. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66702/0.70442. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.66535/0.70283. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66387/0.70279. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.65923/0.70333. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66031/0.70311. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.65860/0.70214. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65961/0.70253. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65987/0.70135. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65542/0.70078. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65490/0.70228. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65158/0.70311. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.65397/0.70313. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.65087/0.70498. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.65346/0.70635. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65374/0.70680. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.65229/0.70639. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64491/0.70897. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.64389/0.70578. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64750/0.70961. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64660/0.70766. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64623/0.70876. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.64648/0.70851. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64172/0.70769. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64232/0.71114. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64446/0.71247. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.63994/0.71460. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64112/0.71613. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.63756/0.71405. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63503/0.71673. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63300/0.71392. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63944/0.71762. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.63870/0.71910. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63759/0.72124. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.63715/0.72337. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63079/0.72363. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.63418/0.71916. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63461/0.72418. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63055/0.72535. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62620/0.73170. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62791/0.72955. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62840/0.72557. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62724/0.72347. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62705/0.72873. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.62056/0.72920. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62916/0.73327. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62459/0.73324. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62311/0.73597. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62027/0.73809. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61890/0.73796. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62140/0.73757. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61487/0.74082. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61658/0.74353. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61699/0.74532. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61606/0.74757. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.61848/0.74640. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61510/0.74926. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61273/0.74981. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.61865/0.74615. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.60978/0.74767. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.61115/0.75079. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61172/0.75072. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60893/0.74719. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69523/0.68660. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69357/0.68803. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69330/0.68887. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69281/0.68922. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69205/0.68938. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69200/0.68991. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69116/0.69048. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69129/0.69058. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69056/0.69105. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69045/0.69183. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68962/0.69244. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68945/0.69291. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68883/0.69342. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68847/0.69411. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68826/0.69439. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68819/0.69501. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68737/0.69547. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68694/0.69648. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68665/0.69668. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68677/0.69731. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68617/0.69783. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68634/0.69789. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68593/0.69828. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68515/0.69851. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68507/0.69890. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68491/0.69913. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68463/0.69971. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68387/0.69994. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68398/0.70015. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68379/0.70095. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68444/0.70120. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68202/0.70158. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68279/0.70168. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68143/0.70236. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68127/0.70301. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68111/0.70401. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68001/0.70431. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67895/0.70568. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67777/0.70537. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67923/0.70581. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67964/0.70633. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67736/0.70735. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67832/0.70734. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67606/0.70871. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67505/0.70984. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67534/0.70921. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67425/0.71054. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67471/0.71069. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67438/0.71076. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67394/0.71198. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67334/0.71270. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67317/0.71388. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67295/0.71317. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67062/0.71531. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66945/0.71706. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67044/0.71572. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66844/0.71746. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66926/0.71841. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66791/0.71940. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66650/0.71899. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66728/0.72163. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66661/0.72141. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66533/0.72024. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66499/0.72088. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66521/0.72385. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66472/0.72404. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66265/0.72433. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66339/0.72531. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66397/0.72376. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66159/0.72626. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66135/0.72498. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65871/0.72889. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65990/0.72695. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65977/0.72809. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65829/0.73013. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65861/0.73168. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65570/0.73118. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65693/0.73192. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65551/0.73195. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65678/0.73404. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65427/0.73317. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65347/0.73378. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65410/0.73636. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65393/0.73483. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65033/0.73483. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65329/0.73679. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64662/0.73613. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64622/0.74054. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64550/0.74030. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64547/0.74126. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64776/0.74100. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64552/0.74123. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64466/0.73822. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64501/0.74221. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64074/0.74273. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64332/0.74464. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63894/0.74487. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63864/0.74366. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63973/0.74668. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63494/0.74520. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69397/0.69276. Took 0.30 sec\n",
      "Epoch 1, Loss(train/val) 0.69312/0.69281. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69276/0.69290. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69233/0.69300. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69109/0.69326. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69141/0.69359. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69161/0.69408. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68953/0.69460. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68944/0.69521. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68905/0.69616. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.69038/0.69688. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68771/0.69764. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68818/0.69846. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68853/0.69907. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68773/0.69955. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68650/0.70028. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68678/0.70118. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68689/0.70187. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68553/0.70241. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68647/0.70296. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68524/0.70344. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68506/0.70374. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68622/0.70416. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68526/0.70430. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68384/0.70479. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68517/0.70520. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68465/0.70510. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68338/0.70550. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68333/0.70551. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68375/0.70585. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68491/0.70598. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68260/0.70613. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68375/0.70638. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68233/0.70681. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68133/0.70722. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68074/0.70767. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68207/0.70786. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67960/0.70821. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68049/0.70877. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67997/0.70920. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67856/0.70930. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67929/0.70980. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67997/0.70988. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67860/0.70954. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67696/0.70951. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67777/0.71000. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67895/0.71027. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67677/0.71012. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67868/0.71016. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67758/0.71000. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67651/0.71012. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67693/0.71026. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67562/0.71043. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67419/0.71057. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67322/0.71098. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67316/0.71119. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67164/0.71119. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67352/0.71064. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67314/0.70986. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67251/0.71060. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67001/0.71067. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67064/0.71148. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66966/0.71084. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66802/0.71136. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67011/0.71137. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66818/0.71120. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66787/0.71130. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66930/0.71166. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66735/0.71173. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66729/0.71092. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66577/0.71049. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66760/0.70980. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66674/0.71045. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66659/0.71104. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66405/0.71069. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66425/0.71006. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66092/0.71064. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66201/0.71016. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66377/0.70916. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66351/0.71064. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65825/0.70970. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66034/0.70988. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65946/0.70947. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65765/0.70958. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65933/0.71098. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65918/0.71124. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65466/0.70972. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65619/0.70963. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65622/0.70920. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65426/0.71128. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65347/0.70825. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65541/0.71056. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65252/0.70811. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65407/0.70898. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65120/0.71307. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64968/0.71075. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65010/0.70910. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65153/0.71135. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64658/0.71011. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64594/0.70976. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69455/0.69315. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69115/0.69340. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69148/0.69362. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69032/0.69386. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68978/0.69420. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68919/0.69466. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68892/0.69518. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68904/0.69557. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68841/0.69602. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68686/0.69650. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68701/0.69723. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68582/0.69783. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68511/0.69832. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68428/0.69909. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68464/0.69992. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68427/0.70061. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68418/0.70107. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68248/0.70186. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68258/0.70242. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68081/0.70307. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68133/0.70374. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67952/0.70462. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67995/0.70412. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67960/0.70597. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67981/0.70757. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67955/0.70950. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67841/0.70902. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67745/0.70990. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67725/0.71132. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67598/0.71228. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67620/0.71115. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67427/0.71146. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67538/0.71347. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67513/0.71510. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67553/0.71195. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67272/0.71425. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67320/0.71375. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67305/0.71456. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67223/0.71522. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67006/0.71519. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66955/0.71455. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67047/0.71258. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67016/0.71602. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66833/0.71543. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66943/0.71501. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66896/0.71446. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66830/0.71351. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66576/0.71529. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66625/0.71199. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66555/0.71583. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66178/0.71403. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66919/0.71244. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66230/0.71229. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66304/0.71232. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66368/0.71225. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66120/0.71392. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66186/0.71308. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66231/0.71135. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66111/0.71239. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65938/0.71230. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65990/0.71320. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65814/0.71213. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65847/0.71110. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65653/0.71356. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65308/0.71210. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65553/0.71192. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65291/0.71113. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65213/0.71173. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65263/0.71367. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64930/0.71255. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65234/0.71339. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64995/0.71047. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64940/0.71052. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64708/0.71009. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64707/0.71035. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64687/0.71179. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64259/0.70965. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63949/0.71007. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63912/0.70813. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63961/0.70858. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63802/0.71007. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63968/0.70991. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63625/0.70913. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63254/0.71218. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63543/0.71143. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63596/0.71102. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63051/0.70833. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63396/0.70968. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63214/0.71212. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62892/0.71140. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62695/0.71225. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62848/0.71244. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62969/0.71496. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62570/0.71852. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62445/0.72055. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62178/0.71786. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62345/0.71976. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62349/0.71898. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62037/0.72288. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61423/0.72298. Took 0.18 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69617/0.69284. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69230/0.69325. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69200/0.69356. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69209/0.69410. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69104/0.69431. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68993/0.69560. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69011/0.69634. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68985/0.69653. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68943/0.69727. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68948/0.69800. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68970/0.69769. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68789/0.69870. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68875/0.69879. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68813/0.69876. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68901/0.69950. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68829/0.69950. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68789/0.69946. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68737/0.69971. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68733/0.70009. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68762/0.70032. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68728/0.69954. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68686/0.70021. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68698/0.70066. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68603/0.69997. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68653/0.70004. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68673/0.70002. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68697/0.69943. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68685/0.69891. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68595/0.69901. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68621/0.69874. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68534/0.69927. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68558/0.69908. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68516/0.69900. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68554/0.69928. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68562/0.69882. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68498/0.69893. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68505/0.69949. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68438/0.69803. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68433/0.69904. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68379/0.69875. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68490/0.69815. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68368/0.69884. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68441/0.69813. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68267/0.69764. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68232/0.69867. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68187/0.69746. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68173/0.69809. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68089/0.69704. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68170/0.69827. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68175/0.69766. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67970/0.69811. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68111/0.69678. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68058/0.69867. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68109/0.69660. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67913/0.69787. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67881/0.69795. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67821/0.69774. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67846/0.69821. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67868/0.69864. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67576/0.69768. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67623/0.69847. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67689/0.70041. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67455/0.69898. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67644/0.69829. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67486/0.69763. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67414/0.69972. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67398/0.69996. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67303/0.69894. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67322/0.69785. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67316/0.69939. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67224/0.69928. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67062/0.69923. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67054/0.69955. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67166/0.70046. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67012/0.69974. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66993/0.70008. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66911/0.70202. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66755/0.69998. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66589/0.70196. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66769/0.70205. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66644/0.70095. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66476/0.70079. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66441/0.70114. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66416/0.70193. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66521/0.70201. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66007/0.70086. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65980/0.70080. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65937/0.70168. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65801/0.70442. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65705/0.70444. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65748/0.70409. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65639/0.70388. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65433/0.70428. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65573/0.70845. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65097/0.70573. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65531/0.70845. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65279/0.70504. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65057/0.70562. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64912/0.70999. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64749/0.70597. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69803/0.68954. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69294/0.69321. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69339/0.69372. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69271/0.69399. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69284/0.69430. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69195/0.69479. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69167/0.69517. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69153/0.69540. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69089/0.69604. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69065/0.69629. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69072/0.69629. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69044/0.69694. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69003/0.69678. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68931/0.69682. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68992/0.69673. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68927/0.69738. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68889/0.69772. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68930/0.69724. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68869/0.69776. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68824/0.69767. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68846/0.69756. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68814/0.69795. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68893/0.69776. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68847/0.69770. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68764/0.69730. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68714/0.69729. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68707/0.69716. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68672/0.69691. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68662/0.69698. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68554/0.69662. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68659/0.69646. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68556/0.69597. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.68572/0.69570. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68533/0.69589. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68535/0.69514. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68535/0.69459. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68356/0.69512. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68395/0.69484. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68399/0.69462. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68337/0.69475. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68338/0.69352. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68380/0.69444. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68245/0.69392. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68385/0.69311. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68288/0.69292. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68096/0.69300. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68075/0.69203. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67897/0.69198. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67944/0.69265. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67960/0.69179. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67839/0.69274. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67947/0.69267. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67716/0.69228. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67723/0.69158. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67444/0.69200. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67522/0.69216. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67577/0.69210. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67593/0.69068. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67460/0.69192. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67426/0.69147. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67270/0.69264. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67380/0.69199. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67374/0.69230. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67273/0.69121. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67352/0.69051. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67213/0.69062. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67199/0.69162. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67059/0.69092. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66917/0.69171. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66870/0.69283. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66853/0.69260. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66706/0.69284. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66559/0.69318. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66674/0.69276. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66596/0.69456. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66491/0.69504. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66575/0.69579. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66540/0.69609. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66381/0.69680. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.65859/0.69679. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66307/0.69731. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66078/0.69739. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66065/0.69585. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66057/0.69648. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66095/0.69606. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66089/0.69679. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66043/0.69787. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65593/0.69940. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65666/0.70071. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65688/0.70095. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65640/0.70209. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65764/0.70421. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65506/0.70338. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65571/0.70392. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65342/0.70398. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65436/0.70398. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64902/0.70540. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65349/0.70580. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65026/0.70607. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65093/0.70679. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69329/0.69749. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69189/0.69730. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69019/0.69731. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68975/0.69750. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68944/0.69763. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68915/0.69802. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68782/0.69824. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68766/0.69817. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68826/0.69860. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68765/0.69880. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68719/0.69900. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68744/0.69929. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68596/0.69976. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68675/0.70006. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68606/0.70039. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68611/0.70089. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68560/0.70108. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68626/0.70110. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68513/0.70137. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68399/0.70157. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68447/0.70163. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68430/0.70183. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68460/0.70190. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68344/0.70220. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68287/0.70230. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68462/0.70238. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68219/0.70262. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68304/0.70328. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68212/0.70314. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68237/0.70348. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68255/0.70385. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68112/0.70407. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68137/0.70471. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68022/0.70469. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67958/0.70529. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68045/0.70526. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67961/0.70568. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68137/0.70623. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67792/0.70691. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67926/0.70770. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67956/0.70848. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67832/0.70904. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67886/0.70895. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67835/0.70957. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67834/0.70886. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67710/0.70973. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67731/0.71042. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67629/0.71130. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67633/0.71157. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67555/0.71367. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67431/0.71397. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67502/0.71437. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67586/0.71385. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67573/0.71347. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67339/0.71528. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67401/0.71548. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67346/0.71533. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67376/0.71521. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67458/0.71562. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67285/0.71590. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67257/0.71678. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67214/0.71710. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67267/0.71628. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67160/0.71615. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67054/0.71609. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67069/0.71652. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67097/0.71697. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66840/0.71862. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67159/0.71774. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67238/0.71940. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67042/0.71867. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66696/0.71959. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66908/0.72070. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66826/0.72044. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66828/0.71941. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66722/0.71965. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66903/0.71963. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66858/0.72169. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66470/0.72037. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66637/0.72171. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66651/0.72061. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66590/0.72226. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66757/0.72199. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66605/0.72331. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66240/0.72465. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66482/0.72441. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66399/0.72308. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66225/0.72383. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66461/0.72287. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66249/0.72384. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66146/0.72193. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66166/0.72294. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66240/0.72251. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66068/0.72304. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65916/0.72509. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65925/0.72383. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65763/0.72406. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66072/0.72383. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65775/0.72517. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65674/0.72371. Took 0.18 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69895/0.69251. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69393/0.69479. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69311/0.69500. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69097/0.69540. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69036/0.69578. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69020/0.69640. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68922/0.69667. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68833/0.69689. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68728/0.69742. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68696/0.69761. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68622/0.69802. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68613/0.69852. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68529/0.69852. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68629/0.69862. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68552/0.69853. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68489/0.69894. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68504/0.69868. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68464/0.69883. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68479/0.69898. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68344/0.69958. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68367/0.70006. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68411/0.69952. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68345/0.70006. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68161/0.69990. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68284/0.70097. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68374/0.70029. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68227/0.70016. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68224/0.70076. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68094/0.70097. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68213/0.70153. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68238/0.70147. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68121/0.70136. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68038/0.70162. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68196/0.70210. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68115/0.70265. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68001/0.70336. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67957/0.70379. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68015/0.70401. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68001/0.70463. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67862/0.70569. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67845/0.70561. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67966/0.70618. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67936/0.70734. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67765/0.70702. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67903/0.70741. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67913/0.70761. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67976/0.70794. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67704/0.70859. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67779/0.70971. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67694/0.71003. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67706/0.71042. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67725/0.71121. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67614/0.71163. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67686/0.71207. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67702/0.71324. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67625/0.71263. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67598/0.71251. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67667/0.71256. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67495/0.71326. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67372/0.71418. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67624/0.71483. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67329/0.71488. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67476/0.71504. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67375/0.71643. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67410/0.71658. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67247/0.71597. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67279/0.71637. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67332/0.71682. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67269/0.71684. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67357/0.71632. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67211/0.71751. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67059/0.71795. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67138/0.71776. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67161/0.71809. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67036/0.71996. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67166/0.71934. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66863/0.72058. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.67019/0.72039. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67013/0.72083. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66936/0.72249. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66795/0.72202. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66828/0.72316. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66853/0.72309. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66861/0.72334. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66685/0.72245. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66690/0.72358. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66543/0.72419. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66546/0.72512. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66447/0.72426. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66258/0.72566. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66467/0.72744. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66248/0.72727. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66237/0.72763. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66228/0.72819. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65866/0.73073. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.66169/0.73090. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66066/0.73189. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65896/0.73090. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65849/0.73029. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65996/0.73199. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69381/0.68818. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69305/0.68886. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69206/0.68913. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69203/0.68900. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69087/0.68888. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69096/0.68886. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68996/0.68855. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68930/0.68816. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68826/0.68774. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68829/0.68774. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68792/0.68760. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68646/0.68690. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68581/0.68724. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68613/0.68691. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68476/0.68699. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68463/0.68722. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68425/0.68733. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68341/0.68798. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68257/0.68768. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68282/0.68840. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68249/0.68918. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68065/0.68902. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68185/0.68915. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68210/0.68998. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68012/0.69045. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68000/0.69129. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67980/0.69134. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67998/0.69231. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67878/0.69216. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67805/0.69281. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67812/0.69337. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67788/0.69416. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67704/0.69521. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67763/0.69561. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67567/0.69560. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67730/0.69547. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67657/0.69672. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67553/0.69722. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67460/0.69720. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67442/0.69898. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67295/0.69914. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67439/0.69946. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67223/0.70056. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67181/0.70136. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67237/0.70143. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67043/0.70214. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67212/0.70142. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67233/0.70230. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67020/0.70301. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67150/0.70243. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67081/0.70448. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66929/0.70451. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66681/0.70476. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66839/0.70549. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66519/0.70728. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66874/0.70796. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66370/0.70772. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66584/0.70821. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66479/0.70901. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66485/0.71017. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66463/0.71167. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66434/0.70982. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66239/0.71277. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66291/0.71245. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66113/0.71313. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66057/0.71371. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65940/0.71344. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66055/0.71502. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65865/0.71473. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65575/0.71515. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65658/0.71650. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65852/0.71588. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65778/0.71725. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65377/0.71778. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65720/0.71823. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65486/0.71962. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65436/0.71992. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65267/0.72153. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65198/0.72086. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65243/0.72137. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65149/0.72079. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65106/0.72174. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64795/0.72113. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64651/0.72366. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64962/0.72586. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64866/0.72639. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64466/0.72597. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64684/0.72804. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64370/0.72738. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64225/0.72715. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64326/0.72711. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64477/0.72728. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.63947/0.72797. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64011/0.72753. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64080/0.72701. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63775/0.73100. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63754/0.73270. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63888/0.73324. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63871/0.73067. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63623/0.73524. Took 0.20 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69601/0.69877. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69205/0.69851. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69079/0.69862. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69103/0.69891. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68949/0.69919. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68798/0.69975. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68739/0.70021. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68738/0.70071. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68653/0.70112. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68589/0.70133. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68611/0.70170. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68609/0.70211. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68516/0.70233. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68527/0.70251. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68588/0.70244. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68481/0.70218. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68434/0.70215. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68419/0.70242. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68278/0.70289. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68319/0.70305. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68394/0.70316. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68288/0.70325. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68309/0.70357. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68278/0.70363. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68161/0.70399. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68207/0.70402. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68193/0.70403. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68166/0.70417. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68058/0.70455. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68088/0.70463. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68175/0.70444. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68087/0.70455. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68062/0.70464. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68084/0.70483. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67961/0.70492. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67958/0.70533. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67806/0.70550. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67854/0.70516. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67666/0.70565. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67815/0.70624. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67727/0.70644. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67509/0.70684. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67489/0.70727. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67486/0.70777. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67501/0.70824. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67520/0.70831. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67451/0.70950. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67209/0.70913. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67214/0.70898. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67364/0.70873. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67210/0.70967. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67118/0.70995. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67015/0.71060. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67060/0.71062. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67004/0.71079. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67002/0.71137. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67082/0.71238. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66974/0.71318. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66717/0.71437. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66695/0.71508. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66861/0.71519. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66861/0.71539. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66970/0.71617. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66612/0.71665. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66550/0.71717. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66495/0.71771. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66528/0.71845. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66378/0.71967. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66481/0.72149. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66351/0.72176. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66371/0.72097. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66282/0.72224. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66266/0.72362. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66295/0.72389. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66076/0.72488. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66127/0.72570. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65840/0.72616. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66025/0.72648. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65888/0.72690. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65795/0.72770. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65894/0.72771. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65627/0.72781. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65363/0.72940. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65801/0.73249. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65823/0.73232. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65592/0.73299. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65411/0.73355. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65423/0.73438. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65276/0.73664. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65188/0.73792. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65539/0.73779. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65133/0.73890. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64930/0.73937. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64914/0.73994. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64919/0.74016. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64997/0.74136. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64891/0.74286. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64759/0.74247. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64737/0.74182. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64577/0.74575. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69532/0.69925. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69304/0.69982. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69206/0.70016. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69089/0.70050. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69034/0.70073. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68989/0.70089. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68966/0.70095. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68873/0.70113. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68869/0.70086. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68790/0.70066. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68755/0.70052. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68697/0.69991. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68692/0.69954. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68652/0.69913. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68648/0.69891. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68617/0.69881. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68581/0.69860. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68578/0.69832. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68501/0.69759. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68460/0.69774. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68447/0.69735. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68473/0.69723. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68340/0.69674. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68407/0.69651. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68482/0.69634. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68409/0.69614. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68335/0.69576. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68334/0.69549. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68347/0.69511. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68463/0.69521. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68411/0.69520. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68330/0.69503. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68155/0.69509. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68241/0.69498. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68271/0.69536. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68123/0.69527. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68261/0.69524. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68243/0.69494. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68222/0.69493. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68189/0.69529. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68159/0.69513. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68101/0.69455. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68110/0.69506. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68091/0.69532. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67990/0.69534. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68078/0.69587. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67933/0.69626. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68083/0.69599. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68012/0.69581. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67926/0.69649. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67846/0.69741. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67688/0.69695. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67913/0.69733. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67871/0.69779. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67735/0.69858. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67719/0.69903. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67726/0.69894. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67849/0.69848. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67595/0.69845. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67518/0.69885. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67604/0.69898. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67542/0.69953. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67618/0.69965. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67533/0.69947. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67272/0.69947. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67453/0.70044. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67323/0.70017. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67489/0.70031. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67256/0.70117. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67187/0.70088. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67386/0.70189. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67184/0.70297. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67097/0.70371. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67186/0.70367. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67129/0.70307. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67192/0.70345. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66874/0.70365. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.67013/0.70369. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66831/0.70417. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66601/0.70607. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66672/0.70611. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66669/0.70759. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66703/0.70884. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66290/0.70996. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66503/0.70944. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66863/0.70963. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66392/0.70994. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66433/0.70959. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66403/0.70989. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66624/0.70965. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66470/0.70995. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66049/0.71130. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66092/0.71080. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66181/0.71298. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66119/0.71341. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66200/0.71306. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66238/0.71485. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65798/0.71419. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65996/0.71372. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.66245/0.71379. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69379/0.69212. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69198/0.69295. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69073/0.69388. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69024/0.69469. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68950/0.69555. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68951/0.69630. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68850/0.69720. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68820/0.69783. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68838/0.69838. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68799/0.69891. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68723/0.69971. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68670/0.70027. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68670/0.70103. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68698/0.70138. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68651/0.70174. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68545/0.70236. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68610/0.70248. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68617/0.70267. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68559/0.70300. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68536/0.70310. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68593/0.70269. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68520/0.70304. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68498/0.70281. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68438/0.70286. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68560/0.70313. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68441/0.70361. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68503/0.70393. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68457/0.70369. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68430/0.70369. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68396/0.70382. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68433/0.70343. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68314/0.70362. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68348/0.70392. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68357/0.70413. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68271/0.70373. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68256/0.70354. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68280/0.70338. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68205/0.70325. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68212/0.70299. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68282/0.70279. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68133/0.70283. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68076/0.70282. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68091/0.70329. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67973/0.70332. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67965/0.70310. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68051/0.70273. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67894/0.70209. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67943/0.70214. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67728/0.70158. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67738/0.70182. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67714/0.70176. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67649/0.70098. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67445/0.70092. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67570/0.70110. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67716/0.70065. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67646/0.70003. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67546/0.70013. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67495/0.69978. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67457/0.70007. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67431/0.69996. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67420/0.69967. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67174/0.69975. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67129/0.70028. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66958/0.70003. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66925/0.69917. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67081/0.69797. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66939/0.69817. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66694/0.69837. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66753/0.69892. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66521/0.69947. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66505/0.69815. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66481/0.69745. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66359/0.69815. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66131/0.69700. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66110/0.69578. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66054/0.69597. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66034/0.69586. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65774/0.69590. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65813/0.69654. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65802/0.69542. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65407/0.69505. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65089/0.69441. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65022/0.69517. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64802/0.69754. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65105/0.69597. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64847/0.69525. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64813/0.69478. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64247/0.69416. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64601/0.69405. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64248/0.69411. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64254/0.69291. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64151/0.69133. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64349/0.69258. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63777/0.68991. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.63379/0.69060. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63599/0.68983. Took 0.21 sec\n",
      "Epoch 96, Loss(train/val) 0.63310/0.69103. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63026/0.69103. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62855/0.69112. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63218/0.69013. Took 0.19 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69478/0.69329. Took 0.35 sec\n",
      "Epoch 1, Loss(train/val) 0.69293/0.69375. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69286/0.69418. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69286/0.69466. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69223/0.69521. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69212/0.69585. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69152/0.69668. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69060/0.69759. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69010/0.69861. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68960/0.69996. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68945/0.70131. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68841/0.70280. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68798/0.70444. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68761/0.70626. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68669/0.70786. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68626/0.70968. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68485/0.71151. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68557/0.71289. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68489/0.71395. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68414/0.71524. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68338/0.71651. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68281/0.71785. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68175/0.71913. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68108/0.72060. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68280/0.72168. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68008/0.72277. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68021/0.72385. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68157/0.72450. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67845/0.72528. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68065/0.72577. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67833/0.72655. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67889/0.72741. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67810/0.72808. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67691/0.72927. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67818/0.72901. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67816/0.72912. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67740/0.72993. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67702/0.73024. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67397/0.73125. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67325/0.73195. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67271/0.73274. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67228/0.73321. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67256/0.73350. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67110/0.73474. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67109/0.73456. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67186/0.73617. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67114/0.73802. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66698/0.73872. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67110/0.73808. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66917/0.73842. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66707/0.73715. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66785/0.73913. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66628/0.73994. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66436/0.74146. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66543/0.74260. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66483/0.74259. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66518/0.74402. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66410/0.74292. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66350/0.74374. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66428/0.74407. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66170/0.74547. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66157/0.74681. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66075/0.74741. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65854/0.74868. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66083/0.74775. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65616/0.74888. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65687/0.74882. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65730/0.74900. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65578/0.75058. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65416/0.75130. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65675/0.75276. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65489/0.75430. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65463/0.75376. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65407/0.75156. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65256/0.75639. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64853/0.75751. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65051/0.75574. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65253/0.75657. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64565/0.75964. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64627/0.76059. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64773/0.75903. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64687/0.76101. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64795/0.76366. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64791/0.76167. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64346/0.76614. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64310/0.76384. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63981/0.76223. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63953/0.76223. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63874/0.76880. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64001/0.76424. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64018/0.76902. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64174/0.76685. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63738/0.76680. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63752/0.76953. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63785/0.76707. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63166/0.76714. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63257/0.76834. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63242/0.77355. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63428/0.77319. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63208/0.77031. Took 0.18 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69559/0.69362. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69290/0.69053. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69206/0.69002. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69169/0.69009. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69052/0.68991. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69011/0.68964. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68999/0.69042. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68937/0.69109. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68978/0.69186. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68874/0.69143. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68818/0.69190. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68827/0.69217. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68720/0.69296. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68725/0.69302. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68571/0.69376. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68605/0.69440. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68598/0.69457. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68387/0.69611. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68391/0.69622. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68345/0.69690. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68153/0.69820. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68231/0.69852. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68095/0.69862. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67971/0.70012. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67940/0.69964. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67731/0.69843. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67725/0.69984. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67599/0.69890. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67648/0.69931. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67530/0.69730. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67365/0.69595. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67128/0.69662. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67035/0.69530. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.66733/0.69424. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66696/0.69386. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66722/0.69032. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66570/0.69037. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.66262/0.68997. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66010/0.68676. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66099/0.68662. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.65983/0.68649. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.65822/0.68626. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.65754/0.68605. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.65322/0.68556. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.65224/0.68702. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65089/0.68679. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65132/0.68833. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.64911/0.68880. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.64757/0.69097. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.64731/0.68958. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.64861/0.69092. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.64611/0.69111. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.64336/0.69326. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64657/0.69338. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64135/0.69466. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64293/0.69457. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.63952/0.69644. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64055/0.69717. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.63353/0.69932. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.63560/0.70074. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.63745/0.69980. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.63374/0.70239. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.62928/0.70425. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.63037/0.70504. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.62719/0.70854. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.62825/0.70864. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.62932/0.70951. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.62639/0.71046. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.62519/0.71036. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.62553/0.71274. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.62248/0.71416. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.61990/0.71484. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62182/0.71764. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62040/0.71593. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.61513/0.71868. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.61273/0.72029. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.61073/0.72201. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.61557/0.72254. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.61087/0.72459. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.61091/0.72476. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61387/0.72378. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61145/0.72484. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.60664/0.72632. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.61113/0.72832. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.60563/0.72874. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.60810/0.72907. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.60677/0.73103. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.60450/0.72982. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60313/0.73236. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.59766/0.73554. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.59789/0.73818. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.59748/0.73764. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.58876/0.73871. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.59260/0.73840. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.59190/0.74015. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.59442/0.74108. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.59230/0.74099. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.58690/0.74352. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59042/0.73996. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.58992/0.74052. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69424/0.69578. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69313/0.69507. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69259/0.69483. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69124/0.69486. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69178/0.69523. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69140/0.69559. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69171/0.69601. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69097/0.69664. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68985/0.69739. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69054/0.69824. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68974/0.69921. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69027/0.70011. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69012/0.70097. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68990/0.70175. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68906/0.70282. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68933/0.70414. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68867/0.70523. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68799/0.70593. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68799/0.70697. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68848/0.70737. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68824/0.70801. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68730/0.70844. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68832/0.70911. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68722/0.70993. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68743/0.71034. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68789/0.71071. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68833/0.71104. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68740/0.71134. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68678/0.71151. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68655/0.71164. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68572/0.71232. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68595/0.71322. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68433/0.71441. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68401/0.71513. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68529/0.71553. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68379/0.71572. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68417/0.71574. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68405/0.71616. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68410/0.71655. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68256/0.71695. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68267/0.71730. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68259/0.71770. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68175/0.71845. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68238/0.71897. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68145/0.71943. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67965/0.71994. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67883/0.72067. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67884/0.72145. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67922/0.72144. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67768/0.72187. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67693/0.72311. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67688/0.72419. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67841/0.72476. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67567/0.72528. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67689/0.72591. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67662/0.72682. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67615/0.72676. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67423/0.72809. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67276/0.72921. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67306/0.72892. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67171/0.72975. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66989/0.73131. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67234/0.73163. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67130/0.73206. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66717/0.73315. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66563/0.73483. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66767/0.73585. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66651/0.73549. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66904/0.73606. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66470/0.73781. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66265/0.73788. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65996/0.73952. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66219/0.73901. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66142/0.74304. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65894/0.74181. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65852/0.74129. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65673/0.74369. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65609/0.74345. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65625/0.74523. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65517/0.74676. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65226/0.74886. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65493/0.74896. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65037/0.74937. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64965/0.75075. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64974/0.74911. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64714/0.75108. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64688/0.75255. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64634/0.75183. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64626/0.74993. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64194/0.75217. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64273/0.75302. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64051/0.74851. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64189/0.75227. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63412/0.75284. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63747/0.75283. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63718/0.75377. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63340/0.75464. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63027/0.75593. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62851/0.75836. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62772/0.76028. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69371/0.69193. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69094/0.69322. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69072/0.69399. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69140/0.69447. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69074/0.69458. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68993/0.69459. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68987/0.69471. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68924/0.69470. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68966/0.69482. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68944/0.69477. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68867/0.69486. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68829/0.69517. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68785/0.69561. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68843/0.69579. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68761/0.69605. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68746/0.69651. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68734/0.69678. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68773/0.69701. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68688/0.69733. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68631/0.69776. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68728/0.69797. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68605/0.69817. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68662/0.69856. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68556/0.69892. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68677/0.69873. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68576/0.69938. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68496/0.69950. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68332/0.70028. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68489/0.70044. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68473/0.70051. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68340/0.70084. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68421/0.70106. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68356/0.70101. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68312/0.70061. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68297/0.70126. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68100/0.70158. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68264/0.70098. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68183/0.70158. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68265/0.70252. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68215/0.70251. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68070/0.70305. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68041/0.70328. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67980/0.70261. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67969/0.70253. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68034/0.70289. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67767/0.70300. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67833/0.70267. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67634/0.70301. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67621/0.70355. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67544/0.70477. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67595/0.70511. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67521/0.70382. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67273/0.70403. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67282/0.70441. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67167/0.70437. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67051/0.70426. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67015/0.70465. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66851/0.70539. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66645/0.70442. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66818/0.70573. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66488/0.70387. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66358/0.70362. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66165/0.70527. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66209/0.70494. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65867/0.70181. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65640/0.70466. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65748/0.70158. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65551/0.70387. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65508/0.70429. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65201/0.70379. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64874/0.70297. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64895/0.70397. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64612/0.69904. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64187/0.70149. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64327/0.70390. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64294/0.70313. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63867/0.69826. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64215/0.69852. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63504/0.70194. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63313/0.69664. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63482/0.69704. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.62888/0.69848. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62605/0.69777. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62947/0.69992. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63118/0.70787. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62906/0.69615. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62654/0.69989. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62580/0.70434. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62367/0.69990. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.61834/0.69734. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62023/0.69420. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61898/0.69565. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61451/0.69716. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61739/0.69936. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61404/0.69970. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60975/0.69847. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60743/0.70053. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60954/0.69934. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61159/0.70400. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60187/0.69761. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69495/0.69257. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69237/0.69190. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69226/0.69145. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69195/0.69128. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69089/0.69113. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69121/0.69115. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68961/0.69140. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68941/0.69173. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68938/0.69212. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68836/0.69234. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68846/0.69257. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68790/0.69299. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68864/0.69348. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68854/0.69370. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68679/0.69420. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68686/0.69468. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68705/0.69507. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68723/0.69532. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68794/0.69548. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68677/0.69559. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68670/0.69585. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68691/0.69599. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68642/0.69610. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68618/0.69626. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68622/0.69673. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68516/0.69686. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68454/0.69711. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68587/0.69761. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68631/0.69788. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68503/0.69779. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68565/0.69799. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68471/0.69812. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68483/0.69818. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68403/0.69828. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68499/0.69876. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68344/0.69881. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68451/0.69900. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68513/0.69891. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68315/0.69888. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68300/0.69900. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68248/0.69905. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68320/0.69925. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68239/0.69930. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68170/0.69955. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68063/0.69944. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68259/0.69986. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68193/0.70005. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68062/0.70005. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68070/0.70030. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67971/0.70019. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67898/0.70029. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67911/0.70053. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67831/0.70062. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67836/0.70022. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67675/0.70041. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67743/0.70098. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67673/0.70136. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67661/0.70068. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67500/0.70079. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67567/0.70068. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67530/0.70052. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67341/0.70076. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67351/0.70102. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67391/0.70072. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67243/0.70017. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67124/0.70066. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67083/0.70044. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66934/0.70052. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66862/0.70127. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66875/0.70048. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66856/0.70122. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66629/0.70084. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66539/0.69979. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66682/0.70017. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66596/0.70022. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66289/0.70042. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66035/0.70040. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66033/0.70012. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66235/0.70136. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65721/0.70076. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65771/0.69972. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65710/0.70160. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65358/0.70192. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65526/0.70194. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65287/0.70290. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65350/0.70532. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65184/0.70491. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65167/0.70347. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64961/0.70495. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64490/0.70741. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64676/0.70689. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64588/0.70657. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63855/0.71036. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63896/0.70996. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64014/0.70995. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64001/0.71212. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63815/0.71527. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63646/0.71512. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63811/0.71315. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62952/0.71368. Took 0.18 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69127/0.68926. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68874/0.68864. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68877/0.68958. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.68862/0.69029. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68774/0.69110. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68747/0.69214. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68720/0.69265. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68700/0.69362. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68700/0.69448. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68665/0.69509. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68672/0.69575. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68588/0.69625. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68561/0.69723. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68625/0.69742. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68574/0.69776. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68520/0.69809. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68559/0.69833. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68482/0.69863. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68334/0.69920. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68447/0.69939. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68427/0.69998. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68410/0.69994. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68299/0.70044. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68403/0.70067. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68352/0.70112. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68367/0.70111. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68242/0.70192. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68252/0.70207. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68216/0.70251. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68354/0.70312. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68251/0.70341. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68143/0.70357. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68039/0.70359. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68040/0.70459. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67988/0.70522. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67932/0.70503. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67999/0.70537. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68012/0.70573. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67864/0.70661. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67898/0.70641. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67796/0.70684. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67694/0.70754. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67735/0.70794. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67558/0.70813. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67615/0.70886. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67405/0.70996. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67585/0.71014. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67345/0.71143. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67451/0.71161. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67343/0.71160. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67452/0.71284. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67295/0.71406. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67202/0.71348. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67061/0.71514. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67257/0.71627. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67107/0.71676. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67020/0.71844. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67050/0.71767. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66982/0.71887. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66895/0.71960. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67167/0.71934. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66958/0.72055. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66783/0.72050. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66932/0.72174. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66695/0.72272. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66610/0.72518. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66698/0.72535. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66496/0.72568. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66703/0.72697. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66792/0.72791. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66349/0.72827. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66219/0.72944. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66455/0.72921. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66405/0.72936. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66374/0.73016. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66369/0.73118. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66345/0.73200. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66255/0.73278. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66159/0.73316. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66226/0.73450. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65991/0.73526. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66053/0.73570. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65849/0.73690. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65752/0.73830. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65804/0.73929. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65696/0.74097. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65402/0.74119. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65873/0.74165. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65665/0.74286. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65684/0.74329. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65596/0.74316. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65464/0.74418. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65202/0.74614. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65312/0.74649. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65220/0.74636. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65541/0.74928. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65212/0.74960. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64695/0.75178. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65116/0.75231. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65262/0.75190. Took 0.21 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69303/0.68625. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69059/0.68331. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68864/0.68148. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68880/0.68031. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68829/0.67925. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68729/0.67821. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68676/0.67738. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68732/0.67664. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68710/0.67598. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68567/0.67501. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68531/0.67430. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68527/0.67366. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68424/0.67290. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68446/0.67223. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68395/0.67166. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68432/0.67094. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68392/0.67020. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68313/0.66969. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68307/0.66932. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68239/0.66859. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68231/0.66780. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68149/0.66747. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68103/0.66672. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68204/0.66635. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67992/0.66594. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67986/0.66553. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68077/0.66557. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67975/0.66504. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.67986/0.66512. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67927/0.66466. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67882/0.66472. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67874/0.66482. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67869/0.66451. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67703/0.66444. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67632/0.66464. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67874/0.66466. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67653/0.66430. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67610/0.66472. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67583/0.66474. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67515/0.66436. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67628/0.66487. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67408/0.66469. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67340/0.66454. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67290/0.66438. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66988/0.66440. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.67219/0.66435. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67000/0.66397. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67387/0.66389. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66938/0.66397. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67005/0.66402. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67036/0.66436. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67049/0.66478. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66820/0.66442. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66863/0.66502. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66752/0.66369. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.66762/0.66379. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66712/0.66384. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66558/0.66352. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66529/0.66397. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66517/0.66383. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66331/0.66436. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66457/0.66585. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66221/0.66617. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66519/0.66587. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66344/0.66686. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66281/0.66686. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66220/0.66641. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66191/0.66577. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65949/0.66837. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66185/0.66565. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66017/0.66507. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65826/0.66666. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66119/0.66570. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66039/0.66510. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65479/0.66563. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65670/0.66698. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65708/0.66676. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65517/0.66801. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65962/0.66687. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65398/0.66498. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65192/0.66495. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65227/0.66584. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65409/0.66810. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65118/0.66750. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65470/0.66752. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65654/0.66817. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65280/0.66727. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64928/0.66873. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64978/0.67024. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64691/0.67099. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64771/0.67123. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64962/0.67073. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64812/0.67263. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64739/0.67306. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64391/0.67364. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64672/0.67511. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64529/0.67632. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64080/0.67389. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64354/0.67567. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64241/0.67425. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69386/0.69165. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69003/0.69028. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68926/0.69022. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68889/0.69068. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68849/0.69106. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68824/0.69167. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68763/0.69247. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68791/0.69334. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68658/0.69412. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68611/0.69527. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68606/0.69621. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68420/0.69735. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68483/0.69848. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68427/0.69959. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68458/0.70054. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68431/0.70141. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68344/0.70227. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68369/0.70302. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68308/0.70399. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68308/0.70473. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68272/0.70571. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68202/0.70668. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68234/0.70745. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68186/0.70803. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68153/0.70858. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68125/0.70927. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68162/0.71032. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68084/0.71109. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68058/0.71141. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68153/0.71154. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67958/0.71200. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68017/0.71262. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67807/0.71331. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67972/0.71381. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68097/0.71397. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67896/0.71358. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67887/0.71404. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67780/0.71466. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67894/0.71480. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67963/0.71481. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67920/0.71507. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67897/0.71539. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67790/0.71548. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67677/0.71567. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67760/0.71573. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67678/0.71598. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67783/0.71642. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67597/0.71661. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67742/0.71637. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67671/0.71681. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67704/0.71681. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67525/0.71765. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67452/0.71789. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67570/0.71800. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67477/0.71758. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67320/0.71870. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67229/0.71907. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67188/0.71927. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67276/0.71973. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67349/0.71936. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67263/0.71958. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67157/0.72022. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67136/0.71945. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67186/0.71962. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67003/0.72025. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67005/0.72004. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66724/0.72048. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67030/0.72118. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66735/0.72179. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66720/0.72202. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66693/0.72250. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66650/0.72270. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66305/0.72385. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66678/0.72387. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66535/0.72371. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66263/0.72532. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66186/0.72522. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66414/0.72584. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66188/0.72592. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66105/0.72746. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65994/0.72909. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65811/0.73017. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65834/0.73057. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65793/0.73008. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65664/0.73077. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65656/0.73045. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65590/0.73207. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65302/0.73433. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65448/0.73586. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65314/0.73657. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65161/0.73608. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65048/0.73951. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65157/0.74033. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64769/0.74038. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64596/0.74291. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65161/0.74274. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64776/0.74351. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64632/0.74266. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64470/0.74447. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64220/0.74625. Took 0.19 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69243/0.68709. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68877/0.68588. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68842/0.68496. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68761/0.68402. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68663/0.68312. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68712/0.68229. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68695/0.68165. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68657/0.68093. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68537/0.68035. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68626/0.67958. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68539/0.67883. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68518/0.67819. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68468/0.67784. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68497/0.67717. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68372/0.67671. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68373/0.67627. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68451/0.67580. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68346/0.67557. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68287/0.67552. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68320/0.67523. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68301/0.67507. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68400/0.67480. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68276/0.67476. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68264/0.67461. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68189/0.67462. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68145/0.67462. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68182/0.67448. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68192/0.67395. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68183/0.67418. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68198/0.67375. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68185/0.67384. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67986/0.67421. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67916/0.67411. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67925/0.67446. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68084/0.67464. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67869/0.67459. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67818/0.67443. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67825/0.67467. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67726/0.67465. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67942/0.67440. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67907/0.67478. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67721/0.67508. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67823/0.67459. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67603/0.67475. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67538/0.67469. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67646/0.67416. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67439/0.67387. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67392/0.67361. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67498/0.67387. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67170/0.67415. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67205/0.67466. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67148/0.67479. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67032/0.67453. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67316/0.67479. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67070/0.67507. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66944/0.67522. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67034/0.67428. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66827/0.67426. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66711/0.67417. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66354/0.67455. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66636/0.67427. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66452/0.67358. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66289/0.67274. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66236/0.67380. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66304/0.67390. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66084/0.67417. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66169/0.67509. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65914/0.67482. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65737/0.67521. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65738/0.67418. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65507/0.67334. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65384/0.67372. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65680/0.67240. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65245/0.67261. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65051/0.67408. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64963/0.67398. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64930/0.67320. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64939/0.67287. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64951/0.67230. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64653/0.67160. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64649/0.67264. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64506/0.67020. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64569/0.67306. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64186/0.67347. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64059/0.67306. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64322/0.67112. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63834/0.67410. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64266/0.67148. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64270/0.67002. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64170/0.67298. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63800/0.67356. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63514/0.67292. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63533/0.67220. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63448/0.67289. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63162/0.67195. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63218/0.67174. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63387/0.67303. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63173/0.67205. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63301/0.67121. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63314/0.67064. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69155/0.68730. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68903/0.68579. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68838/0.68541. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68857/0.68522. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68825/0.68512. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68776/0.68505. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68730/0.68508. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68673/0.68498. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68635/0.68500. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68635/0.68499. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68615/0.68507. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68545/0.68521. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68432/0.68527. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68491/0.68525. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68507/0.68543. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68392/0.68540. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68418/0.68542. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68453/0.68546. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68418/0.68548. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68297/0.68551. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68317/0.68562. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68252/0.68546. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68139/0.68584. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68202/0.68623. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68260/0.68621. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68136/0.68624. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68163/0.68596. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68110/0.68615. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68104/0.68614. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68171/0.68670. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68034/0.68713. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68045/0.68660. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68045/0.68661. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67934/0.68624. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67906/0.68645. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67919/0.68646. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67965/0.68660. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67921/0.68667. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67706/0.68696. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67780/0.68714. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67834/0.68713. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67760/0.68740. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67588/0.68764. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67825/0.68798. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67577/0.68848. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67649/0.68872. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67504/0.68925. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67447/0.68964. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67635/0.68995. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67578/0.68990. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67527/0.69015. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67371/0.69060. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67382/0.69134. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67287/0.69197. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67292/0.69231. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67188/0.69239. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67188/0.69273. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67035/0.69227. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67177/0.69318. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66916/0.69466. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66927/0.69370. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66815/0.69589. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66822/0.69596. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66941/0.69715. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66745/0.69624. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66651/0.69709. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66409/0.69752. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66695/0.69982. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66620/0.69919. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66394/0.69991. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66377/0.70111. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66516/0.70116. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66294/0.70183. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66178/0.70218. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66068/0.70236. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66293/0.70224. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66420/0.70291. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66163/0.70195. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65924/0.70257. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66115/0.70238. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66312/0.70108. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65844/0.70331. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66101/0.70384. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65771/0.70425. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65866/0.70198. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65714/0.70714. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65680/0.70594. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65741/0.70422. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65690/0.70607. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65418/0.70850. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65455/0.70736. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65524/0.70701. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65400/0.70784. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65193/0.70779. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65015/0.71058. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65045/0.71040. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65082/0.70910. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64945/0.70998. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65207/0.70870. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65044/0.70986. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69218/0.69379. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.68855/0.69451. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68745/0.69394. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68670/0.69305. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68654/0.69246. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68470/0.69188. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68512/0.69119. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68487/0.69090. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68474/0.69059. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68436/0.69018. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68385/0.69005. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68436/0.69012. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68403/0.68980. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68289/0.69008. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68325/0.69021. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68225/0.69051. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68168/0.69082. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68195/0.69110. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68104/0.69170. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68123/0.69204. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68019/0.69273. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68040/0.69313. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67985/0.69334. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68015/0.69398. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67983/0.69399. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68028/0.69425. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67984/0.69470. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67872/0.69553. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67837/0.69579. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67881/0.69636. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67789/0.69632. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67765/0.69649. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67773/0.69645. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67714/0.69696. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67691/0.69733. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67730/0.69740. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67622/0.69762. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67737/0.69815. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67750/0.69822. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67660/0.69856. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67662/0.69865. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67521/0.69864. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67593/0.69864. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67408/0.69944. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67533/0.69968. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67430/0.69968. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67361/0.70043. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67323/0.70073. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67243/0.70087. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67250/0.70095. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67209/0.70087. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67374/0.70212. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67140/0.70280. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67169/0.70219. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67142/0.70222. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67198/0.70239. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66986/0.70273. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67017/0.70242. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66904/0.70363. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66975/0.70342. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66948/0.70387. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66903/0.70418. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66869/0.70435. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66655/0.70537. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66805/0.70511. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66573/0.70580. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66691/0.70611. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66394/0.70657. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66560/0.70670. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66584/0.70723. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66452/0.70784. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66516/0.70851. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66321/0.70889. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66371/0.70928. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66367/0.71012. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66166/0.71093. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66138/0.71044. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66139/0.71045. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66042/0.71243. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66192/0.71347. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65708/0.71392. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65803/0.71434. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65976/0.71624. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65815/0.71730. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65725/0.71809. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65733/0.71926. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65568/0.71905. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65622/0.71952. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65327/0.71987. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65089/0.72189. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65341/0.72417. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65200/0.72668. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64979/0.72768. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64922/0.72860. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64993/0.72925. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65248/0.73168. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65024/0.73449. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64669/0.73439. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65003/0.73399. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64577/0.73680. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69235/0.69257. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69135/0.69248. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69001/0.69247. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68939/0.69239. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68994/0.69251. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68965/0.69251. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68945/0.69256. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68908/0.69270. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68819/0.69284. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68857/0.69302. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68778/0.69310. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68793/0.69318. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68798/0.69324. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68655/0.69333. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68765/0.69344. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68759/0.69368. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68690/0.69384. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68736/0.69431. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68682/0.69447. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68632/0.69440. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68637/0.69460. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68662/0.69497. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68506/0.69524. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68523/0.69571. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68503/0.69595. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68549/0.69622. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68464/0.69636. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68376/0.69644. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68441/0.69661. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68406/0.69685. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68365/0.69702. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68402/0.69718. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68361/0.69737. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68352/0.69756. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68391/0.69797. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68337/0.69820. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68237/0.69793. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68231/0.69805. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68211/0.69842. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68190/0.69809. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68089/0.69797. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68180/0.69825. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68176/0.69878. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68090/0.69861. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68067/0.69864. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68102/0.69852. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67980/0.69882. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67935/0.69914. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67924/0.69865. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67904/0.69882. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67927/0.69894. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67754/0.69917. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67808/0.69874. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67652/0.69880. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67708/0.69844. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67592/0.69825. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67468/0.69863. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67631/0.69908. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67528/0.69855. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67525/0.69895. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67489/0.69889. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67271/0.69870. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67253/0.69951. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67162/0.69965. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67272/0.69947. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67560/0.69955. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67225/0.70045. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67211/0.70090. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67073/0.70187. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66981/0.70196. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66978/0.70164. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67015/0.70105. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66936/0.70191. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66777/0.70191. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66780/0.70084. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66668/0.70104. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66693/0.70189. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66691/0.70197. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66499/0.70197. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66411/0.70270. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66322/0.70144. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66437/0.70109. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66307/0.70216. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66466/0.70159. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66350/0.70090. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66348/0.70368. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66100/0.70314. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66022/0.70233. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66166/0.70369. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65975/0.70416. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65503/0.70434. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65581/0.70390. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65574/0.70444. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65627/0.70544. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65673/0.70632. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65435/0.70633. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65721/0.70740. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65496/0.70742. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65353/0.70835. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65215/0.70845. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69306/0.68862. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69051/0.68687. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69070/0.68648. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68982/0.68652. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68974/0.68649. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68917/0.68645. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68903/0.68633. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68859/0.68628. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68904/0.68603. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68855/0.68612. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68840/0.68619. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68814/0.68596. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68796/0.68600. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68758/0.68598. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68733/0.68591. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68765/0.68591. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68678/0.68591. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68781/0.68614. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68639/0.68590. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68752/0.68570. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68675/0.68575. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68671/0.68583. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68584/0.68590. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68684/0.68570. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68707/0.68539. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68559/0.68531. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68659/0.68499. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68700/0.68473. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68470/0.68467. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68536/0.68454. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68567/0.68459. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68475/0.68449. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68500/0.68458. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68547/0.68434. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68560/0.68435. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68476/0.68467. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68397/0.68449. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68296/0.68460. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68412/0.68441. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68384/0.68415. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68248/0.68380. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68257/0.68370. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68191/0.68382. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68443/0.68338. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68333/0.68322. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.68198/0.68356. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68237/0.68345. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68270/0.68323. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68190/0.68339. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68171/0.68367. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68040/0.68395. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68172/0.68420. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.68123/0.68333. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68125/0.68329. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.68096/0.68379. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68129/0.68444. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.68110/0.68385. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.68150/0.68287. Took 0.22 sec\n",
      "Epoch 58, Loss(train/val) 0.67870/0.68294. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67861/0.68279. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 0.67983/0.68290. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.68103/0.68316. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67854/0.68418. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67890/0.68311. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67904/0.68377. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67720/0.68315. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.67871/0.68295. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67815/0.68268. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.67677/0.68324. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67765/0.68270. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67614/0.68235. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67443/0.68429. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.67568/0.68365. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67645/0.68227. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.67336/0.68347. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67421/0.68320. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67370/0.68261. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.67307/0.68263. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.67294/0.68303. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.67305/0.68289. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67143/0.68303. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.67304/0.68433. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.67077/0.68293. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.67212/0.68315. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.67039/0.68329. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66959/0.68369. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66925/0.68429. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66797/0.68419. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66803/0.68490. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66768/0.68452. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66701/0.68444. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66708/0.68490. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66460/0.68710. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66718/0.68627. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66310/0.68484. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66658/0.68491. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66136/0.68527. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66419/0.68426. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66320/0.68335. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.66239/0.68694. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69451/0.68918. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69059/0.68547. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69036/0.68470. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68926/0.68470. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68881/0.68506. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68786/0.68522. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68725/0.68538. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68613/0.68581. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68632/0.68619. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68540/0.68635. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68495/0.68671. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68509/0.68715. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68430/0.68776. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68422/0.68807. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68499/0.68866. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68415/0.68911. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68350/0.68915. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68403/0.69005. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68451/0.68997. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68420/0.69052. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68359/0.69062. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68376/0.69102. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68294/0.69042. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68309/0.69109. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68216/0.69128. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68364/0.69145. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68138/0.69152. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68222/0.69191. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68142/0.69206. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68152/0.69237. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68197/0.69218. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68217/0.69266. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68077/0.69375. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68134/0.69321. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68173/0.69380. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67975/0.69353. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68123/0.69379. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68045/0.69406. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68055/0.69376. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68152/0.69373. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68013/0.69416. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68000/0.69448. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67933/0.69427. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67771/0.69469. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67902/0.69472. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67725/0.69516. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67894/0.69539. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67752/0.69660. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67725/0.69600. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67496/0.69620. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67694/0.69613. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67577/0.69741. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67554/0.69730. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67675/0.69750. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67553/0.69679. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67439/0.69722. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67478/0.69731. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67222/0.69802. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67424/0.69702. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67471/0.69657. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67190/0.69798. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67163/0.69878. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67097/0.69939. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67106/0.69973. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67156/0.70058. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67005/0.70035. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66967/0.70073. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66799/0.70143. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66797/0.70162. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66722/0.70119. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66959/0.70178. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66737/0.70454. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66524/0.70411. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66616/0.70503. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66457/0.70719. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66670/0.70597. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66241/0.70762. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66388/0.70695. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66538/0.70785. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66220/0.70872. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66617/0.71044. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66229/0.70947. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66249/0.71091. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66121/0.71119. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66062/0.71376. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65901/0.71280. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65943/0.71317. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65860/0.71255. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65939/0.71392. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65910/0.71413. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65600/0.71593. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65628/0.71539. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65709/0.71726. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65499/0.71781. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65274/0.71978. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65191/0.72043. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65237/0.72244. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65344/0.72157. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65021/0.72206. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65150/0.72390. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69030/0.69265. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69005/0.69293. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68930/0.69294. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68853/0.69283. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68785/0.69262. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68777/0.69246. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68737/0.69253. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68705/0.69242. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68647/0.69232. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68497/0.69215. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68518/0.69192. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68474/0.69178. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68470/0.69145. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68434/0.69153. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68351/0.69134. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68322/0.69127. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68341/0.69124. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68300/0.69100. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68275/0.69125. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68288/0.69118. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68268/0.69105. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68248/0.69101. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68222/0.69114. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68158/0.69115. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68039/0.69099. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68002/0.69094. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68146/0.69125. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68153/0.69130. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67974/0.69107. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68121/0.69091. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67980/0.69082. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68005/0.69070. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67908/0.69052. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67812/0.69069. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67803/0.69098. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67822/0.69117. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67661/0.69091. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67748/0.69092. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67695/0.69081. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67541/0.69096. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67685/0.69112. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67598/0.69104. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67440/0.69114. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67526/0.69119. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67317/0.69147. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67238/0.69188. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67293/0.69205. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67372/0.69199. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67084/0.69215. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67093/0.69219. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67102/0.69240. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66931/0.69305. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66962/0.69409. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66787/0.69473. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66846/0.69556. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66579/0.69584. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66504/0.69569. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66300/0.69720. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66590/0.69741. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66299/0.69792. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66075/0.69902. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66015/0.69923. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65996/0.69975. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65554/0.70136. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65615/0.70419. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65540/0.70251. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65374/0.70401. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65452/0.70321. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64977/0.70398. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65135/0.70306. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64948/0.70464. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64889/0.70371. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64639/0.70611. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64586/0.70629. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64384/0.70820. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64732/0.70710. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64253/0.70727. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64107/0.70730. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63633/0.70944. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63551/0.71029. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63673/0.71131. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63506/0.71339. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63286/0.71277. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63743/0.71134. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63104/0.71331. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63397/0.71222. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63038/0.71026. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62830/0.71146. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62870/0.71152. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62450/0.71169. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62545/0.71323. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62911/0.71249. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62048/0.71313. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62429/0.71260. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61958/0.71347. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.61908/0.71484. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61731/0.71548. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61941/0.71389. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61434/0.71579. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61938/0.71549. Took 0.18 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69129/0.68460. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69001/0.68214. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69017/0.68065. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68857/0.67948. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68871/0.67841. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68811/0.67723. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68757/0.67621. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68703/0.67512. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68687/0.67428. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68648/0.67352. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68553/0.67254. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68559/0.67187. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68501/0.67068. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68419/0.67026. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68312/0.66968. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68438/0.66916. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.68448/0.66832. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68300/0.66786. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68328/0.66760. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68268/0.66702. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68215/0.66697. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68462/0.66671. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68180/0.66693. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68282/0.66646. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68096/0.66609. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68141/0.66593. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68145/0.66640. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68089/0.66628. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67978/0.66611. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68038/0.66556. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67994/0.66610. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67962/0.66600. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67871/0.66618. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67771/0.66579. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67746/0.66585. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67793/0.66589. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67651/0.66629. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67617/0.66595. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67656/0.66577. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67614/0.66568. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67508/0.66583. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67470/0.66676. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67525/0.66671. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67499/0.66662. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67347/0.66656. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67351/0.66679. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67238/0.66740. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67198/0.66728. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66958/0.66762. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67237/0.66787. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67117/0.66745. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66815/0.66771. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66943/0.66735. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66980/0.66850. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66493/0.66855. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66678/0.66763. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66615/0.66864. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66708/0.66841. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66468/0.66937. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66414/0.66905. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66293/0.66944. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66419/0.67017. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66287/0.67060. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 0.66141/0.67151. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66384/0.67181. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66305/0.67224. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66137/0.67254. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66022/0.67292. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65810/0.67296. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65824/0.67436. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65679/0.67275. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65584/0.67349. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65692/0.67342. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65520/0.67794. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65505/0.67605. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65424/0.67627. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65504/0.67696. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65395/0.67699. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65065/0.67593. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65132/0.67801. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64999/0.67743. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65138/0.68006. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64776/0.67955. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.64966/0.67948. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64857/0.67693. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.64569/0.68053. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64628/0.68032. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64567/0.68305. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64687/0.68274. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.64440/0.68101. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.64397/0.68520. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.64058/0.68398. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64598/0.68247. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63949/0.68696. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64154/0.68510. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64104/0.68932. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63792/0.69025. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63594/0.69250. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63319/0.69246. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63207/0.69388. Took 0.18 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69147/0.69480. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68787/0.69793. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68768/0.69929. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68810/0.69962. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68783/0.69985. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68796/0.70009. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68698/0.70055. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68744/0.70107. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68657/0.70134. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68717/0.70147. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68711/0.70157. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68674/0.70186. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68609/0.70203. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68654/0.70238. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68580/0.70250. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68602/0.70289. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68536/0.70291. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68524/0.70307. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68543/0.70325. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68547/0.70354. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68478/0.70367. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68567/0.70370. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68481/0.70361. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68541/0.70404. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68429/0.70425. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68450/0.70407. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68396/0.70437. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68321/0.70437. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68359/0.70461. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68376/0.70481. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68356/0.70514. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68292/0.70505. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68344/0.70500. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68362/0.70517. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68310/0.70504. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68168/0.70527. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68139/0.70495. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68138/0.70533. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68142/0.70607. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68231/0.70636. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68052/0.70614. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67977/0.70619. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68136/0.70677. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68032/0.70751. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68003/0.70763. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67829/0.70766. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67773/0.70791. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67922/0.70838. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67771/0.70859. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67636/0.70924. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67552/0.70965. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67638/0.70970. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67500/0.71013. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67425/0.71059. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67735/0.70965. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67328/0.71050. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67407/0.71031. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67544/0.71249. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67413/0.71179. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67299/0.71176. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67084/0.71163. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67109/0.71225. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67125/0.71312. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66967/0.71408. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66953/0.71365. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67264/0.71410. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66894/0.71476. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66751/0.71487. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66820/0.71508. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66773/0.71524. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66517/0.71463. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66586/0.71675. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66465/0.71610. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66353/0.71768. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66335/0.71861. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66434/0.71896. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66241/0.71873. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66128/0.71829. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66339/0.71861. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66261/0.71943. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66170/0.72144. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65937/0.72274. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65592/0.72340. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65990/0.72300. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65470/0.72238. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65625/0.72302. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65723/0.72274. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65728/0.72574. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65536/0.72578. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65266/0.72654. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65578/0.72609. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65360/0.72614. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65548/0.72717. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65279/0.72728. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65091/0.72999. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65078/0.72964. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65023/0.72884. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64981/0.73286. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64774/0.73158. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64842/0.72955. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69457/0.69438. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69110/0.69933. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68989/0.70356. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68879/0.70691. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68837/0.70895. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68772/0.71053. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68693/0.71214. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68705/0.71379. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68656/0.71467. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68684/0.71591. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68669/0.71729. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68535/0.71875. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68633/0.72050. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68482/0.72166. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68405/0.72290. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68402/0.72443. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68529/0.72585. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68391/0.72649. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68351/0.72780. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68492/0.72905. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68300/0.72975. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68199/0.73136. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68359/0.73188. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68276/0.73288. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68267/0.73312. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68235/0.73420. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68116/0.73460. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68235/0.73503. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68254/0.73645. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68215/0.73625. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68039/0.73654. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68094/0.73717. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68096/0.73744. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68174/0.73713. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68103/0.73748. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68071/0.73763. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68099/0.73793. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68097/0.73802. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68141/0.73849. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67976/0.73896. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67894/0.73807. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67935/0.73819. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67934/0.73900. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67925/0.73856. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68001/0.73768. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67892/0.73861. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67718/0.73939. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67790/0.73966. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67874/0.73941. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67782/0.74010. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67786/0.73909. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67732/0.73805. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67591/0.73892. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67607/0.73940. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67651/0.73928. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67554/0.73896. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67409/0.73855. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67564/0.73883. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67332/0.74029. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67436/0.73892. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67254/0.74096. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67154/0.74221. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67301/0.74078. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67259/0.74008. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67372/0.73937. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67214/0.74013. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67162/0.74126. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67115/0.74002. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66936/0.74054. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66929/0.74058. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66928/0.74217. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66848/0.74279. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66768/0.74144. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66896/0.74566. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66764/0.74224. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66799/0.74690. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66627/0.74676. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66683/0.74698. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66498/0.74902. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66437/0.74633. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66622/0.74866. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66507/0.74863. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66265/0.75035. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66331/0.75150. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66240/0.75182. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66133/0.75292. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66285/0.75478. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66195/0.75517. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66158/0.75551. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66133/0.75480. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66090/0.75482. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65690/0.75824. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65654/0.75606. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65966/0.75764. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65562/0.75679. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65478/0.75674. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65209/0.75566. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65324/0.76069. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65286/0.76472. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65313/0.76009. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69519/0.69503. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.69560. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69144/0.69556. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69098/0.69519. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68910/0.69511. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68912/0.69517. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68859/0.69500. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68763/0.69509. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68743/0.69548. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68689/0.69554. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68719/0.69591. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68646/0.69642. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68581/0.69659. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68455/0.69728. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68489/0.69783. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68420/0.69807. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68360/0.69845. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68307/0.69904. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68325/0.69948. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68304/0.69951. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68218/0.70007. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68210/0.70124. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68267/0.70121. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68159/0.70135. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68126/0.70225. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68213/0.70247. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68165/0.70293. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68007/0.70325. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67955/0.70394. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68075/0.70442. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68017/0.70535. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67718/0.70578. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67925/0.70720. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67624/0.70770. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67773/0.70803. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67626/0.70946. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67630/0.71089. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67468/0.71209. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67491/0.71224. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67460/0.71283. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67435/0.71328. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67324/0.71397. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67219/0.71456. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67050/0.71528. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67173/0.71598. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67077/0.71632. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66872/0.71815. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66867/0.71941. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67058/0.71914. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66753/0.72038. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66894/0.72067. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66797/0.72180. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66634/0.72290. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66404/0.72349. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66433/0.72491. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66422/0.72461. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66418/0.72525. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66256/0.72517. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66367/0.72466. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66101/0.72642. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66003/0.72595. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65888/0.72916. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65968/0.72899. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65991/0.72932. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65736/0.72904. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65603/0.72929. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65270/0.72972. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65206/0.73208. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65212/0.73142. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65172/0.73196. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65264/0.73096. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64799/0.73282. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64988/0.73251. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64517/0.73237. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64503/0.73171. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64067/0.73376. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63861/0.73241. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63976/0.73310. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64086/0.73367. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63871/0.73398. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63722/0.73513. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63030/0.73600. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.63252/0.73790. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63256/0.73787. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63004/0.73866. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62920/0.74021. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62850/0.74084. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63090/0.74186. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62788/0.74480. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62364/0.74618. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62412/0.74768. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61948/0.74868. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62128/0.75091. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61858/0.75154. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.61531/0.75191. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61420/0.75555. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61569/0.75672. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61663/0.75691. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.61518/0.75719. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60869/0.75910. Took 0.19 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69640/0.68826. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69344/0.68885. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69284/0.68904. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69231/0.68907. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69163/0.68935. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69092/0.68962. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69028/0.68990. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68953/0.69021. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68961/0.69052. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68867/0.69077. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68847/0.69120. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68811/0.69148. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68710/0.69164. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68723/0.69204. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68668/0.69231. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68639/0.69245. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68535/0.69267. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68558/0.69297. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68568/0.69310. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68485/0.69331. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68415/0.69347. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68367/0.69364. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68301/0.69341. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68235/0.69334. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68239/0.69358. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68172/0.69334. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68095/0.69311. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67921/0.69306. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68024/0.69265. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67913/0.69272. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67796/0.69243. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67822/0.69220. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67648/0.69260. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67578/0.69245. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67436/0.69228. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67418/0.69221. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67393/0.69260. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67309/0.69245. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67112/0.69201. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66998/0.69247. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66837/0.69256. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66933/0.69233. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66633/0.69158. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66579/0.69330. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66388/0.69238. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66183/0.69343. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66112/0.69193. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65962/0.69264. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65827/0.69228. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.65613/0.69306. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65500/0.69212. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65460/0.69394. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65199/0.69341. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64896/0.69285. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65045/0.69295. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64675/0.69279. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64574/0.69318. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64471/0.69331. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64377/0.69379. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.64316/0.69427. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64014/0.69535. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64179/0.69513. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63722/0.69664. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63707/0.69622. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63651/0.69782. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63413/0.69973. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63368/0.70026. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63121/0.70018. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63186/0.70157. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.62931/0.70329. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.62791/0.70442. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62780/0.70534. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62389/0.70584. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62231/0.70480. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62304/0.70580. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62262/0.70651. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62050/0.70823. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.61841/0.70787. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.61525/0.70797. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.61625/0.70919. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61105/0.71124. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61314/0.71075. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61300/0.71121. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.61287/0.71171. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.60723/0.71580. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.60802/0.71295. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.60623/0.71325. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.60557/0.71422. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60322/0.71622. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60738/0.71603. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60243/0.71360. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.59586/0.71808. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.59584/0.71532. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.59980/0.71588. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.59388/0.71723. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.59001/0.71626. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.59139/0.71864. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.58920/0.71564. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59208/0.71760. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.58788/0.72028. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69466/0.68965. Took 0.34 sec\n",
      "Epoch 1, Loss(train/val) 0.69306/0.68866. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69153/0.69000. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69088/0.69159. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68917/0.69316. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68873/0.69466. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68805/0.69677. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68712/0.69858. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68654/0.70012. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68657/0.70225. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68634/0.70398. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68525/0.70506. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68507/0.70690. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68454/0.70811. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68365/0.70948. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68373/0.71102. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68326/0.71289. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68320/0.71391. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68254/0.71512. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68183/0.71573. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68163/0.71672. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68206/0.71754. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68035/0.71811. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68132/0.71874. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68091/0.72001. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68114/0.72005. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67972/0.72081. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68027/0.72198. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67957/0.72215. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67954/0.72327. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67914/0.72398. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67925/0.72445. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67852/0.72460. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67821/0.72582. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67736/0.72618. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67834/0.72735. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67814/0.72780. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67697/0.72899. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67772/0.72944. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67603/0.72981. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67753/0.72972. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67666/0.73024. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67602/0.73058. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67544/0.73157. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67507/0.73221. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67402/0.73411. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67395/0.73439. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67198/0.73506. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67368/0.73543. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67258/0.73670. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67321/0.73703. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67150/0.73742. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67134/0.73948. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67446/0.73883. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66966/0.73929. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67213/0.74022. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66993/0.74158. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66945/0.74292. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66943/0.74471. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66977/0.74381. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66784/0.74421. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67005/0.74493. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66673/0.74589. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66789/0.74772. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66760/0.74590. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66834/0.74676. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66742/0.74751. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66481/0.74947. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66313/0.75201. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66334/0.75081. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66583/0.75257. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66442/0.75138. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66131/0.75226. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66261/0.75463. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66440/0.75482. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66395/0.75735. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66063/0.75719. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66072/0.75662. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65799/0.75961. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65787/0.75879. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65929/0.76019. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66006/0.75948. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65650/0.76005. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65771/0.75998. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65699/0.76164. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65667/0.76181. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65567/0.76247. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65654/0.76575. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65350/0.76697. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65246/0.76711. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65359/0.76774. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65306/0.76897. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65049/0.76919. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64966/0.76752. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65049/0.77329. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64954/0.77414. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64571/0.77673. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64570/0.77762. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64487/0.77847. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64699/0.77771. Took 0.19 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69191/0.69729. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69133/0.69819. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69023/0.69853. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.68958/0.69852. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68935/0.69825. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68761/0.69820. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68756/0.69792. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68641/0.69748. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68649/0.69695. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68588/0.69677. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68449/0.69665. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68489/0.69632. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68387/0.69626. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68316/0.69611. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68250/0.69590. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68216/0.69556. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68203/0.69553. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68149/0.69513. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68057/0.69514. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68119/0.69521. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68153/0.69492. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68044/0.69442. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68099/0.69392. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68076/0.69364. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67870/0.69352. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67916/0.69320. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67877/0.69295. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67866/0.69258. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67807/0.69159. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67886/0.69168. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67811/0.69207. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67932/0.69135. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67700/0.69105. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67759/0.69092. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67676/0.69006. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67680/0.68973. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67598/0.69021. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67714/0.68964. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67579/0.68924. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67573/0.68980. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67548/0.68966. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67517/0.68836. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67417/0.68761. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67392/0.68812. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67352/0.68737. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67348/0.68755. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67331/0.68677. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67397/0.68716. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67370/0.68784. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67213/0.68751. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67148/0.68870. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67142/0.68753. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67042/0.68693. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67154/0.68618. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67156/0.68627. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67015/0.68779. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67042/0.68679. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67133/0.68679. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66976/0.68689. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67026/0.68625. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67029/0.68663. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66840/0.68624. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66836/0.68476. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66686/0.68502. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 0.66600/0.68502. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66688/0.68477. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66754/0.68532. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66747/0.68569. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66621/0.68503. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66762/0.68536. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66689/0.68527. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66398/0.68629. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66347/0.68709. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66522/0.68669. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66482/0.68803. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66441/0.68734. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66429/0.68762. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66180/0.68662. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66200/0.68686. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66141/0.68561. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66266/0.68444. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66149/0.68439. Took 0.22 sec\n",
      "Epoch 82, Loss(train/val) 0.66072/0.68378. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66183/0.68526. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65980/0.68455. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66074/0.68628. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66113/0.68536. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65950/0.68558. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65824/0.68529. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65748/0.68683. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65658/0.68604. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65882/0.68596. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65719/0.68424. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65434/0.68488. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65723/0.68599. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65426/0.68711. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.65238/0.68709. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65292/0.68801. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64999/0.68825. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.65233/0.68760. Took 0.20 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69192/0.69518. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68987/0.69615. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68850/0.69729. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68693/0.69818. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68654/0.69941. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68526/0.70056. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68410/0.70138. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68350/0.70220. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68342/0.70316. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68214/0.70366. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68182/0.70438. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68133/0.70478. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68016/0.70479. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68001/0.70469. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68097/0.70518. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.67970/0.70507. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67935/0.70501. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67902/0.70503. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67796/0.70509. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67810/0.70580. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67875/0.70577. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67720/0.70569. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67660/0.70581. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67650/0.70656. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67693/0.70641. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67603/0.70614. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67508/0.70571. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67493/0.70638. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67482/0.70643. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67413/0.70649. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67493/0.70702. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67311/0.70746. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67303/0.70839. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67259/0.70810. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67121/0.70771. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67148/0.70678. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67102/0.70772. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67061/0.70800. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66871/0.70827. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67022/0.70850. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66871/0.70884. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66788/0.70929. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66718/0.71037. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66653/0.71062. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66553/0.71007. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66627/0.71078. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66566/0.71143. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66681/0.71099. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66418/0.71176. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66371/0.71250. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66518/0.71211. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66135/0.71182. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65965/0.71278. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66061/0.71294. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65970/0.71297. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65959/0.71303. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65902/0.71392. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65875/0.71455. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65926/0.71347. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65557/0.71624. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65652/0.71696. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65681/0.71753. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65503/0.71772. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65452/0.71718. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65296/0.71738. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65340/0.71736. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65337/0.71827. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65157/0.71763. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65158/0.71833. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65025/0.71937. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65120/0.72066. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65181/0.72245. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64731/0.72590. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64875/0.72419. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64553/0.72651. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64507/0.72355. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64398/0.72419. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64469/0.72396. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64339/0.72256. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64141/0.72563. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63962/0.72700. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64268/0.72912. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64121/0.73083. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63981/0.73122. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63929/0.73169. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63868/0.73267. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63754/0.73217. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63818/0.73679. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63699/0.73311. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63306/0.73853. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63304/0.74074. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63464/0.74031. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63099/0.74091. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63177/0.74449. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63066/0.74428. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63221/0.74107. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63064/0.74261. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63049/0.74435. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62812/0.74476. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62897/0.74559. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69546/0.69301. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69344/0.69122. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69291/0.68964. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69114/0.68867. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69092/0.68818. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69022/0.68806. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68917/0.68798. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68862/0.68834. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68734/0.68876. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68666/0.68949. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68482/0.68971. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68496/0.69024. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68342/0.69132. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68302/0.69147. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68139/0.69201. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68115/0.69230. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68091/0.69293. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.67959/0.69324. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68030/0.69361. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.67947/0.69368. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67880/0.69433. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67785/0.69469. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67762/0.69484. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67780/0.69446. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67649/0.69423. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67532/0.69436. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67475/0.69461. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67474/0.69504. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67376/0.69537. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67529/0.69438. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67220/0.69400. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67483/0.69401. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67326/0.69299. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67373/0.69270. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67284/0.69269. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67150/0.69284. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67094/0.69292. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66880/0.69266. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66879/0.69233. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.66840/0.69080. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66981/0.69087. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66734/0.68965. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66689/0.68908. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66794/0.68887. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66367/0.68830. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66382/0.68733. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66476/0.68754. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66324/0.68727. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66236/0.68735. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66247/0.68576. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65993/0.68674. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66057/0.68527. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65808/0.68646. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.65925/0.68522. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65838/0.68309. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65915/0.68445. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65854/0.68380. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65748/0.68283. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65722/0.67936. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65440/0.67854. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65513/0.67887. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65066/0.67862. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64975/0.67774. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65243/0.67786. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65257/0.67832. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64734/0.67680. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65055/0.67591. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65044/0.67786. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64524/0.67805. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64717/0.67388. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64810/0.67347. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64819/0.67503. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64699/0.67379. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64352/0.67362. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64333/0.67454. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63947/0.67383. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64124/0.67250. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63883/0.67513. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63653/0.67225. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63652/0.67106. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63764/0.67211. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63517/0.66827. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63993/0.66904. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63698/0.67160. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63051/0.66866. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62688/0.66408. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62999/0.66542. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62856/0.66904. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62364/0.66845. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.62847/0.67123. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62340/0.66748. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62246/0.66745. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62139/0.66600. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62146/0.66504. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62033/0.66401. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62156/0.66400. Took 0.21 sec\n",
      "Epoch 96, Loss(train/val) 0.62272/0.66436. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61758/0.66576. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61962/0.66458. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61687/0.66354. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69205/0.69077. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69123/0.69040. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68955/0.69035. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68948/0.69093. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68925/0.69141. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68772/0.69216. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68661/0.69297. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68674/0.69403. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68509/0.69527. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68494/0.69675. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68473/0.69815. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68346/0.69993. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68192/0.70162. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68414/0.70306. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68190/0.70486. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68128/0.70660. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68165/0.70776. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68191/0.70870. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68095/0.70983. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68088/0.71065. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67963/0.71165. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68046/0.71262. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67904/0.71366. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67970/0.71361. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67834/0.71423. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67899/0.71470. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67956/0.71512. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67719/0.71549. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67768/0.71568. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67738/0.71595. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67883/0.71711. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67751/0.71764. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67707/0.71790. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67633/0.71831. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67568/0.71871. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67574/0.71917. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67484/0.71994. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67580/0.71987. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67442/0.72059. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67522/0.72109. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67342/0.72209. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67498/0.72257. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67254/0.72311. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67237/0.72377. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67478/0.72324. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67271/0.72398. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67052/0.72491. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67336/0.72543. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67126/0.72580. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67256/0.72564. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66907/0.72684. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67108/0.72688. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66779/0.72835. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66972/0.72788. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66725/0.72851. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66688/0.72883. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66735/0.72989. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66725/0.73026. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66807/0.73088. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66450/0.73105. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66621/0.73111. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66557/0.73263. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66530/0.73369. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66635/0.73403. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66460/0.73489. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66370/0.73603. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66415/0.73614. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66192/0.73706. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66125/0.73774. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66251/0.73804. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66511/0.73742. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66074/0.73700. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66246/0.73876. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66358/0.73874. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66055/0.73811. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65843/0.74067. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66062/0.74012. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65826/0.74015. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65547/0.74173. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65539/0.74189. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65762/0.74333. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65690/0.74315. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65342/0.74340. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65316/0.74343. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65465/0.74469. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65447/0.74390. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65499/0.74514. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65672/0.74405. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65262/0.74435. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65361/0.74669. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65112/0.74683. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64894/0.74899. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64919/0.74784. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64893/0.74906. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64815/0.74801. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64777/0.74756. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64760/0.74864. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64623/0.75074. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64743/0.74983. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64551/0.75166. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69225/0.69966. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69012/0.69727. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68873/0.69518. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68755/0.69400. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68730/0.69252. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68630/0.69138. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68531/0.69069. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68425/0.68993. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68304/0.68975. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68301/0.68936. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68225/0.68971. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68143/0.68982. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68016/0.68966. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68040/0.69022. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68014/0.69040. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.67955/0.69098. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.67857/0.69173. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67839/0.69211. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.67852/0.69172. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67876/0.69253. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67810/0.69283. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67763/0.69352. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67667/0.69331. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67728/0.69317. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67602/0.69345. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67535/0.69394. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67529/0.69341. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67456/0.69373. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67484/0.69298. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67413/0.69419. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67296/0.69422. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67256/0.69475. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67189/0.69487. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67264/0.69501. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67157/0.69508. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67244/0.69496. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67127/0.69490. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67006/0.69466. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66873/0.69630. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66908/0.69704. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66796/0.69581. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66872/0.69749. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66631/0.69849. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66668/0.69914. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66531/0.69692. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66575/0.69735. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66367/0.69803. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66448/0.69933. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66386/0.70012. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66101/0.70002. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66001/0.69996. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.65910/0.70068. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65901/0.70036. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65887/0.69905. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65727/0.70046. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65814/0.69955. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65429/0.70320. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65353/0.70236. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65391/0.70306. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65312/0.70332. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65072/0.70106. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65090/0.70249. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64789/0.70327. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64964/0.69981. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64934/0.70083. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64230/0.69915. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64320/0.70015. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64456/0.70035. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64159/0.70060. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64006/0.70059. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64126/0.69847. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63875/0.69643. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63777/0.69867. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.63683/0.69775. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63239/0.69604. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63500/0.69774. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63766/0.69698. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63298/0.69793. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63270/0.69650. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62884/0.69881. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62943/0.69944. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.62763/0.69540. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63032/0.69577. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62896/0.69784. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62706/0.69650. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62815/0.69665. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62347/0.69741. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62504/0.69894. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62157/0.69445. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62165/0.69430. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62050/0.69485. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62168/0.69472. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61704/0.69822. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61718/0.70129. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61617/0.69928. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61870/0.69894. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61263/0.69393. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61372/0.69342. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60923/0.69534. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61159/0.70120. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69516/0.69043. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69302/0.68876. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69161/0.68838. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69106/0.68833. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69098/0.68867. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69046/0.68929. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68993/0.68969. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68961/0.69008. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68924/0.69066. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68882/0.69113. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68908/0.69197. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68829/0.69278. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68817/0.69367. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68682/0.69449. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68738/0.69533. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68582/0.69695. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68518/0.69788. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68469/0.69964. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68483/0.70067. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68374/0.70187. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68327/0.70323. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68247/0.70360. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68287/0.70499. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68260/0.70529. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68254/0.70649. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68084/0.70709. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68163/0.70792. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68205/0.70815. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68123/0.70868. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68103/0.70972. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68019/0.70957. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67858/0.71068. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67801/0.71186. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67925/0.71164. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67844/0.71115. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67706/0.71153. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67580/0.71256. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67530/0.71215. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67604/0.71257. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67388/0.71365. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67443/0.71440. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67445/0.71341. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67290/0.71474. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67272/0.71520. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67149/0.71583. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66990/0.71662. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66862/0.71537. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66991/0.71719. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67019/0.71796. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66943/0.71777. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66918/0.71666. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66902/0.71916. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66788/0.71728. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66768/0.71896. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66471/0.72068. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66475/0.71966. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66479/0.72177. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66472/0.72353. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66639/0.72200. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66291/0.72409. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66266/0.72259. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66187/0.72248. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66226/0.72126. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66279/0.72354. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66086/0.72412. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66186/0.72394. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65866/0.72339. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66036/0.72487. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65931/0.72667. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65686/0.72713. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65764/0.72605. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65431/0.72890. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65295/0.73177. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65747/0.73096. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65309/0.73222. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65232/0.73249. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65466/0.73302. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65435/0.73068. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65429/0.73280. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65326/0.73156. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65271/0.73227. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65263/0.73505. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65048/0.73351. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65263/0.73475. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64903/0.73550. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65096/0.73653. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64684/0.73630. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65065/0.73935. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64410/0.74055. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64691/0.74123. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64523/0.74160. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64279/0.74326. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64407/0.74250. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64235/0.74318. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64534/0.74463. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63855/0.74568. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64304/0.74728. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64251/0.74733. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63792/0.74911. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63959/0.74956. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69392/0.69382. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69228/0.69194. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69053/0.69084. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69001/0.69030. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68915/0.69001. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68874/0.68989. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68769/0.68991. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68702/0.68994. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68607/0.69008. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68531/0.69029. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68467/0.69045. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68306/0.69069. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68294/0.69090. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68267/0.69117. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68151/0.69133. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68141/0.69147. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.67993/0.69152. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.67916/0.69161. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67887/0.69178. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.67840/0.69164. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67880/0.69161. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67831/0.69164. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67707/0.69144. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67608/0.69125. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67633/0.69140. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67496/0.69157. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67444/0.69195. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67453/0.69242. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67330/0.69267. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67317/0.69290. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67322/0.69275. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67115/0.69284. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.66947/0.69360. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67051/0.69371. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.66938/0.69426. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.66753/0.69468. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67046/0.69471. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66859/0.69501. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66771/0.69584. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.66515/0.69631. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66690/0.69644. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66522/0.69712. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66339/0.69862. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66338/0.69893. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66447/0.69905. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66165/0.69920. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66210/0.70016. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66182/0.70151. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66070/0.70122. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66075/0.70147. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65868/0.70229. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.65661/0.70315. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65886/0.70369. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.65812/0.70417. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65428/0.70555. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.65459/0.70682. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65519/0.70766. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65312/0.70835. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65237/0.70809. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65044/0.70932. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.64958/0.71109. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64766/0.71157. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64963/0.71223. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.64889/0.71200. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64690/0.71278. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.64402/0.71476. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64521/0.71513. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.64634/0.71692. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64375/0.71773. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64375/0.71730. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64114/0.71871. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64060/0.72054. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64255/0.72124. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64260/0.72319. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63817/0.72244. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63715/0.72258. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63747/0.72431. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63668/0.72662. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63713/0.72877. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63518/0.72956. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63146/0.73018. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63190/0.73119. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63177/0.73111. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63094/0.73303. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62904/0.73546. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63194/0.73620. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62907/0.73593. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62915/0.73803. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62844/0.73797. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.62168/0.73948. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62516/0.74021. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62284/0.74222. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62449/0.74432. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62043/0.74714. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62050/0.74695. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62045/0.74795. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61827/0.75019. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.61831/0.75178. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61807/0.75375. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61406/0.75530. Took 0.19 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69131/0.67757. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69037/0.67736. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69047/0.67725. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69014/0.67693. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69016/0.67684. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68952/0.67585. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68941/0.67511. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68975/0.67456. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68838/0.67428. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68825/0.67413. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68778/0.67415. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68786/0.67357. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68685/0.67257. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68614/0.67273. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68616/0.67297. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68573/0.67126. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68485/0.67119. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68402/0.67192. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68450/0.67190. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68255/0.67327. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68224/0.67216. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68079/0.67256. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67995/0.67305. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68119/0.67444. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67810/0.67323. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68067/0.67557. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67918/0.67427. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67858/0.67681. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67746/0.67871. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67516/0.67949. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67829/0.68061. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67702/0.68068. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67357/0.68485. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67284/0.68346. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67386/0.68803. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67128/0.68860. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67372/0.69212. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67154/0.69050. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67170/0.69370. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66999/0.69565. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66781/0.69537. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66947/0.69907. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66651/0.70082. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66842/0.70307. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66716/0.70824. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66581/0.70708. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.66438/0.70867. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66446/0.71016. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66403/0.71066. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66377/0.71327. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66353/0.71080. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66144/0.71998. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65878/0.72085. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.65954/0.71807. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65787/0.72346. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65943/0.72376. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66002/0.72463. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65753/0.73027. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65667/0.73022. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65642/0.72886. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65462/0.72653. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65472/0.73222. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65366/0.73463. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65166/0.73901. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64984/0.73617. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65405/0.73444. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65246/0.74087. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65045/0.74026. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64860/0.74195. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65012/0.74165. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64679/0.74509. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65097/0.74425. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64434/0.74801. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64481/0.74535. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64630/0.74952. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64447/0.74686. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64466/0.74614. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64481/0.75118. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64308/0.75861. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64395/0.75488. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64055/0.75117. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63960/0.75422. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63851/0.75909. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64053/0.75758. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63597/0.75654. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64024/0.75947. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63615/0.75861. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63615/0.76503. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63439/0.76391. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63415/0.76271. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63179/0.76421. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63613/0.76073. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63034/0.76354. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62838/0.76707. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63432/0.77494. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62976/0.76829. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63503/0.76479. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62735/0.77016. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63050/0.76440. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62642/0.76777. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69298/0.69004. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69187/0.68953. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69199/0.68933. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69176/0.68939. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69069/0.68958. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69034/0.68983. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68960/0.69021. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69016/0.69058. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68898/0.69109. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68785/0.69186. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68887/0.69269. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68786/0.69354. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68742/0.69452. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68580/0.69539. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68630/0.69641. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68506/0.69785. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68527/0.69885. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68452/0.70006. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68341/0.70119. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68273/0.70227. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68365/0.70350. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68231/0.70447. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68131/0.70560. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68221/0.70593. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68198/0.70690. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67947/0.70765. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67966/0.70776. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67967/0.70859. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68049/0.70873. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67821/0.70941. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67749/0.70982. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67694/0.71028. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67663/0.71037. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67673/0.71090. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67595/0.71088. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67509/0.71105. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67270/0.71236. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67389/0.71234. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67350/0.71263. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67338/0.71296. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67056/0.71304. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67174/0.71360. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67124/0.71377. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67092/0.71433. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67117/0.71480. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66902/0.71540. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66947/0.71472. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66824/0.71517. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66788/0.71602. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66794/0.71629. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66551/0.71717. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66523/0.71697. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66652/0.71744. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66464/0.71755. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66357/0.71845. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66394/0.71877. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66340/0.71937. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66588/0.71915. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66221/0.71968. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66279/0.71974. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66296/0.71989. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65925/0.72135. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66050/0.72246. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65581/0.72296. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65740/0.72303. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65543/0.72484. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66002/0.72606. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65664/0.72562. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65688/0.72646. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65338/0.72688. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65456/0.72777. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65498/0.72873. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65488/0.72826. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65425/0.72925. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65326/0.72972. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65284/0.73069. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65115/0.73003. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65040/0.73012. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65149/0.73148. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64816/0.73309. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64929/0.73088. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64828/0.73332. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64529/0.73362. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64688/0.73393. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64600/0.73401. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64758/0.73553. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64555/0.73549. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64440/0.73737. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64209/0.73654. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64329/0.73716. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64319/0.73955. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64154/0.73892. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63942/0.73825. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63862/0.74121. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63754/0.74054. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63973/0.74225. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63821/0.74231. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63497/0.74339. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63362/0.74274. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63385/0.74605. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69483/0.69592. Took 0.34 sec\n",
      "Epoch 1, Loss(train/val) 0.69268/0.69750. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69193/0.69823. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69169/0.69898. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69103/0.69943. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69092/0.70013. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68983/0.70096. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69082/0.70156. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68890/0.70234. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68843/0.70336. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68917/0.70434. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68850/0.70508. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68794/0.70580. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68764/0.70693. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68839/0.70737. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68888/0.70785. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68601/0.70818. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68616/0.70887. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68600/0.70930. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68734/0.70965. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68670/0.70963. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68548/0.71057. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68553/0.71057. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68543/0.71063. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68602/0.71069. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68474/0.71075. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68419/0.71118. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68483/0.71087. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68344/0.71141. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68553/0.71138. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68416/0.71093. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68379/0.71135. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68258/0.71142. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68297/0.71158. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68270/0.71139. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68091/0.71119. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68279/0.71159. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68163/0.71144. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68102/0.71130. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68077/0.71156. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68047/0.71172. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68000/0.71157. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68011/0.71165. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67939/0.71193. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67794/0.71248. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67717/0.71281. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67624/0.71283. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67632/0.71236. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67573/0.71212. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67522/0.71256. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67452/0.71205. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67466/0.71248. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67504/0.71263. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67460/0.71269. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67394/0.71309. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67276/0.71336. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67002/0.71359. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67224/0.71407. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67116/0.71253. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67090/0.71270. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66941/0.71406. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66565/0.71389. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66505/0.71530. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66727/0.71665. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66386/0.71562. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66350/0.71629. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66350/0.71466. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66084/0.71516. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66117/0.71674. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65832/0.71798. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65714/0.71767. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65503/0.71911. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65728/0.72036. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65705/0.71995. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65242/0.71923. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65391/0.72063. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64995/0.72119. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64806/0.72189. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64629/0.72150. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64494/0.72523. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64745/0.72426. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64641/0.72664. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64516/0.72616. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64485/0.72486. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64596/0.72874. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64262/0.72856. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64057/0.72993. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63927/0.73166. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64081/0.72857. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63341/0.72982. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63536/0.73334. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63867/0.73377. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63619/0.73478. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63379/0.73718. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63421/0.73424. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63120/0.73691. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63391/0.73875. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62552/0.74112. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62843/0.74006. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62903/0.74058. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69574/0.69135. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69353/0.69238. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69254/0.69334. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69228/0.69433. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69179/0.69515. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69216/0.69579. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.69142/0.69624. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69026/0.69704. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69101/0.69758. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68983/0.69834. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68913/0.69917. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68887/0.69993. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68917/0.70046. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68857/0.70131. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68883/0.70169. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68926/0.70249. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68803/0.70312. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68728/0.70387. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68709/0.70464. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68584/0.70560. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68637/0.70632. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68664/0.70662. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68481/0.70734. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68528/0.70745. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68569/0.70738. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68548/0.70760. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68415/0.70799. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68585/0.70830. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68466/0.70826. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68517/0.70829. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68490/0.70824. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68358/0.70792. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68398/0.70838. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68384/0.70805. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68212/0.70818. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68344/0.70819. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68266/0.70791. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68383/0.70811. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68166/0.70848. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68053/0.70906. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68022/0.70902. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67997/0.70963. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.67999/0.70957. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68083/0.71032. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67995/0.70910. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67872/0.70879. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.67810/0.70968. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67654/0.70961. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67649/0.71030. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67681/0.71064. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67626/0.71138. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67460/0.71178. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67536/0.71217. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67441/0.71163. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67103/0.71308. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67185/0.71393. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67329/0.71487. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67189/0.71463. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67241/0.71521. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66869/0.71598. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66896/0.71514. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66819/0.71590. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66775/0.71555. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66792/0.71670. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66777/0.71714. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66515/0.71551. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66580/0.71644. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66566/0.71818. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66259/0.71722. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66387/0.71789. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66200/0.71830. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66044/0.71990. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65965/0.72048. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65964/0.72068. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65844/0.72089. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65950/0.72100. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65622/0.72087. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65527/0.72365. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65501/0.72397. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65183/0.72272. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65066/0.72538. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64933/0.72667. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65105/0.72585. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64774/0.72801. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64627/0.72724. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64879/0.72902. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64664/0.72961. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64708/0.73024. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64721/0.72919. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64236/0.73033. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64241/0.73100. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64570/0.72862. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63872/0.73157. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63892/0.73380. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64001/0.73588. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63547/0.73878. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63932/0.73526. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63708/0.73584. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63512/0.73671. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63162/0.73548. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69580/0.69095. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69366/0.68991. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69300/0.68894. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69252/0.68814. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69189/0.68743. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69133/0.68669. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69198/0.68618. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69055/0.68581. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69036/0.68565. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69077/0.68564. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68997/0.68567. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68944/0.68580. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68935/0.68611. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68961/0.68645. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68916/0.68683. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68910/0.68703. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68836/0.68738. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68798/0.68759. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68818/0.68776. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68819/0.68811. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68809/0.68822. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68814/0.68868. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68788/0.68887. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68723/0.68926. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68755/0.68967. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68734/0.68993. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68653/0.69012. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68670/0.69043. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68578/0.69088. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68769/0.69066. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68560/0.69043. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68527/0.69072. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68534/0.69147. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68550/0.69174. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68569/0.69243. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68579/0.69276. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68313/0.69306. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68450/0.69358. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68372/0.69325. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68440/0.69404. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68380/0.69418. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68267/0.69430. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68307/0.69442. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68312/0.69504. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68183/0.69530. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68034/0.69600. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68159/0.69671. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68086/0.69731. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68056/0.69765. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67894/0.69924. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68082/0.69842. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67989/0.69960. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67801/0.70059. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67887/0.70102. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67672/0.70138. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67563/0.70263. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67895/0.70234. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67709/0.70267. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67514/0.70318. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67375/0.70465. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67434/0.70490. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67527/0.70658. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67358/0.70693. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67249/0.70908. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67409/0.70898. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67315/0.70996. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67189/0.71185. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67249/0.71211. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66944/0.71333. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67117/0.71447. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66823/0.71466. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66729/0.71551. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66859/0.71595. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66592/0.71625. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66934/0.71796. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66774/0.71861. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66473/0.71953. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66599/0.72126. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66467/0.72083. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66442/0.72213. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66272/0.72214. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66055/0.72302. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65997/0.72502. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66068/0.72785. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66035/0.72935. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65809/0.72873. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65809/0.72878. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65780/0.72882. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65731/0.72961. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65772/0.73183. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65627/0.73459. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65480/0.73670. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65607/0.73760. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65537/0.73819. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65425/0.73601. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65149/0.73668. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65042/0.74101. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65106/0.74161. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64935/0.74247. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64888/0.74456. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69459/0.69580. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69342/0.69445. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69312/0.69411. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69237/0.69399. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69212/0.69359. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69226/0.69326. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69222/0.69321. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69160/0.69297. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69164/0.69259. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69206/0.69236. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69207/0.69244. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69108/0.69228. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69087/0.69240. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68990/0.69218. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68958/0.69177. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68924/0.69129. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68961/0.69162. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68924/0.69144. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68979/0.69150. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68936/0.69141. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68752/0.69143. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68801/0.69120. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68765/0.69118. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68738/0.69213. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68699/0.69218. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68661/0.69233. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68612/0.69289. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68521/0.69362. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68529/0.69343. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68483/0.69455. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68480/0.69561. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68378/0.69578. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68393/0.69657. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68338/0.69672. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68318/0.69770. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68096/0.69842. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68080/0.69846. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68011/0.70042. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68222/0.69975. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68099/0.70092. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67911/0.70190. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67835/0.70235. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67823/0.70166. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67844/0.70372. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67615/0.70504. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67757/0.70412. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67635/0.70484. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67497/0.70556. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67361/0.70679. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67453/0.70650. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67479/0.70655. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67197/0.70724. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67108/0.70875. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67110/0.70967. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67047/0.71161. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66888/0.71057. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67073/0.71234. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66687/0.71310. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66646/0.71261. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66557/0.71439. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66532/0.71745. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66190/0.71794. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66629/0.71790. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66283/0.71784. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66125/0.72109. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66173/0.72187. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66192/0.72163. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66061/0.72215. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66203/0.72223. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65647/0.72596. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65991/0.72512. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65723/0.72674. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65621/0.72737. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65292/0.72692. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65430/0.72873. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65273/0.73130. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65146/0.73425. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65209/0.73383. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65285/0.73232. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64917/0.73402. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65048/0.73454. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64776/0.73889. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64993/0.73866. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64473/0.74209. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64548/0.74167. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64351/0.74476. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64198/0.74373. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64072/0.74757. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64016/0.74758. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63856/0.75206. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64342/0.75355. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63894/0.75374. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63757/0.75210. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63735/0.75739. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63252/0.75656. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62952/0.75908. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63633/0.75520. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62919/0.75615. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63259/0.75577. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63146/0.76017. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69347/0.69600. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69275/0.69573. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69249/0.69564. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69218/0.69539. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69154/0.69529. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69207/0.69523. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69131/0.69520. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69076/0.69523. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69054/0.69518. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69072/0.69515. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68995/0.69522. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68981/0.69531. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68985/0.69549. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68932/0.69559. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68972/0.69551. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68933/0.69540. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68893/0.69551. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68890/0.69550. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68950/0.69549. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68745/0.69559. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68820/0.69571. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68777/0.69567. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68741/0.69576. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68829/0.69568. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68674/0.69588. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68740/0.69568. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68722/0.69583. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68588/0.69573. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68760/0.69579. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68639/0.69586. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68593/0.69596. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68520/0.69607. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68587/0.69617. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68478/0.69637. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68468/0.69657. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68539/0.69629. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68364/0.69626. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68396/0.69623. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68509/0.69615. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68342/0.69597. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68439/0.69598. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68292/0.69602. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68259/0.69591. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68398/0.69595. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68213/0.69605. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68465/0.69594. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68140/0.69614. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68173/0.69621. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.68155/0.69606. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.68022/0.69632. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.68035/0.69687. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.68000/0.69682. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.68001/0.69683. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67955/0.69627. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.68094/0.69607. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67955/0.69637. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67916/0.69651. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67750/0.69605. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67757/0.69578. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.67815/0.69596. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67859/0.69574. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67549/0.69623. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.67649/0.69650. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67542/0.69628. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.67548/0.69630. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.67582/0.69624. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.67416/0.69625. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.67424/0.69651. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.67487/0.69659. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67466/0.69622. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67355/0.69618. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67338/0.69594. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.67551/0.69594. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67410/0.69570. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67287/0.69590. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67181/0.69621. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.67133/0.69609. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.66997/0.69636. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66873/0.69696. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66964/0.69768. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66976/0.69657. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.67001/0.69686. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66793/0.69736. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.67013/0.69654. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66930/0.69648. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66569/0.69607. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66629/0.69532. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66912/0.69535. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66503/0.69653. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66262/0.69630. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66708/0.69671. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66683/0.69607. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66421/0.69670. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66506/0.69793. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66434/0.69862. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.66127/0.69676. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66091/0.69827. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66254/0.69788. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66168/0.69850. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66023/0.69898. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69489/0.69491. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69345/0.69424. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69260/0.69353. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69232/0.69320. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69139/0.69274. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69168/0.69224. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69115/0.69238. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69098/0.69209. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69057/0.69210. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69034/0.69215. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69041/0.69240. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69012/0.69245. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68998/0.69241. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68991/0.69224. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68932/0.69239. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68970/0.69264. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68922/0.69245. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68925/0.69248. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68833/0.69313. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68753/0.69336. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68792/0.69352. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68890/0.69420. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68727/0.69411. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68737/0.69381. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68750/0.69410. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68732/0.69418. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68647/0.69449. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68730/0.69453. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68640/0.69460. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68623/0.69430. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68558/0.69424. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68454/0.69448. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68447/0.69513. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68521/0.69499. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68391/0.69582. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68304/0.69542. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68367/0.69507. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68270/0.69562. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68278/0.69567. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68142/0.69612. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68180/0.69591. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68238/0.69575. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68105/0.69549. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68131/0.69583. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68027/0.69556. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68075/0.69562. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67976/0.69493. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68080/0.69563. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67779/0.69595. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68000/0.69591. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67706/0.69632. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67733/0.69519. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67651/0.69639. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67659/0.69545. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67569/0.69662. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67455/0.69536. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67479/0.69397. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67439/0.69431. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67449/0.69505. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67375/0.69350. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67226/0.69365. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67253/0.69412. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67108/0.69264. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67139/0.69253. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66963/0.69246. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66880/0.69259. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67187/0.69276. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66948/0.69195. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66773/0.69196. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66830/0.69054. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66750/0.69136. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66431/0.69047. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66355/0.68990. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66616/0.69063. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66446/0.68942. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66435/0.68867. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66045/0.68959. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66112/0.68808. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65976/0.68669. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65809/0.68678. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65889/0.68793. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65902/0.68759. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65487/0.68765. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65754/0.68669. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65464/0.68713. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65750/0.68522. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65422/0.68482. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65627/0.68460. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65167/0.68438. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64953/0.68591. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64977/0.68325. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64577/0.68445. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64432/0.68337. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64688/0.68136. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64402/0.68205. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64613/0.68062. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64534/0.67945. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64278/0.67932. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64321/0.67904. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64324/0.67974. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69497/0.69033. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69407/0.68775. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69343/0.68772. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69349/0.68804. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69259/0.68786. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69303/0.68771. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69194/0.68765. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69195/0.68813. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69207/0.68809. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69199/0.68776. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69127/0.68780. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69109/0.68776. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69032/0.68762. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69095/0.68762. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69126/0.68799. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69114/0.68826. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69047/0.68826. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69072/0.68847. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68954/0.68839. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.69078/0.68844. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.69083/0.68873. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68876/0.68905. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68935/0.68898. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68956/0.68902. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68914/0.68940. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68830/0.68905. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68889/0.68882. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68842/0.68923. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68739/0.68936. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68822/0.68918. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68747/0.68858. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68807/0.68889. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68727/0.68907. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68719/0.68949. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68585/0.68937. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68561/0.68973. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68617/0.69009. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68579/0.68948. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68542/0.68976. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68534/0.68967. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68379/0.69126. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68343/0.69155. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68370/0.69082. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68391/0.69184. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68348/0.69230. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68276/0.69394. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68283/0.69298. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68223/0.69301. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68040/0.69368. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67966/0.69262. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67997/0.69310. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68028/0.69447. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67749/0.69698. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67847/0.69459. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67935/0.69581. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67716/0.69691. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67588/0.69581. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67669/0.69705. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67442/0.69739. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67462/0.69656. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67461/0.69716. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67443/0.69694. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67321/0.69988. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67356/0.69910. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67154/0.69922. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67020/0.69917. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67132/0.69825. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66940/0.70028. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66931/0.70031. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66966/0.69874. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66898/0.70204. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66682/0.70616. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66670/0.70408. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66406/0.70393. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66480/0.70499. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66392/0.70347. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66199/0.70590. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66195/0.70889. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66030/0.71071. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66044/0.70985. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66128/0.71001. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65766/0.71058. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65710/0.71283. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65697/0.71167. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65549/0.71183. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65606/0.71327. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65262/0.71299. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65455/0.71774. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65203/0.71829. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65069/0.71989. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65044/0.71984. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64906/0.72194. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64859/0.72250. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64755/0.72157. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64606/0.72173. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64501/0.72229. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64442/0.72547. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64070/0.72568. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64200/0.72631. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63677/0.72584. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69409/0.69037. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69267/0.68949. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69332/0.68983. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69268/0.69020. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69227/0.69038. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69210/0.69060. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69194/0.69094. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69191/0.69112. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69108/0.69105. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69149/0.69131. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69123/0.69165. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69075/0.69184. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69052/0.69208. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.69001/0.69221. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69023/0.69243. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69029/0.69249. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68912/0.69258. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68993/0.69264. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69031/0.69291. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68917/0.69295. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.69002/0.69311. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68987/0.69345. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68857/0.69347. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68793/0.69326. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68797/0.69313. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68797/0.69341. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68700/0.69341. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68810/0.69312. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68693/0.69344. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68692/0.69335. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68711/0.69314. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68578/0.69308. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68678/0.69280. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68541/0.69300. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68467/0.69295. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68528/0.69341. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68520/0.69275. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68514/0.69318. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68402/0.69303. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68425/0.69306. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68351/0.69268. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68399/0.69277. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68246/0.69288. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68295/0.69283. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68298/0.69325. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68127/0.69331. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68127/0.69371. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68275/0.69419. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68083/0.69428. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67982/0.69427. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68066/0.69440. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67860/0.69511. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67891/0.69568. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67885/0.69644. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67738/0.69693. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67899/0.69569. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67751/0.69686. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67650/0.69699. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67483/0.69690. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67563/0.69821. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67667/0.69815. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67395/0.69913. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67349/0.69916. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67388/0.70000. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67243/0.69942. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67413/0.69952. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.67113/0.70044. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67243/0.70008. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67078/0.69953. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67158/0.70016. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66900/0.69966. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66871/0.70082. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66786/0.70065. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66819/0.70170. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66436/0.70094. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66400/0.70353. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66531/0.70289. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66256/0.70229. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66481/0.70236. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66167/0.70389. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66371/0.70329. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66116/0.70390. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66143/0.70401. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65939/0.70469. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66098/0.70564. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65667/0.70390. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65631/0.70514. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65469/0.70521. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65731/0.70450. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65322/0.70536. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65123/0.70623. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65372/0.70675. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64918/0.70759. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65289/0.70674. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65056/0.70743. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65014/0.70645. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64962/0.70831. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64729/0.70886. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64412/0.70819. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64594/0.70791. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69393/0.69139. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69314/0.69103. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69271/0.69078. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69301/0.69061. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69206/0.69039. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69233/0.69019. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69159/0.69002. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69141/0.68978. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69126/0.68956. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69113/0.68927. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69001/0.68912. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69040/0.68889. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68973/0.68867. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68955/0.68871. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68940/0.68869. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68905/0.68846. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68852/0.68840. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68696/0.68862. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68775/0.68877. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68792/0.68913. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68688/0.68914. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68709/0.68923. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68825/0.68927. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68708/0.68989. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68479/0.69021. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68634/0.69026. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68485/0.69036. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68510/0.69092. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68428/0.69117. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68415/0.69139. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68365/0.69158. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68387/0.69244. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68339/0.69282. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68333/0.69336. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68070/0.69330. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68179/0.69440. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68133/0.69493. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67976/0.69516. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67985/0.69524. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67868/0.69667. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67904/0.69717. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67732/0.69761. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67822/0.69848. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67743/0.69907. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67490/0.69921. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67392/0.69977. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67519/0.70038. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67357/0.70011. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67347/0.70140. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67032/0.70193. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67218/0.70248. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67238/0.70311. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66825/0.70328. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66805/0.70413. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66859/0.70500. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67147/0.70434. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66653/0.70583. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66852/0.70722. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66581/0.70702. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66717/0.70746. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66323/0.70983. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66332/0.71050. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66182/0.71014. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66287/0.71059. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66130/0.71321. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66211/0.71369. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65881/0.71273. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65996/0.71456. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65700/0.71661. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65655/0.71700. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65592/0.71744. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65717/0.71831. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65513/0.71801. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65358/0.71804. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65098/0.72008. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65247/0.72044. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65149/0.72126. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65024/0.72475. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64930/0.72388. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64764/0.72349. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64453/0.72197. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64318/0.72500. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64432/0.72225. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64619/0.72574. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64176/0.72890. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64213/0.72801. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64247/0.73248. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64060/0.72872. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63830/0.73374. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63610/0.73097. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63367/0.73493. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63052/0.73797. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63612/0.73627. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63459/0.73597. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63220/0.73631. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63038/0.74144. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63243/0.74161. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62616/0.73940. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62394/0.74330. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62163/0.74269. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69350/0.69033. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69208/0.68848. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69137/0.68832. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69067/0.68787. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69101/0.68733. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68996/0.68715. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68971/0.68694. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68977/0.68680. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68941/0.68647. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68911/0.68697. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68804/0.68679. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68872/0.68669. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68720/0.68676. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68783/0.68695. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68686/0.68714. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68644/0.68804. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68567/0.68822. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68563/0.68818. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68629/0.68843. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68536/0.68932. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68400/0.69028. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68481/0.69096. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68414/0.69164. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68394/0.69172. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68275/0.69226. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68231/0.69287. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68234/0.69372. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68247/0.69398. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68129/0.69490. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68142/0.69542. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68042/0.69685. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68084/0.69845. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67943/0.69919. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67863/0.69946. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67891/0.70007. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67820/0.70079. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67688/0.70234. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67794/0.70193. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67538/0.70258. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67406/0.70292. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67358/0.70475. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67138/0.70596. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67298/0.70606. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66977/0.70625. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66982/0.70696. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67039/0.70798. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66730/0.70917. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66738/0.71187. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66615/0.71232. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66508/0.71180. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66551/0.71005. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66220/0.71096. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66152/0.71064. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66014/0.71244. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65924/0.71136. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65661/0.71136. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65947/0.71196. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65658/0.71202. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65459/0.71192. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65857/0.71238. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65658/0.71196. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65215/0.71176. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65490/0.71191. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65026/0.71334. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64902/0.71287. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65053/0.71308. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64905/0.71365. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64512/0.71502. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64561/0.71598. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64556/0.71671. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64379/0.71734. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64496/0.71563. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64328/0.71614. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64148/0.71727. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64246/0.71851. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64053/0.72049. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64144/0.72032. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64069/0.71946. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63981/0.72028. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63454/0.72320. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63670/0.72254. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63393/0.72437. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63484/0.72565. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63256/0.72663. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62871/0.72813. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63167/0.72802. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62943/0.73054. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62912/0.73201. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62515/0.73310. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62829/0.73459. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62654/0.73661. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62934/0.73711. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62530/0.73788. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62432/0.73842. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62472/0.73818. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62020/0.74077. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62230/0.74108. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62415/0.74012. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61980/0.74521. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61711/0.74830. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69865/0.69566. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.69540/0.69463. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69422/0.69402. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69293/0.69363. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69224/0.69339. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69172/0.69328. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69118/0.69307. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69079/0.69297. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69010/0.69287. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68926/0.69286. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68924/0.69283. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68941/0.69288. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68918/0.69286. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68808/0.69283. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68877/0.69283. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68821/0.69291. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68819/0.69288. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68761/0.69275. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68833/0.69292. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68785/0.69295. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68773/0.69286. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68696/0.69275. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68657/0.69284. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68617/0.69270. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68611/0.69262. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68584/0.69252. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68543/0.69240. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68593/0.69233. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68541/0.69221. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68615/0.69212. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68445/0.69205. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68350/0.69214. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68466/0.69187. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68476/0.69179. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68387/0.69164. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68417/0.69162. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68344/0.69135. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68294/0.69151. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68234/0.69142. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68239/0.69141. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68170/0.69112. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68132/0.69096. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68162/0.69058. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68066/0.69077. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67872/0.69073. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67976/0.69058. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67770/0.69051. Took 0.22 sec\n",
      "Epoch 47, Loss(train/val) 0.67854/0.69043. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67749/0.69031. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67740/0.69022. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67613/0.68994. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67622/0.69034. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67590/0.69010. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67584/0.69003. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67301/0.69025. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67379/0.68999. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67443/0.69028. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67326/0.69054. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67042/0.69041. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67116/0.69095. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67060/0.69095. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67027/0.69106. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66997/0.69103. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66876/0.69102. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66935/0.69155. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66794/0.69172. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66513/0.69103. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66766/0.69184. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66495/0.69187. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66380/0.69193. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66441/0.69109. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66019/0.69247. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66112/0.69258. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65871/0.69360. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65829/0.69314. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65743/0.69408. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65807/0.69510. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66037/0.69366. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65593/0.69476. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65470/0.69483. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65505/0.69412. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65310/0.69581. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65205/0.69608. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64855/0.69719. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64885/0.69710. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64922/0.69907. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64666/0.70026. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65144/0.69922. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64597/0.70093. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64603/0.70044. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64675/0.70040. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64197/0.70225. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64057/0.70205. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64687/0.70336. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63990/0.70362. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64345/0.70383. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63919/0.70420. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64065/0.70870. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63533/0.70666. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63716/0.70890. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69431/0.69380. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69206/0.69349. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69184/0.69324. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69129/0.69313. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69190/0.69277. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69078/0.69322. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69016/0.69308. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69030/0.69293. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69029/0.69267. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69006/0.69263. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68960/0.69282. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69031/0.69274. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68986/0.69251. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68939/0.69227. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68914/0.69270. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68859/0.69293. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68819/0.69313. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68843/0.69282. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68793/0.69304. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68655/0.69275. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68826/0.69286. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68681/0.69311. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68643/0.69315. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68695/0.69306. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68712/0.69365. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68698/0.69407. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68622/0.69409. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68495/0.69465. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68622/0.69508. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68578/0.69560. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68497/0.69607. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68473/0.69645. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68354/0.69720. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68318/0.69761. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68345/0.69848. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68225/0.69935. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68268/0.70022. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68310/0.70001. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68227/0.70094. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68096/0.70113. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68290/0.70139. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68121/0.70145. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68148/0.70219. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68091/0.70287. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68048/0.70357. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68059/0.70329. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68166/0.70341. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68104/0.70355. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68177/0.70296. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68026/0.70409. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67865/0.70375. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67902/0.70530. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68001/0.70536. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67765/0.70612. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67909/0.70717. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67877/0.70669. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67828/0.70792. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67805/0.70776. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67802/0.70828. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67622/0.71036. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67676/0.71000. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67625/0.70874. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67704/0.70907. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67537/0.70980. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67467/0.71032. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67637/0.71025. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67380/0.70877. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67501/0.70958. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67525/0.70943. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67432/0.71003. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67458/0.70909. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67390/0.70800. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67513/0.70717. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67339/0.70829. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67397/0.70987. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67241/0.70947. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67020/0.70866. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67320/0.70823. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67182/0.70681. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66830/0.70809. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66976/0.70932. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67063/0.70926. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66970/0.71069. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.67104/0.70900. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.67099/0.70572. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66933/0.70697. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66807/0.70693. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66924/0.70490. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66939/0.70727. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66968/0.70681. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66675/0.70868. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66596/0.70859. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66707/0.70799. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66470/0.70661. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66565/0.70491. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66412/0.70439. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66589/0.70620. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66710/0.70769. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66182/0.70764. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66155/0.70493. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69546/0.68831. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69232/0.68499. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69226/0.68409. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69239/0.68305. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69059/0.68215. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69090/0.68128. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68997/0.68101. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68959/0.67983. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68999/0.67934. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68945/0.67858. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68937/0.67766. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68924/0.67765. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68878/0.67695. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68851/0.67619. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68788/0.67599. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68734/0.67587. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68825/0.67481. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68721/0.67485. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68679/0.67477. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68763/0.67448. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68679/0.67425. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68572/0.67393. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68679/0.67377. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68660/0.67348. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68609/0.67300. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68567/0.67267. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68506/0.67309. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68570/0.67232. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68471/0.67287. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68476/0.67245. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68498/0.67247. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68382/0.67241. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68443/0.67175. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68413/0.67201. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68334/0.67223. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68379/0.67159. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68306/0.67245. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68322/0.67175. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68300/0.67217. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68305/0.67171. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68318/0.67181. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68119/0.67127. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68204/0.67179. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68157/0.67144. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68229/0.67144. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68063/0.67135. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68119/0.67079. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68083/0.67202. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68065/0.67133. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67893/0.67105. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68101/0.67034. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67909/0.67048. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67901/0.67182. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67851/0.67031. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67896/0.67000. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67736/0.66986. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67869/0.67116. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67742/0.67048. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67792/0.66887. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67804/0.66950. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67625/0.66962. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67746/0.66956. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67741/0.66908. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67574/0.66816. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67493/0.66839. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67179/0.66915. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67445/0.66855. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67261/0.66731. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67280/0.66795. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66965/0.66892. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67224/0.66867. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67133/0.66759. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66919/0.66839. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67072/0.66793. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66814/0.66855. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66903/0.66889. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66909/0.66649. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66760/0.66824. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66752/0.66643. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66472/0.66813. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66440/0.66583. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66166/0.66545. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66408/0.66591. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66075/0.66661. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66160/0.66470. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66155/0.66708. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65941/0.66828. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66086/0.66689. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65917/0.66779. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65941/0.66683. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66090/0.66668. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65685/0.66840. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65341/0.66843. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65634/0.66906. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65452/0.66959. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65152/0.67074. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64877/0.66930. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64915/0.67113. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64977/0.67045. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64842/0.67149. Took 0.19 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69418/0.68767. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69114/0.68369. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69019/0.68367. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68958/0.68409. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68876/0.68372. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68929/0.68457. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68882/0.68484. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68780/0.68475. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68686/0.68515. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68710/0.68522. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68652/0.68588. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68673/0.68599. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68582/0.68690. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68576/0.68693. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68543/0.68735. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68498/0.68788. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68474/0.68884. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68381/0.68874. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68322/0.68884. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68293/0.68972. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68241/0.69037. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68127/0.69100. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68147/0.69148. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68137/0.69197. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67972/0.69241. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67920/0.69267. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67896/0.69456. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67839/0.69471. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67886/0.69492. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67618/0.69644. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67718/0.69711. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67716/0.69705. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67522/0.69829. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67525/0.69871. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67475/0.69878. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67421/0.70014. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67348/0.70128. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67406/0.70077. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67291/0.70060. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67275/0.70192. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67373/0.70099. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67234/0.70101. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67006/0.70141. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67031/0.70141. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66665/0.70146. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66917/0.70306. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67005/0.70231. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66629/0.70143. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66850/0.70248. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66623/0.70188. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66674/0.70150. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66403/0.70153. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66575/0.70057. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66628/0.70061. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66361/0.70082. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66103/0.70139. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66330/0.70170. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66052/0.70132. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66021/0.69935. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66147/0.69954. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65871/0.70015. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65840/0.69890. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65848/0.69941. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65792/0.69877. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65452/0.69883. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.65509/0.69986. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65232/0.69921. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65400/0.69926. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65270/0.69859. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65022/0.69853. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65103/0.69883. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64921/0.69905. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64992/0.69813. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65095/0.70119. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64819/0.70156. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65080/0.70077. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64569/0.69926. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64653/0.70064. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64257/0.69941. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64369/0.70171. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64155/0.70208. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64146/0.70604. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64362/0.70073. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64381/0.70033. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64109/0.70316. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64196/0.70226. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64084/0.70263. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63905/0.70471. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63892/0.70482. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63673/0.70132. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63982/0.70621. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63800/0.70861. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63725/0.70577. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63453/0.70490. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63444/0.70468. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63615/0.70406. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63110/0.70801. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63220/0.70887. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63468/0.70824. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62989/0.70791. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69520/0.69158. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69106/0.68888. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68956/0.68797. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68901/0.68785. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68842/0.68790. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68938/0.68801. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68784/0.68821. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68805/0.68861. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68781/0.68889. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68686/0.68919. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68616/0.68960. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68643/0.68994. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68636/0.69028. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68652/0.69081. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68565/0.69146. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68542/0.69214. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68388/0.69280. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68310/0.69361. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68379/0.69445. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68234/0.69556. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68218/0.69658. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68177/0.69752. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68068/0.69835. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67963/0.69923. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68017/0.70026. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68070/0.70126. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67927/0.70235. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67578/0.70382. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67616/0.70494. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67799/0.70584. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67588/0.70705. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67596/0.70815. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67529/0.70750. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67464/0.70902. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67342/0.71005. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67046/0.71203. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67028/0.71286. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67204/0.71346. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66939/0.71483. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67091/0.71528. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67056/0.71687. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66750/0.71712. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66765/0.71826. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66622/0.71952. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66334/0.72238. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66265/0.72357. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66432/0.72343. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66280/0.72330. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66291/0.72686. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66281/0.72691. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66140/0.72822. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.65942/0.72890. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65810/0.72890. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65591/0.73072. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65756/0.73162. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.65476/0.73138. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65739/0.73306. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64958/0.73549. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65228/0.73693. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65019/0.73707. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65115/0.73901. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65241/0.74140. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64956/0.74072. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65179/0.74059. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64639/0.74241. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64385/0.74071. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64720/0.74274. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.64754/0.74404. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64498/0.74397. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64443/0.74668. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64043/0.74486. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64142/0.74871. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63997/0.75044. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64141/0.74985. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64083/0.75117. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63672/0.75058. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63801/0.75299. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63307/0.75834. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63395/0.75822. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62915/0.75543. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63170/0.75874. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63408/0.76106. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62813/0.76598. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63327/0.76502. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62674/0.76873. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62471/0.76568. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62541/0.76613. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62616/0.76721. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62557/0.77285. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.62423/0.77307. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61754/0.77504. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61990/0.77539. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61816/0.77568. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62160/0.77870. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62357/0.77124. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61788/0.77700. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61663/0.77973. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60954/0.78132. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61292/0.77942. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.61049/0.78395. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69379/0.69068. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69081/0.68743. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68979/0.68619. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68926/0.68575. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68839/0.68562. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68898/0.68565. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68878/0.68577. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68826/0.68587. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68813/0.68585. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68739/0.68609. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68776/0.68629. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68786/0.68650. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68794/0.68675. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68700/0.68721. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68690/0.68755. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68677/0.68773. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68695/0.68794. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68649/0.68829. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68536/0.68868. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68511/0.68914. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68563/0.68977. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68555/0.69022. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68531/0.69043. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68414/0.69082. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68441/0.69117. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68306/0.69179. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68435/0.69192. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68340/0.69211. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68220/0.69251. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68338/0.69296. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68216/0.69340. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68179/0.69397. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68146/0.69468. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68178/0.69499. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68142/0.69505. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68195/0.69528. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67995/0.69591. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68039/0.69655. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68017/0.69725. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67893/0.69795. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67806/0.69853. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67899/0.69916. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67751/0.69967. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67646/0.70058. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67673/0.70137. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67623/0.70199. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67783/0.70264. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67578/0.70289. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67548/0.70323. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67622/0.70344. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67283/0.70407. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67458/0.70498. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67465/0.70511. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67301/0.70587. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67050/0.70711. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67195/0.70754. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67122/0.70857. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67105/0.70924. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66967/0.70924. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66913/0.71043. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66961/0.71089. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66780/0.71172. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66570/0.71275. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66884/0.71373. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66762/0.71435. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66405/0.71616. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66342/0.71718. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66540/0.71794. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66404/0.71801. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66200/0.71902. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66123/0.72003. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66054/0.72033. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65901/0.72145. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65845/0.72246. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65784/0.72271. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65753/0.72380. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65697/0.72432. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65622/0.72562. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65700/0.72692. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65201/0.72753. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65202/0.72755. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65254/0.72737. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65100/0.72947. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65026/0.72998. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64930/0.73111. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64834/0.73172. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64342/0.73293. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64351/0.73407. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64368/0.73462. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64547/0.73453. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64633/0.73558. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64118/0.73635. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63931/0.73690. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63682/0.73571. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63686/0.73805. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63657/0.73959. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63405/0.74240. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63163/0.74280. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62985/0.74383. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62813/0.74072. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69426/0.69412. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68935/0.69344. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68888/0.69399. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68854/0.69452. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68892/0.69495. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68764/0.69537. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68722/0.69580. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68798/0.69612. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68767/0.69634. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68707/0.69670. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68784/0.69682. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68767/0.69720. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68761/0.69740. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68711/0.69783. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68573/0.69832. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68617/0.69870. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68590/0.69925. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68420/0.69998. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68528/0.70080. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68525/0.70125. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68478/0.70182. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68540/0.70239. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68508/0.70264. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68421/0.70340. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68386/0.70408. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68407/0.70484. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68351/0.70529. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68325/0.70589. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68334/0.70666. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68317/0.70663. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68306/0.70652. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68324/0.70694. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68220/0.70763. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68223/0.70751. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68085/0.70850. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68085/0.70881. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68098/0.70894. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68002/0.70987. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67970/0.71026. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67950/0.71052. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68077/0.71051. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68072/0.71034. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68055/0.71018. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67889/0.71050. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67801/0.71053. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67751/0.71093. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67651/0.71128. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67747/0.71123. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67692/0.71173. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67707/0.71152. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67290/0.71274. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67423/0.71294. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67324/0.71312. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67298/0.71386. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67445/0.71372. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67283/0.71407. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67184/0.71407. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67173/0.71410. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66829/0.71462. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66952/0.71536. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67002/0.71508. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66929/0.71496. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66798/0.71547. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66842/0.71555. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66576/0.71731. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66641/0.71675. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66603/0.71737. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66495/0.71851. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66263/0.71893. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66235/0.71879. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66294/0.71944. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66217/0.72071. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66094/0.72264. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65953/0.72179. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66040/0.72056. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65905/0.72190. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65631/0.72355. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65826/0.72470. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65626/0.72563. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65654/0.72589. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65301/0.72835. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65445/0.72908. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65201/0.73051. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64999/0.72878. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65070/0.73062. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64912/0.72885. Took 0.22 sec\n",
      "Epoch 86, Loss(train/val) 0.64942/0.73071. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64884/0.73183. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64798/0.73344. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64454/0.73335. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64612/0.73590. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64131/0.73578. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64258/0.73696. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64341/0.73747. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64227/0.74009. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63838/0.74075. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64071/0.74043. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63841/0.74315. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63954/0.74505. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63510/0.74908. Took 0.19 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69239/0.68895. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68841/0.68627. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68658/0.68483. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68543/0.68361. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68520/0.68233. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68433/0.68117. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68465/0.68025. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68505/0.67944. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68442/0.67860. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68396/0.67792. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68348/0.67730. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68188/0.67644. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68283/0.67563. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68222/0.67481. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68208/0.67421. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68116/0.67351. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68126/0.67242. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68076/0.67183. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.67992/0.67135. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67988/0.67070. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68041/0.66990. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67896/0.66932. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67800/0.66872. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67754/0.66761. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.67621/0.66690. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67683/0.66680. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67648/0.66626. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67637/0.66609. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.67367/0.66537. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67554/0.66521. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.67322/0.66544. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67321/0.66532. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67140/0.66541. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67306/0.66511. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67289/0.66510. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67174/0.66521. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67026/0.66531. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.66935/0.66529. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66975/0.66531. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66804/0.66464. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.66739/0.66409. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66702/0.66459. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66865/0.66473. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66827/0.66512. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66649/0.66516. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66595/0.66611. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66447/0.66608. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66361/0.66645. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66406/0.66609. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66077/0.66697. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66372/0.66614. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66077/0.66646. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65939/0.66727. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.65718/0.66762. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65770/0.66781. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66027/0.66784. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65957/0.66755. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65629/0.66822. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65508/0.66899. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65306/0.66955. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65557/0.67011. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65285/0.67022. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65418/0.66990. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.64991/0.66996. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65236/0.67002. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.64871/0.66945. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64895/0.67011. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.64809/0.66995. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.64973/0.66980. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64845/0.66808. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64507/0.66932. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64320/0.67056. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64225/0.67197. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64438/0.67088. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64157/0.67082. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63935/0.67035. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64100/0.66963. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64312/0.66902. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63856/0.66925. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63608/0.66972. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63636/0.67039. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63615/0.67229. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63317/0.67254. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63488/0.67176. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63357/0.67092. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63408/0.67259. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63006/0.67334. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63218/0.67270. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63193/0.67403. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.62646/0.67347. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62943/0.67293. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62787/0.67161. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62801/0.67220. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62430/0.67208. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62339/0.67349. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62500/0.67344. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62242/0.67636. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62300/0.67638. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62094/0.67626. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62133/0.67619. Took 0.19 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.68953/0.67889. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68655/0.67769. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68681/0.67733. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68675/0.67689. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68623/0.67666. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68583/0.67625. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68514/0.67592. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68479/0.67552. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68538/0.67561. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68443/0.67559. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68410/0.67529. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68477/0.67533. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68436/0.67499. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68337/0.67481. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68343/0.67498. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68430/0.67508. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68261/0.67489. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68280/0.67512. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68174/0.67478. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68243/0.67473. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68187/0.67463. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68134/0.67457. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68184/0.67473. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68143/0.67465. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68048/0.67496. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68060/0.67535. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67841/0.67536. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67966/0.67588. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67800/0.67572. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67789/0.67599. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67635/0.67671. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67576/0.67692. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67823/0.67775. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67657/0.67766. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67492/0.67794. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67446/0.67863. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67559/0.67861. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67294/0.67824. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67399/0.67877. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67244/0.67921. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67177/0.67819. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67117/0.67943. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67040/0.67979. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66990/0.67941. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66929/0.67997. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66813/0.68073. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66766/0.68138. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66915/0.68042. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66972/0.68076. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66547/0.68071. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66664/0.68068. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66572/0.68065. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66494/0.68064. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66522/0.67994. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66258/0.68039. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66359/0.68130. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66168/0.68195. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66093/0.68099. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66145/0.68111. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65998/0.68078. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65729/0.68084. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65635/0.68140. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65471/0.68091. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65493/0.68076. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65846/0.68194. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65596/0.67882. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65352/0.67950. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65317/0.67892. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65453/0.67967. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65066/0.68074. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65021/0.67934. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65033/0.67884. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64888/0.67901. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64824/0.68050. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64672/0.67837. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64538/0.67983. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64667/0.67962. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64802/0.67996. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64575/0.67983. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64904/0.67907. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64171/0.67714. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64405/0.67763. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64146/0.67938. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64012/0.67340. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64006/0.67825. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64027/0.67536. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63340/0.67628. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63680/0.67634. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63642/0.67664. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63476/0.67679. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62762/0.67810. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63304/0.67867. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62877/0.67782. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63304/0.67669. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62604/0.67635. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62416/0.67701. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62852/0.67843. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62665/0.67739. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62631/0.67915. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62545/0.67947. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.68849/0.69608. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68553/0.69839. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68424/0.70050. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68418/0.70244. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68339/0.70355. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68358/0.70420. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68350/0.70447. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68289/0.70482. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68313/0.70461. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68269/0.70487. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68280/0.70481. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68268/0.70464. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68252/0.70455. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68150/0.70402. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68140/0.70385. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68178/0.70399. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68125/0.70390. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68075/0.70369. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68117/0.70401. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68033/0.70359. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68057/0.70383. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68008/0.70363. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67940/0.70443. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67898/0.70510. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67880/0.70549. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67830/0.70423. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67768/0.70401. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67786/0.70452. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67548/0.70569. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67695/0.70502. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67690/0.70553. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67639/0.70526. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67413/0.70562. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67397/0.70609. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67441/0.70633. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67354/0.70619. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67259/0.70556. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67200/0.70748. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67250/0.70519. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67055/0.70689. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67173/0.70625. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67021/0.70623. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66977/0.70893. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66885/0.70806. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67000/0.70995. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66784/0.71030. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66563/0.71060. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66750/0.71087. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66619/0.71142. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66682/0.71151. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66382/0.71365. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66407/0.71213. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66465/0.71348. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66203/0.71297. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66116/0.71151. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65965/0.71154. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66209/0.71490. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66034/0.71427. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66063/0.71738. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65813/0.71366. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65712/0.71487. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65691/0.71617. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65635/0.71609. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65813/0.71723. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65420/0.71582. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65474/0.71959. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65276/0.71814. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65171/0.71986. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64900/0.72267. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65374/0.72756. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65021/0.72504. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64828/0.72707. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65213/0.72519. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64667/0.72718. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64709/0.72577. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64691/0.72646. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64501/0.72862. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64560/0.72720. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64446/0.73343. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64328/0.73203. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63901/0.73078. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64107/0.73384. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63931/0.73284. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63970/0.73267. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63916/0.73704. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63625/0.73729. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63535/0.73885. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63697/0.74037. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63522/0.74129. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63469/0.74194. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63151/0.74268. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62723/0.73726. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63324/0.74373. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62829/0.74336. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62911/0.74623. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62684/0.74355. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62668/0.74620. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62907/0.74315. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62658/0.74568. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62230/0.75168. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69489/0.69441. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69328/0.69424. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69232/0.69376. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69228/0.69320. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69246/0.69293. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69129/0.69285. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69134/0.69255. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69124/0.69244. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69104/0.69239. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69094/0.69233. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69036/0.69227. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69063/0.69236. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69042/0.69226. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.69002/0.69232. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69036/0.69244. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68969/0.69237. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68954/0.69248. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.69004/0.69256. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68917/0.69262. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68912/0.69275. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68946/0.69283. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68910/0.69264. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68902/0.69278. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68918/0.69277. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68833/0.69281. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68952/0.69279. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68825/0.69288. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68761/0.69279. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68781/0.69298. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68780/0.69301. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68841/0.69298. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68872/0.69270. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68804/0.69309. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68799/0.69274. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68725/0.69275. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68671/0.69305. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68717/0.69315. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68741/0.69323. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68719/0.69359. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68669/0.69350. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68640/0.69325. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68628/0.69334. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68590/0.69325. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68613/0.69375. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68551/0.69351. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68517/0.69378. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68529/0.69364. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68503/0.69438. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68463/0.69422. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68351/0.69407. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68320/0.69403. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68328/0.69496. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68323/0.69479. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.68247/0.69538. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68286/0.69530. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.68201/0.69614. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.68187/0.69610. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.68167/0.69628. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.68102/0.69637. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.68105/0.69694. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.68262/0.69775. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.68195/0.69800. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.68273/0.69756. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.68036/0.69885. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.68007/0.69909. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.68098/0.69832. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67958/0.69982. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67874/0.70014. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67925/0.69956. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67824/0.70194. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67817/0.70231. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67662/0.70329. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.67806/0.70314. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67884/0.70379. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67574/0.70452. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67593/0.70517. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.67698/0.70450. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.67581/0.70554. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.67596/0.70528. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.67444/0.70717. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.67275/0.70845. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.67519/0.70864. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.67268/0.70881. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.67196/0.70909. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.67343/0.70870. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.67399/0.70952. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.67080/0.70942. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66984/0.71236. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.67116/0.70988. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66903/0.71222. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66867/0.71366. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66810/0.71442. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66876/0.71592. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66919/0.71613. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.66937/0.71614. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66362/0.71930. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66717/0.71952. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66620/0.71815. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.66481/0.72096. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.66465/0.72086. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69430/0.68881. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69316/0.68967. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69251/0.68985. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69184/0.68970. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69190/0.68969. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69089/0.69001. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69113/0.69006. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69004/0.68972. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69014/0.68960. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68996/0.68971. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68981/0.68942. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68922/0.68977. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69008/0.68970. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68895/0.68952. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68955/0.68924. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68905/0.68972. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68855/0.68949. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68944/0.68907. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68806/0.68923. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68811/0.68929. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68802/0.68918. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68874/0.68910. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68806/0.68904. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68748/0.68901. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68807/0.68875. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68797/0.68851. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68763/0.68876. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68729/0.68860. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68698/0.68830. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68642/0.68825. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68653/0.68789. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68727/0.68781. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68632/0.68823. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68554/0.68790. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68538/0.68798. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68417/0.68735. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68572/0.68742. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68412/0.68752. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68340/0.68644. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68370/0.68630. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68411/0.68617. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68395/0.68600. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68306/0.68590. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.68256/0.68572. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 0.68118/0.68601. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68066/0.68599. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68089/0.68564. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68111/0.68542. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68196/0.68565. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68033/0.68466. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67960/0.68473. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67800/0.68413. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67787/0.68386. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67782/0.68379. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67770/0.68421. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67603/0.68343. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67625/0.68296. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67454/0.68244. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67610/0.68188. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67428/0.68264. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67307/0.68243. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67269/0.68256. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67207/0.68281. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67165/0.68318. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67041/0.68251. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67276/0.68152. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66895/0.68162. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66838/0.68186. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66869/0.68317. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66709/0.68303. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66594/0.68218. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66531/0.68109. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66672/0.67972. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66551/0.67952. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66502/0.67846. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66150/0.67853. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66365/0.68008. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66111/0.68110. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66345/0.68029. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66222/0.68009. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66079/0.67898. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66026/0.67783. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 0.65474/0.67694. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65843/0.67836. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65490/0.67836. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65203/0.67694. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65283/0.67694. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65274/0.67769. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64863/0.67778. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65036/0.67642. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65091/0.67691. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64793/0.67777. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64802/0.67864. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64241/0.67733. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64534/0.67719. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64703/0.67764. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64402/0.67804. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64180/0.67942. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64049/0.67974. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64141/0.67987. Took 0.18 sec\n",
      "ACC: 0.3854166666666667\n",
      "Epoch 0, Loss(train/val) 0.69517/0.69889. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69263/0.69913. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69276/0.69945. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69209/0.69957. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69214/0.69914. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69149/0.69903. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69222/0.69856. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69160/0.69826. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69058/0.69815. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69183/0.69846. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69185/0.69784. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69074/0.69756. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69081/0.69741. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69132/0.69725. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69113/0.69676. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69032/0.69688. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.69086/0.69697. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69013/0.69652. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68994/0.69622. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68990/0.69593. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.69029/0.69577. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69002/0.69588. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.69025/0.69565. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68944/0.69514. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68962/0.69531. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.69004/0.69538. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68901/0.69508. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68934/0.69536. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68907/0.69520. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68911/0.69487. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68825/0.69450. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68915/0.69452. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68797/0.69457. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68841/0.69390. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68852/0.69469. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68778/0.69384. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68772/0.69413. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68589/0.69374. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68636/0.69376. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68659/0.69355. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68592/0.69345. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68653/0.69332. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68500/0.69358. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68465/0.69294. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68502/0.69293. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68392/0.69259. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.68358/0.69286. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68389/0.69312. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68373/0.69293. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68355/0.69267. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68242/0.69259. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68222/0.69250. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.68056/0.69219. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.68034/0.69202. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.68152/0.69214. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67997/0.69286. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67861/0.69220. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67735/0.69268. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67759/0.69280. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67507/0.69256. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67667/0.69241. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67521/0.69322. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67560/0.69215. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67256/0.69178. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.67565/0.69150. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67352/0.69223. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67192/0.69152. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67273/0.69144. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.67274/0.69154. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66947/0.69241. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66950/0.69209. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66879/0.69151. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66732/0.69126. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66704/0.69346. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66603/0.69296. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66430/0.69257. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66545/0.69214. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66176/0.69271. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66124/0.69274. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66233/0.69203. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66117/0.69372. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66005/0.69161. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66011/0.69135. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65733/0.69293. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65651/0.69415. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65789/0.69413. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65156/0.69457. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65623/0.69485. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65052/0.69637. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65216/0.69427. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64988/0.69785. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65074/0.69666. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64956/0.69886. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64686/0.69877. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65001/0.69723. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64576/0.69641. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64647/0.69433. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64049/0.69675. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64136/0.69829. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63934/0.69572. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69351/0.68408. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69303/0.68621. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69318/0.68747. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69336/0.68874. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69277/0.68955. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69224/0.69004. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69169/0.69053. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69181/0.69113. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69200/0.69131. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69181/0.69193. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69197/0.69197. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69215/0.69272. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69141/0.69326. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69096/0.69342. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69235/0.69359. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69111/0.69383. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69123/0.69451. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.69107/0.69456. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69038/0.69485. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.69043/0.69512. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.69058/0.69561. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69058/0.69618. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.69073/0.69671. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68998/0.69705. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68979/0.69756. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.69092/0.69741. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68911/0.69819. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68934/0.69878. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68933/0.69872. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68946/0.69855. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68915/0.69926. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68919/0.70032. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68903/0.70034. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68946/0.70054. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68908/0.70042. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68822/0.70087. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68834/0.70060. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68831/0.70121. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68789/0.70146. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.68890/0.70143. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68817/0.70113. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68746/0.70121. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68775/0.70171. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68779/0.70108. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68791/0.69983. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68701/0.70069. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68604/0.70189. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68620/0.70224. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68671/0.70155. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.68451/0.70233. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68525/0.70257. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68466/0.70238. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68452/0.70198. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68374/0.70275. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.68457/0.70246. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.68382/0.70307. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.68355/0.70361. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.68210/0.70243. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.68301/0.70350. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.68367/0.70387. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.68216/0.70298. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.68267/0.70526. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.68014/0.70616. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.68060/0.70602. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67887/0.70577. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67937/0.70592. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67929/0.70575. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67875/0.70597. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67658/0.70668. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67776/0.70866. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67531/0.71031. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67579/0.71058. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67448/0.70986. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67503/0.71120. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67367/0.71088. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67261/0.71164. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67211/0.71040. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.67312/0.71358. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67235/0.71331. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66915/0.71574. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66870/0.71532. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66629/0.71575. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66591/0.71805. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66719/0.71967. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66400/0.72027. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66519/0.72113. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66076/0.72077. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66488/0.72184. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66025/0.72058. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66051/0.72497. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66146/0.72482. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65840/0.72618. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66015/0.72657. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65799/0.72726. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65873/0.72888. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65559/0.72656. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65720/0.72999. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65274/0.73021. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65200/0.73166. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65589/0.73159. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69319/0.69524. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69177/0.69579. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69063/0.69607. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69081/0.69615. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68993/0.69586. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68983/0.69626. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68883/0.69615. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68863/0.69638. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68887/0.69637. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68920/0.69654. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68813/0.69673. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68800/0.69668. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68837/0.69694. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68813/0.69737. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68841/0.69755. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68790/0.69775. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68796/0.69809. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68725/0.69826. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68715/0.69834. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68756/0.69863. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68698/0.69893. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68739/0.69911. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68727/0.69891. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68718/0.69930. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68751/0.70007. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68697/0.69999. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68711/0.70040. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68660/0.70046. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68627/0.70067. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68630/0.70053. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68592/0.70053. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68692/0.70053. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68587/0.70064. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68553/0.70117. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68603/0.70098. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68594/0.70122. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68568/0.70126. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68538/0.70134. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68555/0.70143. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68554/0.70197. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68472/0.70240. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68503/0.70229. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68480/0.70236. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68414/0.70251. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68448/0.70365. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68393/0.70367. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68368/0.70349. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68328/0.70354. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68388/0.70335. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68287/0.70392. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68267/0.70470. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68202/0.70508. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68155/0.70530. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68323/0.70598. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68272/0.70697. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.68292/0.70681. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68034/0.70724. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.68055/0.70760. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68093/0.70827. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.68094/0.70798. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.68046/0.70877. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67988/0.70985. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.68035/0.70956. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67914/0.70994. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.67859/0.71105. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67844/0.71117. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67781/0.71161. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67806/0.71274. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67587/0.71370. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67513/0.71424. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67645/0.71444. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67457/0.71494. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67544/0.71618. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67459/0.71710. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67369/0.71708. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67338/0.71873. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67398/0.71718. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67324/0.72004. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67277/0.72038. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66913/0.72020. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67094/0.72042. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.67117/0.72255. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66795/0.72217. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66770/0.72461. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66619/0.72594. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66697/0.72571. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66638/0.72789. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66414/0.72761. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66198/0.73046. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66443/0.72949. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66106/0.73028. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66200/0.73268. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66046/0.73573. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65543/0.73653. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65979/0.73828. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65907/0.73708. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65491/0.73784. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65259/0.73998. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65403/0.73991. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65144/0.74014. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69283/0.68724. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69230/0.68658. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69319/0.68651. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69242/0.68671. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69165/0.68690. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69150/0.68715. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69117/0.68734. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69138/0.68751. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69167/0.68764. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69113/0.68775. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69052/0.68799. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68931/0.68806. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69054/0.68804. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69022/0.68809. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69012/0.68820. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68964/0.68843. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69070/0.68864. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68887/0.68901. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68997/0.68937. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68946/0.68948. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68793/0.68959. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68792/0.68996. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68880/0.69001. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68779/0.69016. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68903/0.69033. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68817/0.69039. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68809/0.69060. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68932/0.69056. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68852/0.69053. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68653/0.69029. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68855/0.69016. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68684/0.69066. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68685/0.69043. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68666/0.69008. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68529/0.69058. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68656/0.69077. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68589/0.69070. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68587/0.69142. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68546/0.69128. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68611/0.69117. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68474/0.69070. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68381/0.69098. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68350/0.69090. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68401/0.69086. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68308/0.69083. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68245/0.69048. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68402/0.69041. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68315/0.69149. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68234/0.69116. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68209/0.69111. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68145/0.69067. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68092/0.69115. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68003/0.69062. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67911/0.69108. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67961/0.69088. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67951/0.69008. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68009/0.69069. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.68028/0.69091. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67910/0.69118. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67731/0.69093. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67712/0.69125. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67519/0.69035. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67641/0.69090. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67484/0.69044. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67291/0.69116. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67431/0.69169. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67457/0.69116. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67262/0.69162. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67331/0.69123. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66997/0.68994. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67138/0.69085. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67179/0.69013. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66981/0.69030. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66931/0.69153. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67046/0.69079. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66706/0.69041. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66817/0.69060. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66593/0.69126. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66477/0.68897. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66397/0.68815. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66279/0.69162. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66316/0.69113. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66116/0.68966. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66256/0.68904. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66137/0.69015. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65800/0.68973. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66184/0.68996. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65754/0.69006. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65952/0.68945. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65518/0.69070. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65557/0.69188. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65513/0.68839. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65292/0.69099. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65230/0.68882. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65156/0.69126. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65115/0.69061. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64962/0.68850. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64518/0.69047. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65126/0.69003. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64969/0.69027. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69312/0.69487. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69037/0.69598. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69091/0.69580. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69045/0.69600. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68967/0.69593. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69032/0.69577. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68960/0.69547. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68902/0.69565. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68856/0.69564. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68821/0.69592. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68874/0.69602. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68782/0.69644. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68780/0.69618. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68818/0.69634. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68827/0.69665. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68632/0.69666. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68673/0.69631. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68717/0.69678. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68661/0.69674. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68601/0.69722. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68537/0.69722. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68608/0.69733. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68520/0.69786. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68535/0.69779. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68444/0.69779. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68577/0.69777. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68409/0.69768. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68337/0.69825. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68401/0.69863. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68388/0.69835. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68408/0.69841. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68246/0.69875. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68298/0.69881. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68243/0.69873. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68203/0.69877. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68160/0.69931. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68129/0.69955. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68111/0.69968. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68100/0.69973. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68200/0.70023. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67970/0.70063. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67931/0.70118. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67928/0.70089. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67947/0.70152. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67980/0.70192. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67749/0.70228. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67770/0.70317. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67704/0.70346. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67788/0.70424. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67568/0.70516. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67475/0.70458. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67486/0.70626. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67463/0.70679. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67289/0.70732. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67344/0.70814. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67418/0.70741. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67282/0.70917. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67245/0.71065. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67328/0.70924. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67193/0.71014. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67090/0.71086. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66919/0.70996. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66994/0.71091. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66951/0.71145. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66743/0.71301. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66476/0.71381. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66567/0.71511. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66368/0.71649. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66575/0.71934. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66546/0.71806. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66487/0.71906. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66545/0.71900. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66086/0.72255. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66184/0.72015. Took 0.21 sec\n",
      "Epoch 74, Loss(train/val) 0.66003/0.72206. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65947/0.72262. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65976/0.72263. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65917/0.72490. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65859/0.72723. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.65894/0.72412. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65629/0.72672. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65565/0.72797. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65423/0.72665. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65568/0.72980. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65318/0.72711. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65192/0.73191. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65039/0.73402. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64897/0.73389. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64913/0.73656. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64878/0.73484. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64663/0.73296. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64761/0.74450. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64544/0.74498. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64642/0.74643. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64733/0.74129. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64495/0.73857. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63984/0.74052. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63706/0.74601. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63723/0.74619. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63706/0.74823. Took 0.18 sec\n",
      "ACC: 0.375\n",
      "Epoch 0, Loss(train/val) 0.69206/0.70148. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69164/0.70044. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69087/0.70011. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69042/0.69941. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69011/0.69935. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69040/0.69945. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68870/0.69966. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68776/0.69898. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68839/0.69905. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68861/0.69902. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68736/0.69973. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68767/0.69922. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68735/0.69872. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68605/0.69895. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68621/0.69939. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68651/0.70005. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68687/0.69948. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68649/0.69950. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68653/0.69907. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68593/0.69873. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68583/0.69896. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68607/0.69881. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68507/0.69850. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68614/0.69817. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68586/0.69838. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68521/0.69807. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68451/0.69805. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68550/0.69841. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68485/0.69818. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68500/0.69865. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68579/0.69807. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68449/0.69827. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68527/0.69807. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68507/0.69799. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68415/0.69794. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68370/0.69805. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68357/0.69865. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68351/0.69833. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68353/0.69722. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68411/0.69664. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68258/0.69741. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68297/0.69743. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68295/0.69701. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68391/0.69682. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68415/0.69704. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68263/0.69708. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68200/0.69702. Took 0.22 sec\n",
      "Epoch 47, Loss(train/val) 0.68265/0.69676. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.68237/0.69709. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68235/0.69597. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.68126/0.69677. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68044/0.69603. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68093/0.69508. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68223/0.69575. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.68134/0.69576. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68019/0.69559. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67997/0.69408. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67937/0.69447. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67826/0.69386. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.68157/0.69369. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67979/0.69409. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67931/0.69414. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.67715/0.69252. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67763/0.69235. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67885/0.69405. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67995/0.69293. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67746/0.69382. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67590/0.69394. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67662/0.69225. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67734/0.69134. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67764/0.69173. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67605/0.69331. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67408/0.69075. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67569/0.69095. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67315/0.69095. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67425/0.69112. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.67315/0.68987. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67336/0.68836. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.67128/0.69147. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67140/0.69047. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66951/0.69106. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.67104/0.68765. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66955/0.68709. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66856/0.68690. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66789/0.69003. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66775/0.68761. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66591/0.68541. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66557/0.68856. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66576/0.68537. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66328/0.68794. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66095/0.68585. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66343/0.68626. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66272/0.68649. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66246/0.68514. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.66036/0.68800. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65818/0.68501. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65671/0.68492. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65711/0.68402. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65656/0.68283. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65484/0.68930. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69283/0.69042. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69160/0.69087. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69108/0.69085. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.68971/0.69084. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68824/0.69102. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68826/0.69127. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68760/0.69167. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68712/0.69204. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68673/0.69248. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68610/0.69285. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68674/0.69314. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68554/0.69352. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68515/0.69403. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68502/0.69419. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68482/0.69477. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68371/0.69506. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68362/0.69546. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68355/0.69569. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68322/0.69624. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68236/0.69651. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68274/0.69676. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68219/0.69698. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68168/0.69723. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68157/0.69732. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68276/0.69750. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68193/0.69747. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68220/0.69748. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68162/0.69760. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68093/0.69754. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67962/0.69742. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67991/0.69776. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68012/0.69804. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67924/0.69793. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67953/0.69790. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67995/0.69757. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67781/0.69764. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67761/0.69756. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67735/0.69750. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67662/0.69718. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67631/0.69736. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67520/0.69719. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67625/0.69758. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67556/0.69758. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67422/0.69776. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67456/0.69724. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67103/0.69721. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67372/0.69714. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67189/0.69749. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67158/0.69763. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67188/0.69815. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66977/0.69780. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66897/0.69774. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67018/0.69786. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66739/0.69766. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66338/0.69827. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66706/0.69805. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66676/0.69874. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66448/0.69810. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66338/0.69867. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66302/0.69776. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66280/0.69774. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66236/0.69684. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66041/0.69649. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66003/0.69724. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65829/0.69731. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65649/0.69714. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65402/0.69755. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65483/0.69844. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65552/0.69830. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65073/0.69840. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65473/0.70064. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65292/0.69936. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64916/0.70076. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65037/0.70024. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65044/0.69942. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64913/0.69976. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64338/0.70118. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64604/0.70052. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64406/0.70236. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64505/0.70315. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64074/0.70251. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64051/0.70176. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64038/0.70098. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64330/0.70133. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63798/0.70307. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63716/0.70377. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63799/0.70422. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63702/0.70513. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63484/0.70544. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63044/0.70571. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63189/0.70443. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62928/0.70853. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63052/0.70958. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63188/0.71129. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63082/0.71295. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62834/0.71461. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62441/0.71544. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62815/0.71531. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62620/0.71652. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62564/0.71195. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69977/0.69896. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69154/0.70044. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69022/0.70277. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68784/0.70546. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68892/0.70788. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68803/0.71022. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68746/0.71219. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68746/0.71412. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68736/0.71574. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68666/0.71707. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68731/0.71807. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68632/0.71905. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68619/0.72024. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68565/0.72108. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68526/0.72180. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68598/0.72219. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68480/0.72286. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68525/0.72297. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68550/0.72333. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68500/0.72364. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68445/0.72403. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68497/0.72430. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68558/0.72421. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68333/0.72454. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68415/0.72534. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68511/0.72532. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68509/0.72558. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68392/0.72583. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68424/0.72578. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68423/0.72613. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68467/0.72591. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68384/0.72609. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68343/0.72649. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68342/0.72685. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68425/0.72671. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68444/0.72665. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68325/0.72695. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68427/0.72715. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68287/0.72755. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68359/0.72755. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68348/0.72763. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68349/0.72799. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68296/0.72796. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68299/0.72827. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68330/0.72846. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68137/0.72861. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68245/0.72878. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68195/0.72903. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68238/0.72917. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68225/0.72950. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68094/0.72950. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68189/0.72987. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68082/0.72990. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68162/0.72997. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68210/0.72988. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.68148/0.72967. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68085/0.73026. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.68189/0.73000. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68217/0.73001. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.68033/0.73033. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67995/0.73048. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67935/0.73092. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.68001/0.73062. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67962/0.73057. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67869/0.73122. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67909/0.73152. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67889/0.73194. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.68042/0.73192. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67866/0.73179. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.68048/0.73177. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67660/0.73280. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67799/0.73325. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.67836/0.73340. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67835/0.73291. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67758/0.73378. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67757/0.73444. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.67738/0.73444. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67625/0.73393. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.67608/0.73430. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67592/0.73436. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.67570/0.73444. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.67611/0.73406. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.67622/0.73473. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67587/0.73486. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.67433/0.73497. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.67459/0.73447. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.67235/0.73483. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.67321/0.73538. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.67537/0.73487. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.67124/0.73486. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.67189/0.73509. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.67187/0.73451. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.67136/0.73424. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66842/0.73572. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.67071/0.73557. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.67017/0.73519. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.67052/0.73494. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66856/0.73473. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66830/0.73554. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66637/0.73576. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69331/0.69383. Took 0.38 sec\n",
      "Epoch 1, Loss(train/val) 0.69084/0.69376. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69015/0.69369. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68988/0.69362. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68931/0.69360. Took 0.22 sec\n",
      "Epoch 5, Loss(train/val) 0.68819/0.69355. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68941/0.69337. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68878/0.69326. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68817/0.69331. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68762/0.69322. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68777/0.69326. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68714/0.69320. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68804/0.69314. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68745/0.69313. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68676/0.69309. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68608/0.69307. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68645/0.69305. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68656/0.69326. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68441/0.69341. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68606/0.69331. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68452/0.69319. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68487/0.69319. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68323/0.69330. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68355/0.69344. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68354/0.69368. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68289/0.69330. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68248/0.69327. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68240/0.69351. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68290/0.69338. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68284/0.69338. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68240/0.69358. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68260/0.69365. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68214/0.69362. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68059/0.69346. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68132/0.69361. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68068/0.69376. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67911/0.69385. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67932/0.69355. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67922/0.69362. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67807/0.69368. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67829/0.69365. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67596/0.69341. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67768/0.69356. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67860/0.69324. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67621/0.69338. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67576/0.69330. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67670/0.69349. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67619/0.69333. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67505/0.69340. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67168/0.69290. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67556/0.69236. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67544/0.69218. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67324/0.69290. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67084/0.69308. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67110/0.69336. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67198/0.69295. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66956/0.69346. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66867/0.69326. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66788/0.69366. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66813/0.69309. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66792/0.69321. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66612/0.69427. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66921/0.69400. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66548/0.69511. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66601/0.69462. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66277/0.69545. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66473/0.69509. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66403/0.69480. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66432/0.69550. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66215/0.69509. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65927/0.69535. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65857/0.69709. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65787/0.69732. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65977/0.69723. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66013/0.69959. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65771/0.69944. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65445/0.69998. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65560/0.69878. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65560/0.69967. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65409/0.69903. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65330/0.69869. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65266/0.70313. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65010/0.70251. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65127/0.70451. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65267/0.70331. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64624/0.70468. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64948/0.70599. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64835/0.70869. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64625/0.71098. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64253/0.71376. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64189/0.71364. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64537/0.71153. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64135/0.71590. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64229/0.71536. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64147/0.71855. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64406/0.71524. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64107/0.71885. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63907/0.72230. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64094/0.72231. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63745/0.72272. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69250/0.69368. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68949/0.69337. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68994/0.69261. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68957/0.69200. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68870/0.69159. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68863/0.69143. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68776/0.69098. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68831/0.69074. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68786/0.69029. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68701/0.68998. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68705/0.68994. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68682/0.68978. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68568/0.68952. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68502/0.68948. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68535/0.68938. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68493/0.68912. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68458/0.68908. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68455/0.68841. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68295/0.68850. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68324/0.68846. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68254/0.68867. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68172/0.68907. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68162/0.68898. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68201/0.68999. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68098/0.69034. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68062/0.69024. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67977/0.69030. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67897/0.69073. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67841/0.69106. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67951/0.69155. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67779/0.69197. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67633/0.69315. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67580/0.69383. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67524/0.69539. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67468/0.69584. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67496/0.69603. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67327/0.69725. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67380/0.69789. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67117/0.69829. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67257/0.69954. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67272/0.69960. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67100/0.70072. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66850/0.70171. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66971/0.70223. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66966/0.70342. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66899/0.70401. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66614/0.70462. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66808/0.70556. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66726/0.70505. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66467/0.70676. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66344/0.70802. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66625/0.70861. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66401/0.71208. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66218/0.71066. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66140/0.71287. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66022/0.71388. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65967/0.71288. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65763/0.71620. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65860/0.71571. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65725/0.71860. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65653/0.71842. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65394/0.71999. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65479/0.72135. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65447/0.72191. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65501/0.72216. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65198/0.72223. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65192/0.72533. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64889/0.72590. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64922/0.72743. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64779/0.72808. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64900/0.73103. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64864/0.72951. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64829/0.73078. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64621/0.73310. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64289/0.73759. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64278/0.73539. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64513/0.73592. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63979/0.74005. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64038/0.73979. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64345/0.74307. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63971/0.74212. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63851/0.74416. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63636/0.74606. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63535/0.74635. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63782/0.75194. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63369/0.75322. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63662/0.75412. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63387/0.75626. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63294/0.75649. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62774/0.76115. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62948/0.76346. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62813/0.76115. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63080/0.76367. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62236/0.76577. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62311/0.76556. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62617/0.77292. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62815/0.77034. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62107/0.77064. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62195/0.76951. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62473/0.77249. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69460/0.69268. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69221/0.69448. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69144/0.69631. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69030/0.69824. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68925/0.69993. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68894/0.70199. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68800/0.70408. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68784/0.70649. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68692/0.70855. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68626/0.71111. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68525/0.71354. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68449/0.71551. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68425/0.71763. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68476/0.71899. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68265/0.72081. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68267/0.72182. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68335/0.72219. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68298/0.72291. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68167/0.72371. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68187/0.72433. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68069/0.72504. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68099/0.72569. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67995/0.72667. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67977/0.72734. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68005/0.72791. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67885/0.72789. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67953/0.72758. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67844/0.72744. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67698/0.72826. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67789/0.72879. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67617/0.72870. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67741/0.72959. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67575/0.73127. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67728/0.73123. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67590/0.73208. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67642/0.73215. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67399/0.73177. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67406/0.73250. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67474/0.73195. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67240/0.73321. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67428/0.73314. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67339/0.73357. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67175/0.73498. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67240/0.73372. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67098/0.73419. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67184/0.73486. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67290/0.73655. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66886/0.73573. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67078/0.73654. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66839/0.73723. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66778/0.73958. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67000/0.73996. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66886/0.73922. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66605/0.74095. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66709/0.74144. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66746/0.74206. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66726/0.74119. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66529/0.74247. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66535/0.74438. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66498/0.74551. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66521/0.74416. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66404/0.74458. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66198/0.74623. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66307/0.74684. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66244/0.74812. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66092/0.74921. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66227/0.74929. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66025/0.75084. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66154/0.75152. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65695/0.75238. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66012/0.75403. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65802/0.75458. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65862/0.75526. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65886/0.75343. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65714/0.75546. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65548/0.75700. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65656/0.75826. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65380/0.75926. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65626/0.76097. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65364/0.76110. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65570/0.76154. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.65261/0.76175. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65437/0.76304. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65098/0.76549. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65346/0.76787. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65196/0.76837. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64868/0.76959. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65053/0.76906. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64867/0.76972. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64581/0.77068. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64895/0.77221. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64712/0.77268. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64364/0.77434. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64622/0.77612. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64658/0.77682. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64651/0.77834. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64440/0.77984. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.64222/0.78089. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64342/0.78377. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64167/0.78413. Took 0.18 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69514/0.69551. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69246/0.69124. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69116/0.68834. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69141/0.68715. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69047/0.68613. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68995/0.68554. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68969/0.68560. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68977/0.68566. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68817/0.68590. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68811/0.68588. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68810/0.68585. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68805/0.68518. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.68741/0.68576. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68604/0.68607. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68541/0.68573. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68543/0.68564. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68504/0.68574. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68423/0.68526. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68444/0.68564. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68351/0.68612. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68244/0.68599. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68219/0.68668. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68186/0.68595. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68083/0.68595. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68073/0.68657. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68099/0.68738. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68025/0.68720. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67924/0.68759. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67888/0.68732. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67845/0.68801. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67685/0.68861. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67609/0.68836. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67751/0.68896. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67728/0.68914. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67515/0.68746. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67658/0.68825. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67445/0.68914. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67549/0.68973. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67696/0.69015. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67277/0.69134. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67394/0.69049. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67382/0.69204. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67294/0.69131. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67400/0.69259. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67249/0.69343. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66939/0.69203. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67082/0.69462. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67000/0.69430. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67080/0.69432. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67091/0.69494. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67057/0.69483. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66999/0.69655. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66968/0.69648. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66945/0.69768. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66894/0.69915. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66457/0.69897. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66576/0.69923. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66543/0.70100. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66427/0.70023. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66621/0.70223. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66359/0.70265. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66228/0.70173. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66081/0.70286. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66479/0.70358. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66059/0.70635. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65735/0.70774. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66072/0.70809. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66114/0.70882. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66099/0.70968. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65728/0.71093. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65679/0.71109. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65723/0.71278. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65636/0.71326. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65646/0.71561. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65464/0.71637. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65837/0.71773. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65732/0.71569. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65566/0.71827. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65241/0.71859. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65358/0.72145. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65287/0.72123. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65139/0.72348. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65001/0.72688. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65344/0.72582. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65061/0.72402. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65058/0.72676. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65353/0.72569. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64760/0.72947. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64821/0.73135. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64651/0.72877. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64845/0.72901. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64249/0.73373. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64754/0.73595. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64827/0.73451. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64562/0.73811. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64546/0.74088. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64378/0.74142. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64371/0.74065. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64464/0.74303. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64516/0.73960. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69253/0.69066. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69080/0.69157. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68961/0.69239. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68955/0.69324. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68866/0.69419. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68885/0.69495. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68730/0.69558. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68811/0.69647. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68719/0.69728. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68672/0.69804. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68604/0.69869. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68573/0.69929. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68562/0.69986. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68593/0.70010. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68511/0.70097. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68527/0.70134. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68498/0.70143. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68539/0.70161. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68294/0.70179. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68409/0.70167. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68362/0.70146. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68301/0.70174. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68254/0.70147. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68318/0.70140. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68182/0.70162. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68121/0.70209. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68158/0.70170. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68087/0.70149. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68098/0.70109. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67959/0.70088. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67911/0.70109. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67904/0.70102. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67888/0.70073. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67900/0.70006. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67877/0.69978. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67892/0.69959. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67727/0.69909. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67600/0.69879. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67625/0.69890. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67668/0.69743. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67473/0.69869. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67513/0.69787. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67628/0.69704. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67396/0.69681. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67410/0.69689. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67332/0.69675. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67338/0.69652. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67212/0.69596. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67150/0.69529. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67036/0.69431. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67072/0.69468. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67065/0.69415. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66925/0.69372. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66743/0.69356. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66803/0.69247. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66630/0.69224. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66641/0.69241. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66454/0.69219. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66499/0.69180. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66423/0.69135. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66281/0.69119. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66150/0.69037. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66025/0.68966. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66129/0.68917. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66023/0.68888. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65735/0.68886. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65659/0.68778. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65413/0.68634. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65602/0.68630. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65323/0.68644. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65225/0.68791. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65163/0.68784. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65007/0.68653. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65131/0.68583. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64746/0.68575. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64635/0.68728. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64560/0.68705. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64764/0.68960. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64419/0.68931. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64056/0.68997. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64050/0.69017. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64251/0.68939. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63890/0.69213. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63771/0.69084. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63562/0.69297. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63582/0.69408. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63683/0.69516. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63126/0.69354. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63492/0.69409. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63041/0.69324. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63260/0.69412. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63072/0.69575. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62611/0.69598. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62845/0.69921. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62530/0.69874. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62450/0.69544. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62119/0.69579. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62282/0.69922. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62204/0.69798. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61854/0.70133. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69340/0.69491. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69049/0.69853. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68925/0.70107. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68820/0.70332. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68692/0.70504. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68685/0.70629. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68624/0.70752. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68609/0.70795. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68509/0.70852. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68468/0.70923. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68467/0.70949. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68504/0.70964. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68445/0.70966. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68390/0.70975. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68411/0.70976. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68420/0.70944. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68326/0.70948. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68442/0.70905. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68247/0.70910. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68313/0.70905. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68159/0.70892. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68117/0.70896. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68120/0.70874. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68015/0.70848. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68065/0.70911. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67973/0.70864. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67979/0.70854. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67942/0.70875. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67882/0.70842. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67897/0.70822. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67933/0.70784. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67826/0.70706. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67890/0.70671. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67849/0.70610. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67881/0.70574. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67762/0.70634. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67780/0.70622. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67714/0.70619. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67698/0.70571. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67672/0.70598. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67590/0.70619. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67569/0.70615. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67614/0.70573. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67442/0.70627. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67591/0.70617. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67464/0.70602. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67498/0.70552. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67468/0.70591. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67298/0.70556. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67456/0.70548. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67407/0.70488. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67260/0.70425. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67325/0.70380. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67454/0.70390. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67069/0.70456. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67142/0.70464. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67147/0.70431. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67304/0.70360. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67223/0.70317. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67276/0.70380. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66997/0.70378. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66924/0.70308. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66847/0.70383. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66920/0.70467. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 0.66929/0.70346. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66836/0.70324. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66779/0.70263. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66750/0.70357. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66591/0.70341. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66675/0.70369. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66508/0.70384. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66558/0.70305. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66537/0.70264. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66610/0.70252. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66420/0.70308. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66411/0.70155. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66222/0.70251. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66533/0.70155. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66352/0.70129. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66071/0.70054. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66327/0.70115. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66078/0.70224. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66126/0.69993. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65930/0.70026. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66036/0.70006. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66228/0.70040. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65943/0.69957. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65925/0.70074. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65662/0.70116. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65727/0.70191. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65679/0.70199. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65787/0.70018. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65391/0.70092. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65397/0.70243. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65776/0.70069. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65600/0.70020. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65358/0.70158. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65112/0.69924. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65117/0.70028. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65569/0.70135. Took 0.19 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69179/0.68460. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69084/0.68386. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69039/0.68427. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68997/0.68438. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68876/0.68451. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68842/0.68475. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68793/0.68491. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68779/0.68510. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68739/0.68507. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68653/0.68510. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68657/0.68518. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68533/0.68549. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68513/0.68525. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68541/0.68530. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68403/0.68529. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68349/0.68559. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68366/0.68511. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68208/0.68516. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68168/0.68520. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68221/0.68408. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68076/0.68331. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68068/0.68319. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68079/0.68301. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68031/0.68248. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68116/0.68238. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67821/0.68196. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67993/0.68132. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67850/0.68077. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67801/0.68090. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67664/0.68016. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.67509/0.67949. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67617/0.67913. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67542/0.67922. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67539/0.67907. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 0.67372/0.67926. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67590/0.67938. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67560/0.67848. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67261/0.67825. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67137/0.67839. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67324/0.67773. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67360/0.67810. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66887/0.67739. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67210/0.67715. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66987/0.67735. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66801/0.67661. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66961/0.67899. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66835/0.67840. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66886/0.67829. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66779/0.67712. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66860/0.67653. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66676/0.67900. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66769/0.67865. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66441/0.67972. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66361/0.68042. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66754/0.68002. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66539/0.68096. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66253/0.68192. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66415/0.68243. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66324/0.68118. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66156/0.68339. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66006/0.68514. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65967/0.68543. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66093/0.68554. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66100/0.68514. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66027/0.68427. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65593/0.68685. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65985/0.68727. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65703/0.68541. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65695/0.68763. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65396/0.68847. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65451/0.68773. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65116/0.68921. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65556/0.69087. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65117/0.69126. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65170/0.69113. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65281/0.69180. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65023/0.69341. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65106/0.69050. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65000/0.69236. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64749/0.69323. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65020/0.69337. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64950/0.69249. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64458/0.69519. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64601/0.69868. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64545/0.69610. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64363/0.69793. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64701/0.69740. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64054/0.69571. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64317/0.69810. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64083/0.70338. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64332/0.70268. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63964/0.70113. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63927/0.70278. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63944/0.70438. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64109/0.70461. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63506/0.71055. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63492/0.70589. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63936/0.70704. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63648/0.70639. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63465/0.71063. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69288/0.68814. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68905/0.68561. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68843/0.68421. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68873/0.68292. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68706/0.68190. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68682/0.68078. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68624/0.67992. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68631/0.67910. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68442/0.67831. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68462/0.67765. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68413/0.67672. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68269/0.67647. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68276/0.67614. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68221/0.67577. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68215/0.67564. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68167/0.67574. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68093/0.67575. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.67939/0.67575. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68015/0.67599. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67973/0.67592. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68000/0.67617. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67849/0.67650. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67945/0.67681. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67930/0.67686. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67860/0.67722. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67799/0.67732. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67709/0.67771. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67657/0.67852. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67736/0.67901. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67707/0.67901. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67613/0.67909. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67438/0.68050. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67555/0.68054. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67595/0.68092. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67413/0.68135. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67581/0.68221. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67489/0.68278. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67415/0.68305. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67436/0.68410. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67422/0.68478. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67270/0.68519. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67251/0.68593. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67212/0.68633. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67215/0.68647. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67256/0.68658. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67114/0.68698. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67195/0.68764. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67189/0.68847. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67239/0.68950. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67183/0.68984. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67265/0.69013. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66997/0.69033. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67238/0.69076. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66946/0.69152. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66882/0.69249. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67010/0.69365. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66949/0.69367. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67025/0.69411. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66722/0.69514. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66616/0.69581. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66765/0.69608. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66786/0.69677. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66639/0.69704. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66632/0.69784. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66560/0.69899. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66580/0.69993. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66609/0.70013. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66558/0.70079. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66407/0.70163. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66533/0.70218. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66294/0.70266. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66370/0.70282. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66342/0.70267. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66233/0.70347. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66206/0.70411. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66276/0.70433. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66262/0.70537. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66047/0.70651. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66078/0.70657. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65803/0.70780. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65893/0.70851. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65845/0.70903. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65918/0.70981. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65916/0.70921. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65727/0.71063. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65799/0.71050. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65559/0.71128. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65533/0.71209. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65712/0.71249. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65601/0.71319. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65393/0.71329. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65359/0.71404. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65283/0.71585. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65194/0.71711. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65150/0.71767. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65256/0.71754. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65028/0.71802. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65194/0.71979. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64823/0.72053. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64769/0.72090. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69236/0.69310. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68956/0.69459. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68963/0.69537. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68896/0.69562. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68915/0.69583. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68839/0.69602. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68778/0.69635. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68795/0.69616. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68739/0.69617. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68670/0.69606. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68647/0.69605. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68678/0.69614. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68585/0.69626. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68592/0.69635. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68485/0.69613. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68439/0.69597. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68479/0.69561. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68402/0.69548. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68237/0.69545. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68175/0.69563. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68261/0.69613. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68058/0.69638. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68328/0.69614. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68062/0.69632. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68069/0.69661. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67912/0.69683. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67946/0.69667. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67721/0.69746. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67629/0.69793. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67518/0.69852. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67488/0.69872. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67441/0.69914. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67195/0.69940. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66997/0.70060. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67127/0.70043. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67035/0.70031. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67080/0.70077. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66680/0.70226. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66555/0.70270. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66552/0.70379. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66322/0.70346. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66324/0.70386. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66174/0.70438. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66226/0.70452. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65753/0.70536. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66174/0.70547. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65933/0.70558. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65579/0.70582. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65425/0.70723. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.65431/0.70795. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65167/0.70827. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65243/0.70896. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.64881/0.70841. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65038/0.70872. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65017/0.70785. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64944/0.70891. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64783/0.70812. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64655/0.71131. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64092/0.71098. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64596/0.71176. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64143/0.71347. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.63988/0.71389. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63982/0.71511. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63390/0.71643. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63908/0.71696. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63755/0.71827. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63485/0.71882. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63518/0.71849. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63362/0.71942. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.63358/0.72141. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63069/0.72196. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62979/0.72071. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63070/0.72470. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62777/0.72579. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63084/0.72409. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.62827/0.72762. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62666/0.72668. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62554/0.72808. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62288/0.72749. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62139/0.73081. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62214/0.72631. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62164/0.72907. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.61575/0.72895. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62035/0.72838. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61167/0.73305. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61470/0.73492. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61896/0.73765. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.61624/0.74026. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61560/0.73850. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61326/0.74108. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60984/0.74056. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.61123/0.74302. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61119/0.74149. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60858/0.74386. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60047/0.74542. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60325/0.74510. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60778/0.74655. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60365/0.74819. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59703/0.75162. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.59902/0.74647. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69300/0.69120. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69087/0.68892. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69015/0.68791. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68953/0.68748. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68892/0.68737. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68845/0.68729. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68820/0.68701. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68750/0.68681. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68754/0.68687. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68699/0.68689. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68590/0.68660. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68612/0.68670. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68464/0.68667. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68463/0.68658. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68423/0.68680. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68361/0.68682. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68326/0.68663. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68250/0.68695. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68222/0.68702. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68015/0.68753. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67996/0.68749. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67894/0.68783. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67853/0.68826. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67803/0.68855. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67769/0.68859. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67651/0.68858. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67738/0.68838. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67426/0.68880. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67356/0.68856. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67337/0.68911. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67067/0.68924. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67001/0.68933. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67095/0.68951. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.66867/0.68946. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66856/0.69048. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66651/0.69022. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66574/0.69155. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66354/0.69258. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66232/0.69259. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.66360/0.69382. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66232/0.69358. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66260/0.69487. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66052/0.69536. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.65936/0.69550. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.65819/0.69654. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65429/0.69729. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.65531/0.69856. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.65252/0.70002. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65266/0.70035. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.65291/0.70204. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.64982/0.70295. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.64744/0.70529. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.64832/0.70770. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.64570/0.70868. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64846/0.70742. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.64591/0.70773. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.64892/0.70883. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64284/0.70915. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64157/0.71243. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.64134/0.71432. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.64136/0.71487. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.63695/0.71500. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63838/0.71618. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.63440/0.71550. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63842/0.71688. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.63256/0.71669. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63379/0.71591. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63361/0.71983. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.63144/0.72159. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.63578/0.72316. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63028/0.72452. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.62878/0.72543. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62941/0.72413. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62712/0.72458. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62619/0.72565. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.62977/0.72467. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.62548/0.72507. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62602/0.72673. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62266/0.72846. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62172/0.72884. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62322/0.73058. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61803/0.73213. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61792/0.73261. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62122/0.73156. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.61675/0.73282. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62116/0.73168. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62022/0.73444. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61983/0.73478. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61321/0.73982. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61193/0.73884. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61424/0.73745. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61376/0.73590. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.61038/0.73699. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61545/0.73633. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61208/0.73877. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.61641/0.73684. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.60483/0.73976. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.61111/0.74290. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60493/0.74401. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60543/0.74446. Took 0.19 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69554/0.69720. Took 0.34 sec\n",
      "Epoch 1, Loss(train/val) 0.69386/0.69459. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69230/0.69399. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69206/0.69316. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69128/0.69288. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69170/0.69214. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69042/0.69156. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69070/0.69147. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69030/0.69160. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69003/0.69145. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69022/0.69113. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68906/0.69163. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68846/0.69117. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68796/0.69150. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68779/0.69156. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68736/0.69172. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68785/0.69170. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68658/0.69184. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68635/0.69285. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68588/0.69300. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68519/0.69305. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68423/0.69364. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68473/0.69425. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68418/0.69460. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68340/0.69516. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68334/0.69588. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68091/0.69657. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68190/0.69761. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68110/0.69764. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68120/0.69855. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68113/0.70041. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68099/0.69943. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68080/0.69992. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67944/0.69991. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67917/0.70127. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67813/0.70170. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67749/0.70242. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67831/0.70179. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67731/0.70278. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67732/0.70286. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67752/0.70247. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67523/0.70478. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67669/0.70366. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67716/0.70422. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67383/0.70515. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67432/0.70321. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67371/0.70524. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67517/0.70537. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67383/0.70527. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67452/0.70646. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67458/0.70646. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67359/0.70507. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67139/0.70715. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67239/0.70717. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67250/0.70671. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67195/0.70782. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67088/0.70629. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67094/0.70948. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66961/0.70787. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67128/0.70982. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66874/0.70771. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66912/0.71063. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66717/0.71154. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66758/0.71174. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66672/0.70986. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66574/0.71345. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66577/0.71345. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66716/0.71300. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66623/0.71779. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66464/0.71372. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66499/0.71371. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66426/0.71649. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66456/0.71251. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66238/0.71795. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66292/0.71685. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66090/0.71761. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66147/0.71475. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66254/0.71129. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65709/0.71533. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65790/0.71855. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66007/0.71935. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65685/0.71498. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65574/0.71995. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65489/0.71904. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65536/0.72062. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65665/0.72601. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65445/0.72271. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65361/0.72310. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65089/0.72160. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65284/0.72402. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64942/0.72870. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65282/0.73064. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65091/0.73020. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64911/0.72444. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65192/0.72817. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64665/0.73231. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64761/0.72569. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64569/0.72778. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64496/0.73247. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64730/0.72540. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69452/0.68911. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69225/0.69423. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69169/0.69630. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69097/0.69840. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69104/0.70013. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69034/0.70188. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69065/0.70234. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68998/0.70341. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68947/0.70460. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68960/0.70542. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68930/0.70615. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68862/0.70695. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68846/0.70717. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68841/0.70748. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68894/0.70806. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68747/0.70780. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68815/0.70882. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68841/0.70910. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68777/0.70915. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68835/0.70918. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68802/0.70942. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68750/0.71019. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68855/0.70966. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68795/0.71058. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68828/0.71037. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68817/0.71016. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68644/0.71043. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68701/0.71050. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68664/0.71118. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68684/0.71105. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68641/0.71196. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68669/0.71165. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68597/0.71178. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68586/0.71101. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68612/0.71157. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68571/0.71235. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68493/0.71173. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68490/0.71255. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68486/0.71235. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68541/0.71254. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68535/0.71233. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68532/0.71241. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68487/0.71301. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68563/0.71352. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68591/0.71402. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68385/0.71392. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68549/0.71406. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68475/0.71406. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68499/0.71390. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68274/0.71479. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68504/0.71395. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68380/0.71423. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68339/0.71383. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68318/0.71443. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68318/0.71359. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68361/0.71394. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68216/0.71431. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.68240/0.71498. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68197/0.71522. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.68183/0.71410. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.68124/0.71470. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.68220/0.71488. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.68288/0.71471. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.68021/0.71520. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.68112/0.71397. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.68042/0.71360. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67847/0.71367. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.68084/0.71312. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.68057/0.71400. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67840/0.71524. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67882/0.71340. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67961/0.71411. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.68109/0.71336. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67838/0.71387. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67734/0.71350. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67817/0.71375. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67916/0.71346. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67753/0.71299. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67754/0.71435. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.67743/0.71357. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67755/0.71120. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67590/0.71172. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67646/0.71402. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.67756/0.71280. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.67482/0.71361. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.67488/0.71439. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.67741/0.71198. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.67615/0.71266. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.67549/0.71395. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.67118/0.71473. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.67358/0.71476. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.67203/0.71426. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.67442/0.71385. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.67202/0.71238. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.67345/0.71395. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.67042/0.71291. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66870/0.71530. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.67071/0.71334. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66978/0.71265. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.66756/0.71356. Took 0.18 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69544/0.69438. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69165/0.69719. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69114/0.69964. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69046/0.70148. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68901/0.70309. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68947/0.70456. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68864/0.70567. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68812/0.70685. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68886/0.70762. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68790/0.70838. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68829/0.70876. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68793/0.70908. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68773/0.70958. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68700/0.70995. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68767/0.70990. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68704/0.70975. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68591/0.70991. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68622/0.71004. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68667/0.71001. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68574/0.70991. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68512/0.70995. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68540/0.70970. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68579/0.70934. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68541/0.70919. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68444/0.70885. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68526/0.70885. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68429/0.70901. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68271/0.70914. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68401/0.70868. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68317/0.70873. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68346/0.70826. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68351/0.70774. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68183/0.70762. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68238/0.70762. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68163/0.70789. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68098/0.70749. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68187/0.70692. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68207/0.70647. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68088/0.70578. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68011/0.70562. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67911/0.70577. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67898/0.70508. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67864/0.70489. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67869/0.70525. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67687/0.70545. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.67705/0.70537. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67620/0.70452. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67722/0.70421. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67716/0.70409. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67602/0.70371. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67641/0.70335. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67508/0.70305. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67470/0.70316. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67476/0.70340. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67247/0.70276. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67408/0.70229. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67304/0.70362. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67267/0.70346. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67001/0.70247. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67059/0.70370. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67043/0.70401. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66967/0.70507. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66769/0.70437. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66622/0.70316. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66851/0.70463. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66828/0.70421. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66841/0.70380. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66682/0.70407. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66632/0.70427. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66463/0.70351. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66635/0.70360. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66190/0.70286. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65927/0.70177. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66221/0.70290. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66204/0.70348. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66258/0.70279. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65904/0.70405. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65686/0.70363. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65875/0.70316. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65763/0.70168. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65712/0.70430. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65753/0.70403. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65580/0.70455. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65203/0.70458. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65254/0.70458. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65131/0.70377. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65266/0.70191. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65119/0.70521. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65187/0.70650. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65167/0.70689. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65080/0.70665. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64848/0.70784. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64985/0.70659. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64921/0.71094. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64726/0.71095. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64263/0.70859. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64372/0.70987. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64577/0.71189. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64170/0.71049. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64660/0.71255. Took 0.19 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69498/0.68690. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69276/0.68737. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69088/0.68771. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68979/0.68731. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68997/0.68652. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68827/0.68570. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68840/0.68456. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68692/0.68384. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68628/0.68296. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68565/0.68212. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68510/0.68176. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68412/0.68038. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68327/0.68072. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68290/0.68037. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68177/0.68162. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68223/0.68177. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68131/0.68193. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68079/0.68155. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67975/0.68167. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67970/0.68208. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67920/0.68278. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67879/0.68307. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67946/0.68231. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67669/0.68298. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67701/0.68385. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67629/0.68367. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67753/0.68371. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67623/0.68322. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67480/0.68367. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67338/0.68368. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67365/0.68411. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67195/0.68323. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67267/0.68445. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67122/0.68443. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67375/0.68348. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66827/0.68304. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67064/0.68202. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67106/0.68173. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67024/0.68104. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66972/0.68376. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66938/0.68145. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66759/0.68317. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66737/0.68314. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66482/0.68346. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66651/0.68350. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66527/0.68261. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66329/0.68183. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66225/0.68200. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66062/0.68216. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65840/0.68173. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65933/0.68138. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65844/0.68168. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65856/0.68156. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.65982/0.68109. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65903/0.68089. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65400/0.68066. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65554/0.68188. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65143/0.68130. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65157/0.68109. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65040/0.68141. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65070/0.68145. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64735/0.67959. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65041/0.67974. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64615/0.67712. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64575/0.67903. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64305/0.68149. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64044/0.68312. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64682/0.68252. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64045/0.68036. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64101/0.67989. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64022/0.67833. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64128/0.67843. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64055/0.67769. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.63638/0.67796. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63582/0.67687. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.63335/0.67732. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63820/0.68028. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63526/0.68268. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63278/0.67766. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63213/0.67710. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63198/0.67852. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62696/0.68071. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63047/0.68419. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62533/0.68150. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62643/0.67938. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62876/0.67892. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62674/0.68219. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62421/0.68144. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62155/0.68109. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61661/0.68596. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61783/0.68344. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61828/0.68789. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61771/0.69262. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61838/0.68475. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61675/0.68764. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61772/0.68630. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60955/0.68624. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.61797/0.68799. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.61716/0.68369. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61348/0.68484. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69499/0.69231. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69433/0.69216. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69323/0.69206. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69136/0.69208. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69121/0.69211. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69122/0.69226. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69011/0.69245. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68959/0.69265. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68929/0.69299. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68751/0.69337. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68786/0.69382. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68718/0.69428. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68784/0.69462. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68652/0.69500. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68483/0.69536. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68613/0.69569. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68558/0.69588. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68476/0.69621. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68386/0.69661. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68369/0.69687. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68337/0.69701. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68370/0.69699. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68306/0.69680. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68233/0.69675. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68103/0.69676. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68176/0.69680. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68354/0.69668. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68118/0.69679. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68112/0.69678. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68093/0.69711. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68052/0.69666. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68088/0.69641. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67846/0.69632. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67869/0.69638. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67971/0.69636. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67869/0.69695. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67893/0.69692. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67891/0.69697. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67812/0.69683. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67566/0.69722. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67760/0.69758. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67762/0.69763. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67737/0.69774. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67686/0.69744. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67659/0.69803. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67541/0.69835. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67552/0.69867. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67501/0.69854. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67572/0.69871. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67618/0.69892. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67515/0.69912. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67441/0.69891. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67595/0.69858. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67387/0.69940. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67390/0.70036. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67380/0.70071. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67147/0.70106. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67113/0.70125. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67236/0.70096. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67181/0.70055. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67292/0.70034. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67039/0.70083. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67034/0.70125. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67108/0.70123. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67082/0.70198. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67148/0.70271. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66994/0.70368. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66955/0.70362. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67008/0.70283. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66943/0.70413. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66863/0.70370. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67011/0.70361. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66810/0.70420. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66764/0.70434. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66986/0.70329. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66865/0.70415. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66530/0.70453. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66808/0.70498. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66724/0.70653. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66662/0.70645. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66905/0.70715. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66809/0.70748. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66504/0.70780. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66433/0.70832. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66643/0.70867. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66661/0.70838. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66427/0.70830. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66368/0.70810. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66484/0.70740. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66449/0.70641. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66488/0.70709. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66346/0.70751. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66364/0.70724. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66320/0.70753. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66055/0.70875. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66256/0.70877. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66025/0.70971. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66164/0.70835. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66028/0.70769. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.66235/0.70800. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69390/0.69475. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69339/0.69513. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69266/0.69534. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69268/0.69562. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69312/0.69597. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69245/0.69619. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69138/0.69655. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69070/0.69691. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69087/0.69762. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68923/0.69851. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68942/0.69891. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68848/0.69961. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68781/0.70050. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68653/0.70151. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68707/0.70237. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68504/0.70302. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68475/0.70423. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68326/0.70519. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68291/0.70631. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68292/0.70762. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68251/0.70850. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68145/0.70945. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68099/0.71025. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67985/0.71086. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67790/0.71170. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67757/0.71262. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67819/0.71392. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67754/0.71484. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67577/0.71484. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67528/0.71596. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67523/0.71749. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67454/0.71985. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67355/0.72066. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67304/0.72146. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67311/0.72221. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67112/0.72324. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67136/0.72479. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.66984/0.72637. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67108/0.72761. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67204/0.72816. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.66826/0.72840. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67016/0.73029. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66840/0.73073. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66649/0.73220. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66720/0.73220. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66597/0.73276. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66488/0.73465. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66809/0.73557. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66411/0.73650. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66474/0.73682. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.66241/0.73751. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.66328/0.73995. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.66283/0.73977. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66115/0.74126. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66009/0.74196. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65850/0.74159. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65854/0.74487. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66028/0.74434. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65728/0.74496. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65321/0.74638. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65296/0.74764. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.65701/0.74589. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.65136/0.74740. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64874/0.74817. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65114/0.74910. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65302/0.74858. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65089/0.75110. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64765/0.75044. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64607/0.75144. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64567/0.75145. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64789/0.75144. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64264/0.75270. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64576/0.75324. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64215/0.75374. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64501/0.75263. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64064/0.75393. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63520/0.75344. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64187/0.75568. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63882/0.75706. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63922/0.75940. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63770/0.76081. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63537/0.76231. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63430/0.76234. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63558/0.76178. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63518/0.76119. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63168/0.76272. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63287/0.76336. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62964/0.76219. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62731/0.76299. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63244/0.76389. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62589/0.76732. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62998/0.76841. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63087/0.76924. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.62365/0.77097. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62772/0.77149. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62832/0.77185. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62985/0.77361. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62298/0.77210. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62368/0.77288. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62798/0.77242. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69449/0.69805. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69372/0.69748. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69259/0.69741. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69274/0.69814. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69169/0.69857. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69224/0.69928. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69157/0.69982. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69165/0.70069. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69028/0.70152. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69009/0.70257. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68994/0.70406. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68868/0.70571. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68840/0.70777. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68697/0.70956. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68697/0.71165. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68755/0.71359. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68556/0.71606. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68487/0.71816. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68487/0.72001. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68379/0.72237. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68170/0.72482. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68193/0.72720. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68198/0.72899. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68158/0.73017. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68119/0.73180. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68000/0.73341. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68053/0.73496. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67940/0.73640. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67807/0.73779. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67848/0.73933. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67784/0.74070. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67834/0.74215. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67775/0.74361. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67748/0.74429. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67768/0.74494. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67785/0.74621. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67542/0.74765. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67601/0.74867. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67736/0.74810. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67557/0.74832. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67439/0.74922. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67365/0.74945. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67451/0.74930. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67383/0.74959. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67467/0.75004. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67344/0.75101. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67235/0.75142. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67178/0.75228. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67270/0.75173. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67368/0.75224. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67041/0.75413. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67148/0.75479. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67013/0.75560. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66815/0.75706. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67069/0.75780. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66917/0.75681. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66977/0.75789. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66827/0.75781. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67020/0.75873. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66766/0.75935. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66775/0.75914. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66825/0.75991. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66605/0.76069. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66648/0.76135. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66594/0.76149. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66419/0.76137. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66643/0.76210. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66493/0.76164. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66464/0.76350. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66500/0.76201. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66384/0.76114. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66318/0.76158. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66335/0.76244. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66189/0.76393. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66119/0.76491. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66238/0.76388. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65941/0.76337. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66005/0.76402. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65985/0.76579. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66193/0.76600. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66006/0.76595. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66016/0.76672. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65839/0.76622. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65794/0.76643. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65745/0.76763. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65647/0.76809. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65605/0.76893. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65561/0.76984. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65457/0.76884. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65281/0.77148. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65298/0.77125. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65309/0.76996. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64874/0.77161. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65257/0.77298. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65130/0.77459. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65079/0.77583. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64888/0.77742. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64713/0.77608. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65069/0.77541. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64567/0.77754. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69325/0.69332. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69163/0.69297. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69148/0.69268. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69097/0.69229. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69060/0.69202. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69018/0.69176. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68922/0.69164. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68887/0.69138. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68808/0.69122. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68812/0.69113. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68684/0.69099. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68642/0.69103. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68674/0.69094. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68530/0.69084. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68361/0.69132. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68355/0.69139. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68440/0.69144. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68373/0.69171. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68267/0.69159. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68132/0.69219. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68145/0.69324. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68013/0.69393. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68144/0.69445. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67905/0.69493. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67998/0.69531. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67783/0.69600. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67798/0.69727. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67728/0.69826. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67653/0.69978. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67640/0.70063. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67612/0.70123. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67621/0.70181. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67507/0.70272. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67392/0.70421. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67512/0.70497. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67485/0.70584. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67354/0.70618. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67362/0.70722. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67231/0.70754. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67208/0.70871. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67183/0.70906. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67327/0.70965. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67418/0.71004. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66918/0.71103. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67103/0.71205. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67021/0.71271. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66915/0.71311. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66972/0.71391. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66931/0.71472. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67036/0.71443. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66872/0.71408. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66915/0.71454. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66752/0.71517. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66800/0.71620. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66621/0.71626. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66603/0.71719. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66704/0.71796. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66483/0.71867. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66398/0.72021. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66447/0.72149. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66420/0.72218. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66367/0.72294. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66220/0.72418. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66175/0.72485. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66135/0.72565. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66213/0.72568. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66131/0.72583. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65891/0.72776. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66027/0.72832. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65948/0.72845. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66014/0.72882. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65747/0.72934. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65607/0.73067. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65779/0.73230. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65703/0.73288. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65455/0.73412. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65526/0.73574. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65592/0.73635. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65524/0.73494. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65457/0.73440. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65241/0.73900. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65337/0.73834. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65085/0.73923. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65248/0.73985. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65086/0.73897. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65240/0.74062. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64900/0.74153. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64996/0.74055. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65030/0.74363. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64679/0.74402. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64541/0.74490. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64723/0.74596. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64537/0.74803. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64683/0.74889. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64180/0.74982. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64522/0.75086. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64150/0.75118. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64643/0.75446. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64183/0.75564. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64403/0.75750. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69375/0.69584. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69311/0.69555. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69285/0.69549. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69263/0.69542. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69223/0.69522. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69189/0.69516. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69115/0.69521. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69130/0.69517. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69097/0.69528. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69098/0.69535. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68923/0.69520. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69046/0.69514. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68933/0.69528. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68895/0.69516. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68851/0.69517. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68725/0.69536. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68710/0.69525. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68718/0.69553. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68698/0.69553. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68532/0.69551. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68584/0.69518. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68469/0.69530. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 0.68295/0.69571. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68361/0.69610. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68300/0.69650. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68141/0.69750. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68099/0.69765. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67829/0.69808. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67862/0.69867. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67827/0.69874. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67790/0.69939. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67496/0.69918. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67519/0.69940. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67273/0.70040. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67179/0.70146. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67275/0.70256. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67071/0.70378. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.66812/0.70464. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66714/0.70473. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.66480/0.70529. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66519/0.70582. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66188/0.70796. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66415/0.70852. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66154/0.70986. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.66004/0.71113. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65879/0.71230. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.65590/0.71424. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65747/0.71837. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65417/0.71709. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65470/0.71888. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65161/0.71925. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65147/0.72054. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.64832/0.72337. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64983/0.72510. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.64674/0.72402. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64582/0.72539. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64524/0.72768. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64737/0.72787. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64207/0.73248. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64446/0.73211. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.64397/0.73317. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64338/0.73626. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64058/0.73522. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63711/0.73973. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63967/0.73702. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63691/0.73751. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.63786/0.73845. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63578/0.73553. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63145/0.73417. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63133/0.74027. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63172/0.74118. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62966/0.74598. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63103/0.74353. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63000/0.74458. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62565/0.74317. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.62265/0.74677. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62659/0.74895. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62198/0.74933. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62569/0.75076. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62392/0.74679. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62460/0.74772. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61951/0.75163. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62082/0.75622. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62435/0.75200. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61791/0.75632. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.61953/0.75952. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61706/0.76000. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61307/0.76224. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60938/0.76435. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61305/0.77063. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61627/0.76617. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61756/0.76871. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61559/0.76944. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61177/0.76946. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61159/0.77406. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.61427/0.77533. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60984/0.77538. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60859/0.76742. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60880/0.76532. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61165/0.76650. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69537/0.70136. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69393/0.69713. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69250/0.69525. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69241/0.69483. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69176/0.69483. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69180/0.69486. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69065/0.69574. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69006/0.69549. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69008/0.69566. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68871/0.69677. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68999/0.69776. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68819/0.69843. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68752/0.69928. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68618/0.69979. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68620/0.70024. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68623/0.70192. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68580/0.70254. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68431/0.70378. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68452/0.70466. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68325/0.70521. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68366/0.70453. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68272/0.70612. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68196/0.70696. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68169/0.70580. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68180/0.70575. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68103/0.70783. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68111/0.70823. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67909/0.70660. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67826/0.71042. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67791/0.70913. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67750/0.71022. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67821/0.70957. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67512/0.70894. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67704/0.71009. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67571/0.71077. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67459/0.70967. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67451/0.71047. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67207/0.71049. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67569/0.71096. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67417/0.71044. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67311/0.70882. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67294/0.70901. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67136/0.71067. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67142/0.70894. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67193/0.70897. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66965/0.70961. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67013/0.70924. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66780/0.71043. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66956/0.70722. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66947/0.70768. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66739/0.70835. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66628/0.70946. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66589/0.70743. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66569/0.70800. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66438/0.70869. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66583/0.70903. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66493/0.70848. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66280/0.70953. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66487/0.70953. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66404/0.71120. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66179/0.71006. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66190/0.70932. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66218/0.70988. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66292/0.70920. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66180/0.70795. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65985/0.71013. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65926/0.70900. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66136/0.70844. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65973/0.70923. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65843/0.70978. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65670/0.70989. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65607/0.70875. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65926/0.70828. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65662/0.70996. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65511/0.70955. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65075/0.70960. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65124/0.71059. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65208/0.71216. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65393/0.71026. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65001/0.71018. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64996/0.70753. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64801/0.70991. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64972/0.70687. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64706/0.71039. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64914/0.70828. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64815/0.70895. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64481/0.70679. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64832/0.70748. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64815/0.70731. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64709/0.70886. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64454/0.70470. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64378/0.70703. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64329/0.70717. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64048/0.70871. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63822/0.70428. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64047/0.70582. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64109/0.70850. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63813/0.70744. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63957/0.70713. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63958/0.70617. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69525/0.69322. Took 0.38 sec\n",
      "Epoch 1, Loss(train/val) 0.69358/0.69222. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69348/0.69191. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69326/0.69184. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69314/0.69174. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69209/0.69170. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69195/0.69165. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69163/0.69172. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69116/0.69187. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69123/0.69200. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69011/0.69212. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69022/0.69220. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68976/0.69248. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68969/0.69288. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68932/0.69331. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68828/0.69378. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68884/0.69432. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68869/0.69492. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68765/0.69544. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68665/0.69614. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68646/0.69691. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68626/0.69774. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68664/0.69881. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68624/0.69985. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68589/0.70075. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68400/0.70167. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68423/0.70266. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68462/0.70372. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68444/0.70483. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68321/0.70560. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68332/0.70645. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68296/0.70730. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68257/0.70823. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68138/0.70947. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68253/0.71030. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68143/0.71092. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68023/0.71165. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68033/0.71262. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67998/0.71325. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67890/0.71444. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67987/0.71494. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67867/0.71568. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67738/0.71657. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67788/0.71771. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67980/0.71778. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67582/0.71872. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67679/0.71841. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67876/0.71873. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67600/0.71990. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67591/0.72023. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67515/0.72077. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67400/0.72124. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67400/0.72153. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67312/0.72216. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67262/0.72264. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67266/0.72321. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67175/0.72356. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67167/0.72377. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67026/0.72455. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66995/0.72475. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67012/0.72509. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66844/0.72605. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66928/0.72606. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66786/0.72638. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66673/0.72722. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66607/0.72680. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66302/0.72748. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66414/0.72869. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66261/0.72949. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66285/0.72988. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66180/0.72930. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66203/0.72951. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65866/0.73166. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65742/0.73164. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65661/0.73138. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65716/0.73039. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65259/0.73237. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65321/0.73380. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64935/0.73449. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65076/0.73706. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64950/0.73520. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64683/0.73772. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64478/0.73555. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64659/0.73813. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64031/0.73779. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64014/0.74151. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63541/0.74464. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63464/0.74257. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63639/0.74292. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63503/0.74906. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63247/0.74882. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63026/0.74644. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63419/0.75039. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62842/0.75223. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62419/0.75431. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62789/0.75525. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62468/0.75078. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62620/0.75897. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61706/0.76028. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61885/0.76188. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69599/0.68992. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69362/0.69004. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69216/0.69043. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69195/0.69076. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69191/0.69095. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69076/0.69103. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69051/0.69132. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69008/0.69159. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69045/0.69167. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68980/0.69187. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68893/0.69212. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68913/0.69215. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68876/0.69254. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68870/0.69279. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68754/0.69325. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68752/0.69335. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68822/0.69363. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68769/0.69396. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68653/0.69439. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68681/0.69474. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68725/0.69493. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68684/0.69517. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68595/0.69551. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68565/0.69580. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68582/0.69627. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68587/0.69651. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 0.68548/0.69673. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68373/0.69733. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68451/0.69746. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68613/0.69792. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68419/0.69826. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68543/0.69867. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68424/0.69902. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68410/0.69907. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68364/0.69966. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68379/0.69999. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68383/0.70050. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68318/0.70080. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68382/0.70091. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68244/0.70095. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68309/0.70128. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68268/0.70186. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68233/0.70181. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68240/0.70251. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68103/0.70264. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68094/0.70313. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68100/0.70333. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68171/0.70405. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68086/0.70426. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67972/0.70542. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67983/0.70482. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68135/0.70476. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67971/0.70478. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67904/0.70521. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67950/0.70616. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67950/0.70616. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.68016/0.70655. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67849/0.70706. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 0.67856/0.70693. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67721/0.70820. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67821/0.70842. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67940/0.70878. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67746/0.70945. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67768/0.70966. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.67497/0.71016. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.67533/0.71103. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67617/0.71099. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67535/0.71203. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67383/0.71190. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67526/0.71365. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 0.67331/0.71412. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67345/0.71368. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.67442/0.71377. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67312/0.71515. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.67377/0.71598. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67126/0.71625. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.67222/0.71674. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67277/0.71771. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.67315/0.71762. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67177/0.71852. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.67172/0.72021. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67080/0.72115. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.66886/0.72123. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.67122/0.72098. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66824/0.72153. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66969/0.72159. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.67000/0.72173. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.67043/0.72261. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66650/0.72361. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66582/0.72496. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66722/0.72474. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66418/0.72572. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66545/0.72632. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66309/0.72684. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.66485/0.72688. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66497/0.72741. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.66291/0.72868. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66239/0.73131. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.66179/0.73223. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65981/0.73237. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69277/0.69220. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69161/0.69141. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69105/0.69113. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69043/0.69086. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69047/0.69074. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69015/0.69066. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69009/0.69069. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68971/0.69059. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68903/0.69054. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68885/0.69063. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68961/0.69060. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68990/0.69060. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68948/0.69051. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68854/0.69018. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68879/0.69029. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68798/0.69016. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68834/0.69022. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68772/0.69024. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68734/0.69033. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68814/0.69028. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68727/0.69029. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68706/0.69032. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68614/0.69040. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68642/0.69045. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68692/0.69050. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68656/0.69041. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68499/0.69045. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68514/0.69018. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68367/0.69060. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68335/0.69058. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68289/0.69083. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68283/0.69102. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68233/0.69098. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68281/0.69068. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68163/0.69127. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68105/0.69160. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67969/0.69247. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67954/0.69240. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67973/0.69232. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67775/0.69207. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67757/0.69291. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67606/0.69312. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67616/0.69338. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67426/0.69401. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67277/0.69440. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67156/0.69437. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67114/0.69466. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67091/0.69344. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66877/0.69416. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66694/0.69517. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66673/0.69428. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66672/0.69451. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66423/0.69518. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66119/0.69356. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66049/0.69313. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66038/0.69467. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65787/0.69327. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65896/0.69309. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65516/0.69234. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65591/0.69232. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65379/0.69286. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65090/0.69097. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65069/0.69224. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65117/0.69114. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64883/0.69311. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64954/0.69349. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64368/0.69299. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.64399/0.69431. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64460/0.69436. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64176/0.69545. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64460/0.69666. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64026/0.69513. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64052/0.69881. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63895/0.70012. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64096/0.69976. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63553/0.69812. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63210/0.69665. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63340/0.69872. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62825/0.70073. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63172/0.70480. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63188/0.70554. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63062/0.70455. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62841/0.70437. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62547/0.70703. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62577/0.70509. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62202/0.70955. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62488/0.70991. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62268/0.71275. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62018/0.71359. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62356/0.71261. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61557/0.71307. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61913/0.71800. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61830/0.71857. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61377/0.71704. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61657/0.71931. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61332/0.72087. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61430/0.72350. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60968/0.72216. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61290/0.72112. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61349/0.72224. Took 0.18 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69349/0.68721. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69069/0.68429. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69022/0.68386. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68907/0.68341. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68880/0.68312. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68866/0.68327. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68851/0.68335. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68799/0.68339. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68785/0.68375. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68725/0.68411. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68743/0.68442. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68755/0.68473. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68690/0.68490. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68731/0.68528. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68688/0.68572. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68641/0.68608. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68624/0.68658. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68639/0.68670. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68601/0.68724. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68669/0.68739. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68570/0.68805. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68586/0.68828. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68560/0.68896. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68523/0.68898. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68486/0.68949. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68457/0.69026. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68466/0.69035. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68494/0.69091. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68426/0.69115. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68394/0.69154. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68390/0.69193. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68414/0.69254. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68306/0.69332. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68343/0.69396. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68360/0.69443. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68286/0.69510. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68187/0.69540. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68278/0.69599. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68249/0.69637. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68104/0.69662. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68206/0.69721. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68097/0.69780. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68123/0.69764. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68246/0.69801. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67989/0.69839. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68100/0.69909. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68028/0.69957. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67868/0.69994. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.68077/0.70083. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67947/0.70072. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67991/0.70048. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67818/0.70092. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67832/0.70166. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67795/0.70217. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67781/0.70251. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67724/0.70278. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67653/0.70364. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67675/0.70431. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67660/0.70432. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67626/0.70450. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67606/0.70489. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67514/0.70544. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67547/0.70521. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67254/0.70550. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67354/0.70568. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67300/0.70651. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67114/0.70814. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67255/0.70897. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67031/0.70939. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66968/0.71058. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66807/0.71096. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66716/0.71083. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66756/0.71202. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66901/0.71223. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66555/0.71254. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66693/0.71258. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66548/0.71373. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66411/0.71477. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66373/0.71561. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66323/0.71520. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66175/0.71652. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66149/0.71749. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65979/0.71810. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66015/0.71889. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65764/0.71957. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65420/0.72167. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65523/0.72252. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65511/0.72328. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65472/0.72337. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65317/0.72401. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65314/0.72575. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65055/0.72692. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65223/0.72703. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64846/0.72795. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64859/0.72858. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64624/0.73017. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64765/0.73286. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64526/0.73365. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64829/0.73299. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64699/0.73416. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69260/0.69253. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69128/0.69218. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69118/0.69192. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69003/0.69177. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69003/0.69177. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68962/0.69193. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68900/0.69207. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68855/0.69225. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68802/0.69244. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68786/0.69253. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68856/0.69263. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68770/0.69278. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68705/0.69292. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68715/0.69302. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68703/0.69322. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68718/0.69341. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68664/0.69381. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68460/0.69413. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68634/0.69431. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68564/0.69464. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68594/0.69492. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68411/0.69520. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68459/0.69555. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68305/0.69586. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68330/0.69626. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68290/0.69655. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68214/0.69718. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68182/0.69775. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68094/0.69861. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67985/0.69893. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68072/0.69964. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68135/0.70000. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67932/0.70053. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67907/0.70163. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67924/0.70198. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67886/0.70256. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67762/0.70316. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67633/0.70369. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67811/0.70407. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67901/0.70436. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67587/0.70511. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67763/0.70534. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67576/0.70599. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67499/0.70613. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67549/0.70695. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.67440/0.70678. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67515/0.70693. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67388/0.70756. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67374/0.70798. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67289/0.70860. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67220/0.70926. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67304/0.70977. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67297/0.71014. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67343/0.70995. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67344/0.71083. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67250/0.71124. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67003/0.71171. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66978/0.71275. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67058/0.71356. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66919/0.71400. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67062/0.71441. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66827/0.71444. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66834/0.71477. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66865/0.71543. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66718/0.71610. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66561/0.71644. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66755/0.71627. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66595/0.71683. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66763/0.71713. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66476/0.71791. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66516/0.71924. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66473/0.71996. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66391/0.72028. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66377/0.72091. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66401/0.72204. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66272/0.72323. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66462/0.72299. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65995/0.72372. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66024/0.72582. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66070/0.72606. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66071/0.72570. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66048/0.72634. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66007/0.72713. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65715/0.72774. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65906/0.72854. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65712/0.72966. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65580/0.73120. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65322/0.73303. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65667/0.73345. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65547/0.73470. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65422/0.73473. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65583/0.73491. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65320/0.73656. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65366/0.73752. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65263/0.73779. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65173/0.73718. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65038/0.73816. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65152/0.73881. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65000/0.73825. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64749/0.74040. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69593/0.68433. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69198/0.68887. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69092/0.69229. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68943/0.69523. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68846/0.69773. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68850/0.69993. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68791/0.70165. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68806/0.70298. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68729/0.70403. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68722/0.70477. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68698/0.70493. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68712/0.70545. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68640/0.70589. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68624/0.70645. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68622/0.70692. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68554/0.70706. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68608/0.70679. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68481/0.70672. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68528/0.70665. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68454/0.70672. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68470/0.70740. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68507/0.70733. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68462/0.70726. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68323/0.70736. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68436/0.70728. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68372/0.70770. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68341/0.70799. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68335/0.70777. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68289/0.70755. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68298/0.70790. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68256/0.70769. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68361/0.70745. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68281/0.70797. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68264/0.70765. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68169/0.70813. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68057/0.70837. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68209/0.70835. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68068/0.70838. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68022/0.70870. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68039/0.70978. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68016/0.70940. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67895/0.70906. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68017/0.70996. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67894/0.71062. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67899/0.71137. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67860/0.71096. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67717/0.71115. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67810/0.71125. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67746/0.71097. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67746/0.71076. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67686/0.71100. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67571/0.71122. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67597/0.71276. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67570/0.71333. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67486/0.71316. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67410/0.71236. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67482/0.71161. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67504/0.71333. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67444/0.71435. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67308/0.71345. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67219/0.71459. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67279/0.71502. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67143/0.71568. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67057/0.71733. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67089/0.71633. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66814/0.71609. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66770/0.71679. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66796/0.71757. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66862/0.71821. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66769/0.71845. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66574/0.72033. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66705/0.72035. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66585/0.71882. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66409/0.71898. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66463/0.72154. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66104/0.72287. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66182/0.72281. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66049/0.72331. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66039/0.72457. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66062/0.72617. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65805/0.72622. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65970/0.72640. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65847/0.72558. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65715/0.72866. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65694/0.73050. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65598/0.73001. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65440/0.72726. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65093/0.72854. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65306/0.72933. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65423/0.73009. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65210/0.73048. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65232/0.73240. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64768/0.73227. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64822/0.73419. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64745/0.73268. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64783/0.73356. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64654/0.73277. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64815/0.73698. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64015/0.73570. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64517/0.73521. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69777/0.69872. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69238/0.69728. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69126/0.69695. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68955/0.69694. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68868/0.69706. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68872/0.69740. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68772/0.69777. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68810/0.69826. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68814/0.69876. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68785/0.69914. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68762/0.69952. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68721/0.69990. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68693/0.70032. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68622/0.70058. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68645/0.70099. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68641/0.70165. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68606/0.70218. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68585/0.70266. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68612/0.70310. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68601/0.70346. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68608/0.70374. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68615/0.70423. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68585/0.70449. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68537/0.70466. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68472/0.70515. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68570/0.70532. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68496/0.70575. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68488/0.70619. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68487/0.70658. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68423/0.70706. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68380/0.70733. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68406/0.70769. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68388/0.70797. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68297/0.70836. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68364/0.70848. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68332/0.70883. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68344/0.70927. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68323/0.70952. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68299/0.70960. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68333/0.70996. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68219/0.71030. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68244/0.71049. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68198/0.71060. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68300/0.71077. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68182/0.71076. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68203/0.71113. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68089/0.71170. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68127/0.71178. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68090/0.71145. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.68131/0.71179. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67989/0.71182. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68012/0.71201. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68016/0.71221. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67902/0.71214. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67728/0.71247. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67817/0.71277. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67822/0.71257. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67779/0.71292. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67807/0.71277. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67563/0.71295. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67524/0.71337. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67508/0.71331. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67562/0.71338. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67534/0.71315. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67531/0.71360. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67550/0.71292. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67477/0.71267. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67414/0.71265. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67164/0.71321. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67192/0.71327. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67160/0.71348. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67034/0.71265. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66976/0.71285. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67173/0.71308. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66978/0.71371. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67030/0.71366. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66735/0.71399. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66532/0.71406. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66966/0.71336. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66805/0.71309. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66953/0.71333. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66755/0.71292. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66762/0.71232. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66713/0.71231. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66346/0.71267. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66650/0.71239. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66529/0.71302. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66296/0.71174. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66062/0.71221. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66230/0.71221. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66246/0.71278. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65904/0.71252. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66093/0.71211. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65896/0.71291. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65894/0.71397. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65841/0.71409. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65792/0.71379. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65955/0.71290. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65594/0.71338. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65651/0.71328. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69258/0.69032. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69176/0.69081. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69076/0.69113. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69074/0.69162. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68938/0.69202. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68946/0.69233. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69020/0.69268. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68916/0.69303. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68909/0.69330. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68884/0.69350. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68900/0.69365. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68838/0.69373. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68788/0.69367. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68831/0.69379. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68827/0.69377. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68765/0.69368. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68785/0.69361. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68705/0.69356. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68703/0.69337. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68653/0.69336. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68710/0.69332. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68650/0.69325. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68692/0.69331. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68616/0.69323. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68631/0.69321. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68484/0.69328. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68612/0.69309. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68485/0.69311. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68550/0.69280. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68334/0.69250. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68508/0.69227. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68443/0.69231. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68364/0.69235. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68401/0.69196. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68371/0.69135. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68309/0.69091. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68362/0.69069. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68104/0.69076. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68268/0.69061. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68092/0.69005. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68146/0.69010. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68084/0.69004. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.68052/0.68955. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67860/0.68963. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67885/0.68957. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67847/0.68983. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67808/0.68958. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67695/0.68977. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67636/0.68982. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67629/0.68889. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67524/0.68862. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67517/0.68870. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67297/0.68943. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67164/0.68996. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67203/0.69062. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67007/0.69100. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66824/0.69153. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66630/0.69117. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66689/0.69074. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66819/0.69183. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66506/0.69204. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66604/0.69189. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66301/0.69302. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66187/0.69366. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 0.66083/0.69355. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66056/0.69511. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65947/0.69682. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65940/0.69643. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65825/0.69745. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65394/0.69657. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65671/0.69757. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65666/0.69839. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65474/0.69843. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65141/0.69931. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65057/0.70030. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65025/0.70212. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64717/0.70110. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65076/0.70191. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64490/0.70551. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64796/0.70508. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64414/0.70317. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64306/0.70572. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64363/0.70703. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63908/0.70650. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64201/0.71157. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64090/0.70846. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.63557/0.70994. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63822/0.71530. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63679/0.71483. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63367/0.71419. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63657/0.71529. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63344/0.71492. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63228/0.71581. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63342/0.71442. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62993/0.71582. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62976/0.72011. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.62443/0.72146. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62724/0.72183. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.62793/0.72309. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62619/0.72313. Took 0.20 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69246/0.69017. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69040/0.69041. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69014/0.69146. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69031/0.69220. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68906/0.69303. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68838/0.69399. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68808/0.69485. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68756/0.69571. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68699/0.69655. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68756/0.69724. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68730/0.69807. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68675/0.69838. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68511/0.69936. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68612/0.69945. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68550/0.69951. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68595/0.69969. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68577/0.70004. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68563/0.70023. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68402/0.70097. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68548/0.70122. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68518/0.70114. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68490/0.70101. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68386/0.70137. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68259/0.70177. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68292/0.70202. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68323/0.70254. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68198/0.70301. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68200/0.70253. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68103/0.70244. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68263/0.70320. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68123/0.70268. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68095/0.70295. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68019/0.70286. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68081/0.70311. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67995/0.70291. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67850/0.70275. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67886/0.70278. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67839/0.70386. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67756/0.70369. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67709/0.70394. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67675/0.70483. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67610/0.70513. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67812/0.70456. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67533/0.70518. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67451/0.70500. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67384/0.70533. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67442/0.70504. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67267/0.70638. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67147/0.70779. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67093/0.70803. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67333/0.70874. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67184/0.70927. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67114/0.71040. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67080/0.71083. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66864/0.71077. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66824/0.71086. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66883/0.71110. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66789/0.71171. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66802/0.71345. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66709/0.71471. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66572/0.71559. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66746/0.71439. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66485/0.71581. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66316/0.71676. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66496/0.71718. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66294/0.71759. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66219/0.71863. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66078/0.71898. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66288/0.71970. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65926/0.72068. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66018/0.72087. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65928/0.72142. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65856/0.72175. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65958/0.72241. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65829/0.72287. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65880/0.72429. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65502/0.72304. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65653/0.72414. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65305/0.72665. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65443/0.72758. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65378/0.72832. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65454/0.73017. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65290/0.72903. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64893/0.73185. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64976/0.73163. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64896/0.73218. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65193/0.73314. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65205/0.73451. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65081/0.73470. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64750/0.73531. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64507/0.73496. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64121/0.73701. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64528/0.73952. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64581/0.74256. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64582/0.74290. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64354/0.74289. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64332/0.74409. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64232/0.74383. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64101/0.74628. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64293/0.74591. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69290/0.69114. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69236/0.69071. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69024/0.69048. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69031/0.69044. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69099/0.69048. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68950/0.69051. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68939/0.69049. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68909/0.69059. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68898/0.69076. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68865/0.69090. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68819/0.69108. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68744/0.69133. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68706/0.69178. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68634/0.69216. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68581/0.69265. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68643/0.69311. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68565/0.69377. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68399/0.69451. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68373/0.69518. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68303/0.69593. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68236/0.69690. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68097/0.69729. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68140/0.69822. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68038/0.69872. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68047/0.69959. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68214/0.70018. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67999/0.70064. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67934/0.70137. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67829/0.70237. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67718/0.70342. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67642/0.70417. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67618/0.70514. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67423/0.70610. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67344/0.70669. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67413/0.70717. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67469/0.70730. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67170/0.70777. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67049/0.70851. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67074/0.70968. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.66970/0.70937. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66832/0.71009. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66858/0.71164. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66632/0.71209. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66463/0.71173. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66293/0.71208. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66409/0.71203. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66184/0.71266. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66292/0.71268. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65954/0.71258. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.65662/0.71358. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65339/0.71423. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65565/0.71426. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65021/0.71284. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64904/0.71363. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64791/0.71278. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64701/0.71434. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64663/0.71445. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64200/0.71449. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64201/0.71405. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.64102/0.71628. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.63647/0.71755. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.63714/0.71694. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63582/0.71732. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63458/0.71750. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63473/0.71764. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.62938/0.71701. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.62861/0.71824. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.62640/0.71882. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.62945/0.71854. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.62354/0.72094. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.62367/0.72137. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62304/0.72052. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62142/0.72285. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.61808/0.72558. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.61848/0.72518. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.61674/0.72576. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.61845/0.72478. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.61337/0.72472. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.61485/0.72736. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.60967/0.72760. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.60859/0.72711. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.60744/0.72798. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.60424/0.72995. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.60617/0.72758. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.60108/0.72999. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.60067/0.73192. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.60437/0.72971. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.59764/0.73391. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.59549/0.73577. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.59523/0.73784. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.59518/0.73559. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.59284/0.73556. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.59548/0.73342. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.59597/0.73691. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.58857/0.73680. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.59100/0.74169. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.58588/0.74133. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.58752/0.74391. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.58427/0.74646. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.57736/0.74630. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69302/0.69309. Took 0.38 sec\n",
      "Epoch 1, Loss(train/val) 0.69301/0.69278. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69314/0.69261. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69238/0.69243. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69210/0.69221. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69185/0.69204. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69211/0.69177. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69128/0.69155. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69040/0.69129. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69100/0.69105. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69137/0.69076. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69048/0.69045. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69053/0.69025. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69086/0.68992. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68904/0.68957. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68923/0.68915. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68907/0.68871. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68737/0.68815. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68856/0.68821. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68840/0.68792. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68750/0.68756. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68771/0.68737. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68718/0.68709. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68738/0.68678. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68647/0.68670. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68503/0.68628. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68486/0.68574. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68606/0.68515. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68636/0.68468. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68427/0.68454. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68358/0.68413. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68396/0.68363. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68462/0.68320. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68434/0.68298. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68431/0.68270. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68293/0.68231. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68339/0.68198. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68248/0.68144. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68303/0.68133. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68109/0.68070. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68134/0.68033. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68057/0.67980. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68032/0.67948. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67960/0.67941. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68070/0.67876. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67762/0.67865. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67981/0.67827. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67856/0.67822. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 0.67897/0.67810. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67634/0.67783. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67641/0.67729. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67868/0.67729. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67472/0.67721. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67619/0.67721. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67545/0.67680. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67444/0.67666. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67230/0.67629. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67371/0.67571. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67111/0.67527. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67285/0.67458. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67115/0.67445. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67049/0.67415. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66971/0.67461. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67111/0.67418. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66764/0.67419. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66727/0.67414. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66593/0.67365. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66513/0.67331. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66589/0.67314. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66570/0.67335. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66432/0.67338. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66241/0.67350. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66211/0.67429. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65977/0.67420. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65816/0.67461. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65993/0.67413. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66070/0.67446. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65819/0.67445. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65815/0.67494. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65690/0.67525. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65562/0.67532. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65146/0.67590. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65534/0.67607. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65112/0.67524. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64971/0.67519. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65202/0.67687. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64896/0.67629. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64628/0.67745. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64509/0.67669. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64568/0.67799. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64410/0.67840. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64321/0.67844. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64250/0.67906. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64369/0.67876. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.63520/0.67903. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64223/0.67961. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63621/0.68100. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63428/0.68101. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63433/0.68122. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63894/0.68458. Took 0.20 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69601/0.69502. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69297/0.69462. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69330/0.69371. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69277/0.69325. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69229/0.69300. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69221/0.69284. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.69163/0.69272. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69142/0.69228. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.69137/0.69216. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69034/0.69210. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69002/0.69221. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68939/0.69223. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68865/0.69194. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68858/0.69165. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68887/0.69164. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68861/0.69143. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68847/0.69103. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68781/0.69130. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68744/0.69148. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68755/0.69179. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68711/0.69156. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68707/0.69176. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68571/0.69156. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68613/0.69140. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68535/0.69110. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68444/0.69113. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68498/0.69141. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68432/0.69147. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68377/0.69180. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68228/0.69193. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68305/0.69195. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68163/0.69188. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68210/0.69207. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67957/0.69287. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67976/0.69284. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68000/0.69297. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68125/0.69372. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68055/0.69386. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67957/0.69390. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67845/0.69397. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67626/0.69437. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67829/0.69503. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67942/0.69557. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67925/0.69508. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67520/0.69575. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67581/0.69547. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67664/0.69581. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67432/0.69678. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67447/0.69773. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67339/0.69837. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67180/0.70007. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67342/0.70044. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67339/0.70027. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67268/0.70128. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66900/0.70275. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67126/0.70325. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67184/0.70347. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66772/0.70343. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66648/0.70425. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66765/0.70450. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66621/0.70502. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66773/0.70668. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66510/0.70669. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66651/0.70741. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66372/0.70873. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66269/0.70967. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66447/0.70966. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66493/0.71020. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66166/0.71249. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66120/0.71246. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66060/0.71317. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65567/0.71521. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65555/0.71791. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65686/0.71963. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65434/0.72023. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65663/0.72095. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65644/0.72149. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65166/0.72355. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65222/0.72570. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64888/0.72777. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65047/0.72845. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64950/0.72792. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64324/0.72986. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64468/0.73266. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64530/0.73162. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64354/0.73220. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64756/0.73442. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64563/0.73384. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64287/0.73412. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64041/0.73414. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63909/0.73536. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63944/0.73545. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63732/0.74119. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63703/0.74398. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63541/0.74319. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63528/0.74458. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63398/0.74305. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63708/0.74323. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63349/0.74232. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63478/0.74612. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69447/0.69440. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69310/0.69354. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69191/0.69372. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69206/0.69419. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69186/0.69466. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69095/0.69530. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69146/0.69590. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69081/0.69636. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68977/0.69692. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68944/0.69755. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68979/0.69824. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68885/0.69874. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68867/0.69942. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68817/0.70008. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68814/0.70068. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68786/0.70120. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68734/0.70189. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68755/0.70265. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68745/0.70348. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68739/0.70387. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68589/0.70437. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68612/0.70496. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68565/0.70573. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68458/0.70696. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68513/0.70770. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68495/0.70828. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68457/0.70915. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68431/0.70987. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68327/0.71103. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68215/0.71198. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68231/0.71253. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68085/0.71360. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68186/0.71471. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68127/0.71542. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68084/0.71619. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67949/0.71740. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67972/0.71754. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67921/0.71813. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67838/0.71964. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67782/0.72043. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67708/0.72236. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67693/0.72202. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67643/0.72254. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67664/0.72308. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67605/0.72380. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67472/0.72484. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67442/0.72405. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67484/0.72715. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67405/0.72761. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67348/0.72815. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67291/0.72922. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67169/0.72961. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67127/0.73040. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67069/0.73235. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66978/0.73454. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66998/0.73492. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67002/0.73621. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66823/0.73579. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66779/0.73834. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67021/0.73719. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66782/0.73796. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66819/0.73857. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66773/0.73954. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66874/0.74100. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66676/0.73986. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66512/0.74278. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66745/0.74340. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66493/0.74507. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66396/0.74562. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66240/0.74569. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66095/0.74821. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66415/0.74540. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66134/0.74787. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66107/0.75054. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65805/0.75016. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65884/0.75514. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65895/0.75376. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65985/0.75253. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65883/0.75482. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65851/0.75699. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65528/0.75659. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65645/0.75841. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65553/0.75668. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65262/0.75698. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65283/0.75940. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65392/0.75829. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65232/0.75959. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64810/0.76032. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65174/0.75812. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65066/0.76076. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64887/0.76234. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65180/0.76326. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64737/0.76432. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64576/0.76305. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64858/0.76492. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64549/0.76554. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64469/0.76646. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64563/0.76959. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64293/0.76986. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64167/0.77172. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69265/0.69348. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69124/0.69312. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69105/0.69301. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69170/0.69293. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69050/0.69289. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69012/0.69277. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69032/0.69283. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68982/0.69287. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68986/0.69288. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68906/0.69297. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68957/0.69302. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68978/0.69301. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68925/0.69294. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68817/0.69287. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68787/0.69290. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68888/0.69285. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68844/0.69270. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68800/0.69258. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68788/0.69254. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68709/0.69255. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68746/0.69252. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68748/0.69240. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68702/0.69238. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68748/0.69236. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68677/0.69238. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68768/0.69237. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68703/0.69238. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68585/0.69229. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68596/0.69216. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68464/0.69216. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68515/0.69214. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68508/0.69194. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68573/0.69163. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68537/0.69191. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68460/0.69215. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68401/0.69211. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68453/0.69211. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68467/0.69228. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68438/0.69242. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68446/0.69243. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68526/0.69244. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68432/0.69230. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68308/0.69225. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68360/0.69219. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68312/0.69242. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68274/0.69214. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68171/0.69234. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68221/0.69248. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.68108/0.69232. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68104/0.69240. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68130/0.69248. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68047/0.69249. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68096/0.69269. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.68109/0.69297. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68095/0.69311. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.68074/0.69292. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68053/0.69289. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.68162/0.69302. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67796/0.69301. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67838/0.69308. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67843/0.69305. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67873/0.69314. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67830/0.69351. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67737/0.69309. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67807/0.69310. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67780/0.69339. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67775/0.69314. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67866/0.69286. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67620/0.69304. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67517/0.69253. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67636/0.69275. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67526/0.69313. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67558/0.69326. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67537/0.69354. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67497/0.69329. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67376/0.69303. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67227/0.69276. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67249/0.69285. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67220/0.69288. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67349/0.69270. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67222/0.69265. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67153/0.69173. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.67126/0.69224. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67130/0.69302. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.67176/0.69302. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.67377/0.69248. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.67023/0.69171. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.67050/0.69251. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.67059/0.69152. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.66871/0.69221. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66925/0.69126. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66840/0.69104. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66898/0.69251. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66499/0.69236. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.66664/0.69210. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66962/0.69232. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66433/0.69182. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66682/0.69257. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66450/0.69218. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66636/0.69188. Took 0.18 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69120/0.69855. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69038/0.69831. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68990/0.69890. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68909/0.69912. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68894/0.69963. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68830/0.69990. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68826/0.70017. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68849/0.70065. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68789/0.70089. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68775/0.70139. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68636/0.70160. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68672/0.70204. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68736/0.70203. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68618/0.70257. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.68612/0.70280. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68632/0.70314. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.68606/0.70324. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68554/0.70319. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68493/0.70356. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68475/0.70394. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.68404/0.70428. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68385/0.70439. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68470/0.70469. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68278/0.70378. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68263/0.70403. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68191/0.70428. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68139/0.70393. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68096/0.70394. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68021/0.70405. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68040/0.70392. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68033/0.70301. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67888/0.70363. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.67845/0.70320. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67870/0.70345. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67804/0.70336. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67784/0.70425. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67792/0.70376. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67565/0.70335. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67661/0.70295. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67529/0.70390. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67528/0.70395. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67210/0.70351. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67509/0.70261. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67185/0.70280. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66956/0.70264. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67027/0.70216. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67176/0.70283. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66899/0.70334. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66739/0.70365. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66840/0.70362. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66950/0.70409. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66821/0.70332. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66571/0.70241. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66447/0.70265. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66248/0.70069. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66010/0.70063. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66092/0.70257. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66107/0.70068. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65982/0.70174. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65883/0.69995. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65859/0.69815. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.65687/0.69987. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65558/0.69949. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65260/0.69910. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65426/0.69967. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65307/0.69871. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65084/0.69626. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65234/0.69784. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64810/0.69745. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64509/0.69761. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64432/0.69675. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64221/0.69721. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64163/0.69558. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64307/0.69586. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64294/0.69806. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64085/0.69530. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63836/0.69776. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63741/0.69846. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63770/0.69847. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63575/0.69796. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63401/0.69905. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63334/0.69996. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63236/0.69671. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63361/0.69830. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63443/0.69924. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62954/0.69933. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63096/0.69909. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63050/0.69523. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63000/0.69738. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.62943/0.70099. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62768/0.70568. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62477/0.70502. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62138/0.69949. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62098/0.69915. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.61976/0.70078. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62243/0.70076. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62343/0.70167. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61792/0.70386. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61835/0.69997. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61698/0.70317. Took 0.18 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69491/0.69352. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69305/0.69328. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69288/0.69313. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69202/0.69296. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69176/0.69308. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69152/0.69322. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69108/0.69340. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69056/0.69350. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69058/0.69360. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69075/0.69380. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69012/0.69404. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69011/0.69418. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68918/0.69449. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68909/0.69471. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68829/0.69506. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68824/0.69545. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68819/0.69584. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68839/0.69600. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68768/0.69629. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68807/0.69655. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68707/0.69719. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68669/0.69766. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68711/0.69777. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68618/0.69795. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68636/0.69813. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68566/0.69823. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68515/0.69863. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68455/0.69873. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68456/0.69854. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68485/0.69880. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68422/0.69867. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68335/0.69830. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68393/0.69796. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68221/0.69769. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68230/0.69794. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68116/0.69738. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68061/0.69779. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68101/0.69801. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68037/0.69799. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68037/0.69734. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67716/0.69615. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67869/0.69619. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67649/0.69499. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67678/0.69489. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67793/0.69517. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67624/0.69440. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67562/0.69386. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67533/0.69393. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67540/0.69338. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67395/0.69244. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67240/0.69214. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67434/0.69312. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67496/0.69309. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67364/0.69233. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67026/0.69244. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67009/0.69119. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66815/0.69104. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66949/0.69044. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66902/0.69193. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66779/0.69172. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66810/0.69136. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66507/0.69025. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66672/0.68982. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 0.66942/0.69095. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66620/0.69034. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66599/0.69063. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66477/0.68913. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66626/0.69023. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66243/0.69068. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66349/0.69042. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66176/0.68977. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66273/0.68986. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65857/0.69066. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66194/0.69066. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65921/0.69152. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66008/0.68968. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66108/0.69022. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65954/0.69039. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65910/0.69068. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65769/0.69231. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65658/0.69174. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65601/0.69027. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65557/0.69144. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65414/0.69119. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65650/0.69194. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65556/0.69216. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65467/0.69198. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64953/0.69232. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65094/0.69159. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65169/0.69086. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65272/0.69172. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64894/0.69174. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64664/0.69290. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65262/0.69257. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65006/0.69413. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64715/0.69308. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64564/0.69324. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64581/0.69323. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64728/0.69237. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64381/0.69254. Took 0.18 sec\n",
      "ACC: 0.375\n",
      "Epoch 0, Loss(train/val) 0.69377/0.69036. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69248/0.69083. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69141/0.69121. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69136/0.69166. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69095/0.69217. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68983/0.69272. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68977/0.69327. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68953/0.69387. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68893/0.69453. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68784/0.69529. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68717/0.69617. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68669/0.69706. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68658/0.69808. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68602/0.69919. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68614/0.70020. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68603/0.70105. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68579/0.70205. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68483/0.70297. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68446/0.70407. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68478/0.70496. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68423/0.70561. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68421/0.70644. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68258/0.70760. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68212/0.70875. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68203/0.70983. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68132/0.71065. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68145/0.71162. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68135/0.71279. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68046/0.71409. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67988/0.71526. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67927/0.71635. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67690/0.71803. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67758/0.71953. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67439/0.72175. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67532/0.72334. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67447/0.72409. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67414/0.72613. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67330/0.72765. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67225/0.72906. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67090/0.73110. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67207/0.73214. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.66890/0.73456. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66699/0.73723. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66799/0.73935. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66575/0.74109. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66545/0.74170. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66441/0.74350. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66337/0.74573. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66255/0.74712. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66197/0.74573. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66064/0.74800. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65667/0.75025. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65910/0.75141. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65528/0.75271. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65338/0.75331. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65244/0.75525. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65382/0.75333. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64948/0.75535. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64938/0.75655. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65100/0.75666. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64898/0.75434. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64366/0.75641. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63741/0.75753. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64213/0.75528. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64080/0.75307. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63682/0.75387. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63682/0.75470. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63534/0.75403. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63771/0.75586. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63313/0.75434. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63109/0.75583. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63208/0.75495. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63177/0.75494. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62546/0.75950. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62749/0.75895. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.62299/0.75705. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62063/0.76022. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.61787/0.76110. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.61720/0.76349. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61145/0.76569. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61746/0.76504. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61442/0.76511. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61095/0.76374. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.60978/0.76881. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.60874/0.77110. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.60794/0.77206. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.60363/0.77555. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.60342/0.77359. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60252/0.77565. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.60405/0.77869. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60332/0.77660. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.60065/0.77715. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.59840/0.77781. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.59766/0.78290. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.59352/0.77933. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.59250/0.78644. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.58385/0.78725. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.58826/0.79020. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59526/0.78906. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59051/0.79105. Took 0.21 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69326/0.69220. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69152/0.69244. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69046/0.69254. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69060/0.69266. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69090/0.69285. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68924/0.69291. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68968/0.69316. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68897/0.69348. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68946/0.69389. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68931/0.69431. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68847/0.69494. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68829/0.69559. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68773/0.69623. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68733/0.69700. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68675/0.69759. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68706/0.69806. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68728/0.69869. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68652/0.69943. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68628/0.69980. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68593/0.70034. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68585/0.70128. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68408/0.70190. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68552/0.70238. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68454/0.70255. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68444/0.70309. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68461/0.70344. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68458/0.70381. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68428/0.70401. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68346/0.70455. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68398/0.70483. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68271/0.70546. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68097/0.70613. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68300/0.70673. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68170/0.70756. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68185/0.70748. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68155/0.70775. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67995/0.70801. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68068/0.70793. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68063/0.70795. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67944/0.70816. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68016/0.70837. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67719/0.70846. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67900/0.70837. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67929/0.70887. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67737/0.70900. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67677/0.70921. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67628/0.70943. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67759/0.70995. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67705/0.70941. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67671/0.70811. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67397/0.70752. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67482/0.70716. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67607/0.70780. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67282/0.70789. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67273/0.70805. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67314/0.70783. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67290/0.70720. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66960/0.70640. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67044/0.70644. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66943/0.70661. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66767/0.70572. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66782/0.70496. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66515/0.70562. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66477/0.70590. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66455/0.70581. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66196/0.70605. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66319/0.70541. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66178/0.70595. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65980/0.70668. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66061/0.70437. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65698/0.70376. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65993/0.70479. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65725/0.70520. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65501/0.70466. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65369/0.70423. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65425/0.70551. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65384/0.70544. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64980/0.70485. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64850/0.70482. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64701/0.70700. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64681/0.70725. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64343/0.70849. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64318/0.70861. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64347/0.70782. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64005/0.70932. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63841/0.71082. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63549/0.71346. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63499/0.71337. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63615/0.71539. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63334/0.71643. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63454/0.71725. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62924/0.71917. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62936/0.72008. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62412/0.71988. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62773/0.72211. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62307/0.72197. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62511/0.72613. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62208/0.72885. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62071/0.72681. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61766/0.72668. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69345/0.69495. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69256/0.69498. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69197/0.69506. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69158/0.69513. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69093/0.69520. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69071/0.69516. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69066/0.69512. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69039/0.69531. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68992/0.69572. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68945/0.69576. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68967/0.69595. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68888/0.69605. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68833/0.69623. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68833/0.69651. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68736/0.69691. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68769/0.69710. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68772/0.69719. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68619/0.69723. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68632/0.69740. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68649/0.69749. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68455/0.69791. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68457/0.69822. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68340/0.69850. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68227/0.69833. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68227/0.69778. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68067/0.69789. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68020/0.69832. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68043/0.69833. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67842/0.69861. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67721/0.69910. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67546/0.69947. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67563/0.70060. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67544/0.70054. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67376/0.70193. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67385/0.70230. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67150/0.70231. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67203/0.70291. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66882/0.70234. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66768/0.70310. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66705/0.70282. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66514/0.70327. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66384/0.70320. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66245/0.70383. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66084/0.70463. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65869/0.70460. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65959/0.70438. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65700/0.70574. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65796/0.70671. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65795/0.70734. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65568/0.70709. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65349/0.70672. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.65099/0.70605. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.64774/0.70564. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65117/0.70500. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64993/0.70469. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64819/0.70566. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64647/0.70507. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64511/0.70419. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64444/0.70567. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64088/0.70646. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64351/0.70532. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.63896/0.70459. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64109/0.70486. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63967/0.70712. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63779/0.70760. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63542/0.70874. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63929/0.70863. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63701/0.70600. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63493/0.70723. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.63160/0.70615. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63250/0.70746. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62989/0.70782. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62570/0.70745. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62696/0.70646. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62943/0.70564. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.62699/0.70309. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62243/0.70740. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.62295/0.70716. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62389/0.70660. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61882/0.70665. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62419/0.70530. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61914/0.70675. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62133/0.70731. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61969/0.70716. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61598/0.70831. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61360/0.71142. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61592/0.71283. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61487/0.71040. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61403/0.71211. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61024/0.71344. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61067/0.71668. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.60822/0.71757. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61099/0.71558. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60583/0.71302. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.60162/0.71478. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60565/0.71523. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60070/0.71488. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.60664/0.71575. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60245/0.71555. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60242/0.72061. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69404/0.69082. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69108/0.69024. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69046/0.69039. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69082/0.69068. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68880/0.69109. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68841/0.69178. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68835/0.69266. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68765/0.69355. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68806/0.69447. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68657/0.69547. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68523/0.69655. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68518/0.69769. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68443/0.69910. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68554/0.70051. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68249/0.70191. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68307/0.70320. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68379/0.70440. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68308/0.70566. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68260/0.70704. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68223/0.70884. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68167/0.70975. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68146/0.71069. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67970/0.71137. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68002/0.71251. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68034/0.71392. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67825/0.71547. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67895/0.71646. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67815/0.71759. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67707/0.71867. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67756/0.72029. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67609/0.72157. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67604/0.72211. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67554/0.72391. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67641/0.72425. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67380/0.72467. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67547/0.72510. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67271/0.72599. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67248/0.72696. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67201/0.72807. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67122/0.72847. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67175/0.72918. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67139/0.72896. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67284/0.73011. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66824/0.73101. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66846/0.73200. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66878/0.73155. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66820/0.73317. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66853/0.73362. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66623/0.73466. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66704/0.73490. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66518/0.73595. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66527/0.73607. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66572/0.73683. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66520/0.73808. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66449/0.73877. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66263/0.73789. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66301/0.73927. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66116/0.73973. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66025/0.74108. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66183/0.74029. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66084/0.73991. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65950/0.74159. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65960/0.74320. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65754/0.74262. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65614/0.74454. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65660/0.74435. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65622/0.74568. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65612/0.74609. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65455/0.74447. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65520/0.74810. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65440/0.74693. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65306/0.74719. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65305/0.74798. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65134/0.74697. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65020/0.74861. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64883/0.75147. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64856/0.75005. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64632/0.75215. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64499/0.75289. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64836/0.75360. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64510/0.75281. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64300/0.75788. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64465/0.75694. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64318/0.75839. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.63991/0.75961. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64212/0.75747. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64324/0.75453. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63847/0.75544. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63793/0.75990. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63951/0.75950. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63313/0.76134. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63481/0.76282. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63661/0.76009. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63281/0.76311. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.63228/0.76253. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63532/0.76053. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63062/0.76342. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63344/0.75975. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62837/0.76505. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62854/0.76472. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69459/0.69161. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69403/0.69199. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69303/0.69233. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.69351/0.69260. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69325/0.69294. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69207/0.69316. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69221/0.69337. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69249/0.69346. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69155/0.69368. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69188/0.69400. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69188/0.69434. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69138/0.69466. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69112/0.69494. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69088/0.69544. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69001/0.69594. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69035/0.69665. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68941/0.69769. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68948/0.69836. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68813/0.69921. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68753/0.70047. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68797/0.70151. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68638/0.70285. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68715/0.70388. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68604/0.70546. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68541/0.70690. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68573/0.70840. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68524/0.70953. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68429/0.71103. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68382/0.71202. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68328/0.71358. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68214/0.71465. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68298/0.71558. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68016/0.71710. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68125/0.71810. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68057/0.71887. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68150/0.71901. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68073/0.72010. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67852/0.71994. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67763/0.72147. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67841/0.72149. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67752/0.72218. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67588/0.72288. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67572/0.72400. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67497/0.72414. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67325/0.72421. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67368/0.72464. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67371/0.72502. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67140/0.72559. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66986/0.72602. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67165/0.72431. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66885/0.72560. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66842/0.72657. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67042/0.72743. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66839/0.72764. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66738/0.72784. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66402/0.72773. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66445/0.72993. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66340/0.72988. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66217/0.73059. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66483/0.73177. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66069/0.72993. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65930/0.73373. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65670/0.73318. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65782/0.73498. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65572/0.73476. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65405/0.73750. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65348/0.73669. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65085/0.73849. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65116/0.73957. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64942/0.74255. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64836/0.74034. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64705/0.74138. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64332/0.74367. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64360/0.74551. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64172/0.74767. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64099/0.74527. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64321/0.74848. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63593/0.74860. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63844/0.74898. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63536/0.74871. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63334/0.74962. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63484/0.75072. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63542/0.75410. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63215/0.75471. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63086/0.75530. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62792/0.75950. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63046/0.75835. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62589/0.75793. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62712/0.76074. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62768/0.76075. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62056/0.76143. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61811/0.76183. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62191/0.76495. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61670/0.76723. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62358/0.76627. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.61888/0.76581. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61689/0.76635. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61737/0.76804. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60991/0.76757. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.61223/0.76659. Took 0.18 sec\n",
      "ACC: 0.3854166666666667\n",
      "Epoch 0, Loss(train/val) 0.69605/0.70610. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69400/0.70167. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69213/0.70092. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69142/0.70083. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69160/0.70095. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69108/0.70056. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69075/0.70039. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68996/0.70075. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68947/0.70068. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68944/0.70161. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68902/0.70243. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68866/0.70238. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68843/0.70329. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68790/0.70360. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68748/0.70412. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68656/0.70483. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68722/0.70550. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68699/0.70600. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68642/0.70717. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68629/0.70755. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68547/0.70917. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68486/0.70999. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68520/0.71103. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68463/0.71110. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68438/0.71184. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68374/0.71308. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68286/0.71408. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68287/0.71550. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68311/0.71602. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68177/0.71625. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68195/0.71718. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68215/0.71791. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68227/0.71995. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68104/0.71938. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68113/0.72089. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68048/0.72111. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68123/0.72175. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67988/0.72205. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67984/0.72302. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67953/0.72371. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68077/0.72313. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67959/0.72325. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67803/0.72483. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67776/0.72497. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67826/0.72527. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67706/0.72639. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67810/0.72491. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67884/0.72552. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67789/0.72673. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67749/0.72699. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67689/0.72749. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67712/0.72838. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67606/0.72714. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67521/0.72805. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67693/0.72946. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67530/0.72927. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67477/0.72997. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67614/0.72962. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67455/0.72964. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67474/0.73185. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67347/0.73135. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67524/0.73186. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67520/0.73188. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67371/0.73183. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67188/0.73318. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67444/0.73438. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67356/0.73344. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67273/0.73381. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67238/0.73495. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67133/0.73419. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67389/0.73493. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67422/0.73520. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67172/0.73468. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67274/0.73607. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67063/0.73509. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66993/0.73524. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67164/0.73490. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67088/0.73469. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67159/0.73416. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67003/0.73524. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66933/0.73461. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66875/0.73551. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66993/0.73570. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66865/0.73563. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66793/0.73615. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66995/0.73962. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66786/0.73646. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66870/0.73722. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66666/0.73854. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66835/0.73734. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66807/0.73801. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66604/0.73752. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66808/0.73809. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66557/0.73792. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66522/0.73881. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66545/0.73757. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66561/0.73897. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66460/0.73903. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66550/0.73853. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.66530/0.73786. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69359/0.69376. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69181/0.69194. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69140/0.69141. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69100/0.69134. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69020/0.69125. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69064/0.69118. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68984/0.69113. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68923/0.69112. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68833/0.69095. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68880/0.69085. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68834/0.69077. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68779/0.69072. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68778/0.69088. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68753/0.69094. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68632/0.69104. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68567/0.69101. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68561/0.69145. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68545/0.69137. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68385/0.69137. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68422/0.69149. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68364/0.69146. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68267/0.69173. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68202/0.69181. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68130/0.69168. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68053/0.69190. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67957/0.69207. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67777/0.69217. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67881/0.69279. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67644/0.69319. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67472/0.69355. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67402/0.69335. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67208/0.69451. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66930/0.69390. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.66770/0.69440. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66691/0.69333. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66384/0.69550. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66190/0.69546. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.65870/0.69546. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.65607/0.69692. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.65449/0.69843. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.65527/0.69844. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.65342/0.69894. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.65169/0.70104. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.64977/0.70224. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.64809/0.70250. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.64590/0.70384. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.64797/0.70436. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.64572/0.70582. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.64404/0.70498. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.64361/0.70552. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.64189/0.70506. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.64197/0.70709. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.63744/0.70918. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64031/0.70925. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.63986/0.71022. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.63494/0.71092. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.63210/0.71260. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.63369/0.71152. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.63573/0.71121. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.63317/0.71275. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.63264/0.71377. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.62919/0.71487. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.62844/0.71564. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63216/0.71690. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63060/0.71500. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63089/0.71548. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.62599/0.71467. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.62807/0.71629. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.62750/0.71614. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.62741/0.71580. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.62497/0.71885. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62139/0.71897. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.61947/0.71930. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.62532/0.71860. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62119/0.72092. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.62008/0.72016. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62021/0.72069. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.61985/0.71883. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.61759/0.72053. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61900/0.71766. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61286/0.72035. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61433/0.72175. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61712/0.72271. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61449/0.72442. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61190/0.72564. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61051/0.72281. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.60783/0.72721. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.60982/0.72341. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61056/0.72768. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60860/0.72452. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60308/0.72774. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.60626/0.72915. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60841/0.72600. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60240/0.73045. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60274/0.72854. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.59871/0.72770. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60236/0.72958. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.59856/0.72694. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60492/0.72909. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59340/0.72959. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69351/0.69023. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69249/0.68861. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69207/0.68789. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69168/0.68753. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69173/0.68741. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69134/0.68739. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69139/0.68742. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69088/0.68721. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.69087/0.68717. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69003/0.68725. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69053/0.68739. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69030/0.68750. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68979/0.68778. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68947/0.68784. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68902/0.68797. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68859/0.68818. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68884/0.68845. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68813/0.68877. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68813/0.68905. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68812/0.68916. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68713/0.68952. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68694/0.68989. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68665/0.69016. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68706/0.69046. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68595/0.69096. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68597/0.69139. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68555/0.69194. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68383/0.69241. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68614/0.69279. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68499/0.69314. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68367/0.69359. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68350/0.69396. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68403/0.69459. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68245/0.69474. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68322/0.69489. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68349/0.69532. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68134/0.69611. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68199/0.69644. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68185/0.69708. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68138/0.69755. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68090/0.69819. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68114/0.69849. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68111/0.69892. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67969/0.69870. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68005/0.69940. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67944/0.69999. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67907/0.69988. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67793/0.70067. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67942/0.70152. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67684/0.70177. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 0.67828/0.70220. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67749/0.70246. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67705/0.70315. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67671/0.70382. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67663/0.70415. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67642/0.70458. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67650/0.70441. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67453/0.70505. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67453/0.70609. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67533/0.70589. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67475/0.70650. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67331/0.70788. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67392/0.70788. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67206/0.70942. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67199/0.70947. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67261/0.70985. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.67166/0.71154. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67078/0.71234. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 0.67137/0.71165. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67109/0.71224. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67004/0.71286. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66908/0.71448. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66926/0.71440. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66899/0.71449. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66732/0.71517. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66774/0.71536. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66762/0.71635. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66784/0.71811. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66683/0.71878. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66499/0.71870. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66846/0.71967. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66706/0.71914. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66631/0.71975. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66494/0.72153. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66623/0.72259. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66370/0.72371. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66296/0.72290. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66192/0.72394. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66263/0.72599. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66261/0.72764. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66217/0.72684. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66217/0.72643. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66170/0.72825. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65902/0.72944. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65981/0.73004. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66082/0.73123. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66062/0.73235. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65981/0.73324. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65954/0.73223. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65829/0.73496. Took 0.20 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69188/0.69607. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69105/0.69646. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69030/0.69698. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69019/0.69739. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68992/0.69792. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68886/0.69861. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68866/0.69930. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68817/0.70005. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68817/0.70082. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68718/0.70194. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68724/0.70302. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68700/0.70396. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68624/0.70495. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68554/0.70580. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68551/0.70694. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68516/0.70773. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68466/0.70836. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68450/0.70941. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68461/0.71026. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68307/0.71096. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68253/0.71172. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68133/0.71248. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68175/0.71256. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67941/0.71295. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68015/0.71369. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67934/0.71391. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67853/0.71395. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67882/0.71399. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67735/0.71440. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67574/0.71512. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67667/0.71544. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67610/0.71546. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67505/0.71633. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67418/0.71686. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67200/0.71752. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67352/0.71780. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67270/0.71802. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67077/0.71771. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67115/0.71801. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67133/0.71773. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67032/0.71749. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66912/0.71824. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66815/0.71911. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66782/0.72037. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66639/0.72003. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66537/0.71953. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66783/0.71915. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66831/0.71992. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66292/0.72153. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66736/0.72124. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66407/0.72237. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66499/0.72279. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66374/0.72278. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66382/0.72249. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66234/0.72267. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66139/0.72316. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66243/0.72343. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66065/0.72420. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65929/0.72555. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65965/0.72593. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65801/0.72491. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65965/0.72547. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65666/0.72580. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65867/0.72575. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65733/0.72623. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65512/0.72774. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65608/0.72759. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65453/0.72867. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65297/0.73091. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65351/0.73101. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65166/0.73204. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65089/0.73158. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64981/0.73148. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65206/0.73343. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64964/0.73607. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64958/0.73549. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64859/0.73472. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64838/0.73730. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64690/0.73545. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64714/0.73920. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64876/0.73892. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64281/0.74054. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64984/0.74131. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64476/0.74144. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64458/0.74422. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64106/0.74411. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64522/0.74229. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64095/0.74537. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64130/0.74693. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64223/0.74418. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64078/0.74349. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64122/0.74644. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64086/0.74768. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63404/0.74477. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63932/0.75037. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63596/0.74793. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63896/0.74521. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63918/0.75080. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63918/0.75315. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63603/0.75455. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69864/0.69545. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69214/0.68837. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69074/0.68593. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69022/0.68526. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68999/0.68510. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68964/0.68530. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68994/0.68563. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68940/0.68583. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68901/0.68608. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68816/0.68662. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68834/0.68719. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68779/0.68735. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68788/0.68800. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68685/0.68843. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68825/0.68890. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68761/0.68927. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68644/0.68949. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68630/0.68954. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68522/0.69000. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68430/0.69019. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68449/0.69060. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68375/0.69090. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68368/0.69075. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68298/0.69134. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68412/0.69125. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68348/0.69093. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68137/0.69206. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68086/0.69174. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68098/0.69188. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68098/0.69230. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67948/0.69286. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68004/0.69233. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68012/0.69358. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67818/0.69490. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67714/0.69472. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67687/0.69518. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67574/0.69510. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67510/0.69457. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67623/0.69546. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67407/0.69543. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67403/0.69524. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67244/0.69690. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67229/0.69719. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67199/0.69861. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66984/0.69776. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67048/0.69935. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66981/0.69872. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66946/0.69912. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66799/0.69823. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66765/0.69889. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66606/0.69904. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66788/0.69862. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66562/0.69825. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66447/0.69830. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66431/0.70038. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66338/0.70079. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66382/0.70183. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66179/0.70454. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66341/0.70075. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66109/0.70369. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66040/0.70126. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65981/0.70111. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65872/0.70275. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65724/0.70282. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65873/0.70581. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65655/0.70553. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65556/0.70765. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65295/0.70754. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65587/0.70776. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65261/0.70805. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65199/0.70788. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65221/0.70607. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64895/0.70906. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65242/0.71001. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65051/0.70879. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65085/0.71123. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64859/0.71008. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64705/0.70887. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64657/0.71170. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64577/0.70838. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64668/0.70755. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64603/0.71055. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64294/0.71145. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64301/0.71161. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64359/0.71079. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64300/0.71207. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63893/0.71138. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63805/0.71494. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63945/0.71547. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63466/0.71441. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64246/0.71655. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63753/0.71719. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63846/0.71505. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63440/0.71660. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63442/0.71775. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63057/0.71781. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63273/0.71897. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63109/0.72022. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63425/0.72096. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62884/0.72483. Took 0.18 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69272/0.68961. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69101/0.68724. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69019/0.68542. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68993/0.68435. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69001/0.68372. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68975/0.68319. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68902/0.68279. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68902/0.68249. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68914/0.68214. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68827/0.68188. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68873/0.68161. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68760/0.68143. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68762/0.68120. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68788/0.68095. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68744/0.68077. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68773/0.68089. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68613/0.68083. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68621/0.68078. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68557/0.68033. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68554/0.67973. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68510/0.67952. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68504/0.67933. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68398/0.67917. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68396/0.67914. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68417/0.67872. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68326/0.67864. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68254/0.67872. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68163/0.67867. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68140/0.67846. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68206/0.67843. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68091/0.67856. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68148/0.67879. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68024/0.67928. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68013/0.67938. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67960/0.67973. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67862/0.68004. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67709/0.68062. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67783/0.68045. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67729/0.68090. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67755/0.68114. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67680/0.68120. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67683/0.68162. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67495/0.68235. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67408/0.68287. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67315/0.68306. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67385/0.68420. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67287/0.68494. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67368/0.68531. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67245/0.68615. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67007/0.68699. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67184/0.68720. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67010/0.68810. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67011/0.68957. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66885/0.69027. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67118/0.69100. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66561/0.69110. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66962/0.69160. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66624/0.69317. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66726/0.69374. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66629/0.69466. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66582/0.69493. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66562/0.69587. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66616/0.69706. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66612/0.69795. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66637/0.69802. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66394/0.70015. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66290/0.69945. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66283/0.69999. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66199/0.70072. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66109/0.70124. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66106/0.70173. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66054/0.70301. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66442/0.70468. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66000/0.70539. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65828/0.70549. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65975/0.70493. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65704/0.70746. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65702/0.70935. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65704/0.70860. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65821/0.70818. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65804/0.70819. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65683/0.70860. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65523/0.70860. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.65311/0.71104. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65430/0.71055. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65438/0.71096. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65119/0.71240. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65173/0.71365. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65103/0.71410. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65235/0.71341. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65093/0.71509. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64762/0.71677. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65158/0.71603. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64645/0.71895. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64765/0.71947. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64935/0.71889. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64607/0.72033. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64774/0.72066. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64659/0.72043. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64242/0.72178. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69235/0.69228. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68922/0.69415. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68950/0.69482. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.68778/0.69544. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68740/0.69586. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68742/0.69627. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68620/0.69690. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68554/0.69770. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68626/0.69802. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68521/0.69857. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68410/0.69927. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68356/0.69993. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68429/0.70043. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68380/0.70116. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68347/0.70196. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68278/0.70237. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68282/0.70287. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68173/0.70375. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68087/0.70416. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68245/0.70486. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67999/0.70515. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67993/0.70561. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67993/0.70647. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67868/0.70716. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67848/0.70779. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67967/0.70855. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67866/0.70861. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67859/0.70880. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67634/0.70906. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67567/0.70968. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67497/0.71022. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67520/0.71110. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67580/0.71196. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67582/0.71227. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67389/0.71286. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67421/0.71354. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67406/0.71410. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67339/0.71454. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67205/0.71590. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67251/0.71628. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67054/0.71677. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67161/0.71754. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66907/0.71818. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66940/0.71883. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66639/0.71978. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66674/0.72171. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66796/0.72278. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66723/0.72331. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66551/0.72461. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66553/0.72405. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66607/0.72467. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66546/0.72547. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66511/0.72657. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66637/0.72824. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66320/0.72888. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66397/0.72891. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66212/0.73060. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66350/0.73104. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66351/0.73176. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66157/0.73263. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65743/0.73457. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66211/0.73493. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66107/0.73479. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65698/0.73651. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65650/0.73895. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65779/0.73982. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65524/0.74175. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65887/0.74108. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65675/0.74274. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65625/0.74352. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65440/0.74525. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65544/0.74551. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65427/0.74757. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65413/0.74794. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65307/0.74915. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65396/0.75108. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65502/0.75197. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65426/0.75189. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65235/0.75295. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64972/0.75416. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65057/0.75486. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65145/0.75565. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65121/0.75635. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65087/0.75582. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64686/0.75726. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64409/0.75912. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64890/0.76033. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64687/0.76196. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64828/0.76093. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64637/0.76185. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64607/0.76420. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64391/0.76490. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64605/0.76423. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64551/0.76599. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64176/0.76619. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63992/0.76800. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64436/0.76896. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64003/0.77001. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63912/0.77097. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64043/0.77138. Took 0.18 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69272/0.68155. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68982/0.68239. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68895/0.68288. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68770/0.68328. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68766/0.68377. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68685/0.68404. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68594/0.68425. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68551/0.68495. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68525/0.68555. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68468/0.68585. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68479/0.68628. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68375/0.68644. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68373/0.68664. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68285/0.68685. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68262/0.68709. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68216/0.68701. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68215/0.68747. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68233/0.68780. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68158/0.68803. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68144/0.68814. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68175/0.68779. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68141/0.68792. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68166/0.68838. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68115/0.68805. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68078/0.68817. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67968/0.68804. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67939/0.68809. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68067/0.68833. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67941/0.68812. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67975/0.68825. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67909/0.68776. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67911/0.68787. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67783/0.68765. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67903/0.68851. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67828/0.68812. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67822/0.68772. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67781/0.68821. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67845/0.68818. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67712/0.68758. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67714/0.68708. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67594/0.68739. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67597/0.68745. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67551/0.68890. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67582/0.68721. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67549/0.68773. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67574/0.68742. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67429/0.68788. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67498/0.68783. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67495/0.68828. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67439/0.68781. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67379/0.68811. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67283/0.68754. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67188/0.68719. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67236/0.68720. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67212/0.68701. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67267/0.68698. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67009/0.68791. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67041/0.68705. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67023/0.68815. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67089/0.68784. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66929/0.68663. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66854/0.68733. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66928/0.68810. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66990/0.68850. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66595/0.68774. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66590/0.68706. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66676/0.68704. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66566/0.68855. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66578/0.68704. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66476/0.68935. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66307/0.68876. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66402/0.68785. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66353/0.68783. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66061/0.69173. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66068/0.68932. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66033/0.68990. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66059/0.68988. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66089/0.68953. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65889/0.68967. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66108/0.68920. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65844/0.68997. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65806/0.69161. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65703/0.68938. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65777/0.69155. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65493/0.68993. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65389/0.69168. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65465/0.69561. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65260/0.69161. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65294/0.69371. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65106/0.69473. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65174/0.69666. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64812/0.69648. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65029/0.69498. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64996/0.69438. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64603/0.69891. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64645/0.69780. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64523/0.69834. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64534/0.69573. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64459/0.69729. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64473/0.70279. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69451/0.69455. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68729/0.69794. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68603/0.69941. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68529/0.70053. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68534/0.70180. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68561/0.70302. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68456/0.70429. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68455/0.70576. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68296/0.70710. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68283/0.70829. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68319/0.70969. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68271/0.71087. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68232/0.71194. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68048/0.71388. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68059/0.71544. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68102/0.71611. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68023/0.71680. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68023/0.71777. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67973/0.71820. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67875/0.71929. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67965/0.71957. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67918/0.71942. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67788/0.71952. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67826/0.72048. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67883/0.72068. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67734/0.72082. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67808/0.72089. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67722/0.72127. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67684/0.72185. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67771/0.72167. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67703/0.72261. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67706/0.72277. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67786/0.72213. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67741/0.72226. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67473/0.72207. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67505/0.72185. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67527/0.72125. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67430/0.72198. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67548/0.72205. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67323/0.72202. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67330/0.72216. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67320/0.72207. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67310/0.72197. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67160/0.72222. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67164/0.72249. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67162/0.72237. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67144/0.72205. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67256/0.72200. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67081/0.72138. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67153/0.72029. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67034/0.72023. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66810/0.71900. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66849/0.71925. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66925/0.71839. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66724/0.71713. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66874/0.71857. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66750/0.71858. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66730/0.71696. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 0.66688/0.71760. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66621/0.71658. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66486/0.71617. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66517/0.71612. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66222/0.71553. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66313/0.71455. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66068/0.71482. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66371/0.71466. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66134/0.71251. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66141/0.71188. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66030/0.71065. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65908/0.71118. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65946/0.70915. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65701/0.70843. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65864/0.70742. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65948/0.70674. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65645/0.70529. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65661/0.70513. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65588/0.70537. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65538/0.70719. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65558/0.70418. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65475/0.70240. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65291/0.70084. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65147/0.69790. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65550/0.70079. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65123/0.69740. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64846/0.70073. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65214/0.69804. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64987/0.69662. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64501/0.69644. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64915/0.69617. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64901/0.69515. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64536/0.69309. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.64477/0.69267. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64578/0.69049. Took 0.22 sec\n",
      "Epoch 93, Loss(train/val) 0.64733/0.69134. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64391/0.68979. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63881/0.68987. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64115/0.68792. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63862/0.68793. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64117/0.68330. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.63786/0.68618. Took 0.20 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69238/0.69496. Took 0.38 sec\n",
      "Epoch 1, Loss(train/val) 0.69047/0.69530. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68948/0.69587. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.68885/0.69639. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68862/0.69705. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68822/0.69757. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68833/0.69831. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68826/0.69884. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68706/0.69937. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68716/0.70012. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68706/0.70059. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68730/0.70110. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68618/0.70130. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68647/0.70141. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68570/0.70191. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68512/0.70225. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68553/0.70243. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68587/0.70260. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68531/0.70290. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68467/0.70291. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68428/0.70299. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68405/0.70303. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68433/0.70343. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68363/0.70323. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68276/0.70369. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68301/0.70380. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68310/0.70414. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68329/0.70438. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68217/0.70444. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68137/0.70486. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68181/0.70505. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68180/0.70486. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68098/0.70525. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68106/0.70553. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68062/0.70595. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67964/0.70573. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67952/0.70576. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67956/0.70601. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67854/0.70618. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67737/0.70651. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67805/0.70654. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67795/0.70725. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67778/0.70739. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67746/0.70756. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67521/0.70808. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67613/0.70852. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67540/0.70853. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67398/0.70914. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67371/0.70950. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67494/0.70971. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67286/0.70960. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67425/0.70961. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67207/0.71022. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67257/0.71046. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67077/0.71062. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67234/0.71027. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66867/0.71147. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66917/0.71190. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66908/0.71246. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66921/0.71337. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66888/0.71309. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66512/0.71323. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66646/0.71441. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66490/0.71488. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66556/0.71542. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66435/0.71691. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66341/0.71669. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66302/0.71618. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66003/0.71769. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66224/0.71821. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65854/0.71920. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65981/0.72076. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65695/0.72027. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65551/0.72061. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65881/0.72098. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65628/0.72149. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65352/0.72236. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65702/0.72223. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65506/0.72290. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65027/0.72397. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65173/0.72579. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65189/0.72583. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64897/0.72560. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64776/0.72623. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64908/0.72631. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64491/0.72794. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64741/0.72903. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64680/0.73063. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64034/0.73094. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64432/0.72984. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64187/0.73236. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64366/0.73054. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64122/0.73107. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64163/0.72975. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64306/0.73030. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63800/0.73207. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63830/0.73170. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63848/0.73290. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63173/0.73446. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63423/0.73572. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69399/0.69338. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69306/0.69184. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69257/0.69122. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69153/0.69103. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69220/0.69116. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69174/0.69147. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69153/0.69204. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69127/0.69233. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69137/0.69256. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69110/0.69297. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69085/0.69329. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69031/0.69355. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69002/0.69389. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68970/0.69445. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68890/0.69506. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68928/0.69542. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68929/0.69577. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68857/0.69625. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68863/0.69697. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68856/0.69740. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68764/0.69794. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68813/0.69826. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68806/0.69864. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68721/0.69882. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68790/0.69912. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68722/0.69930. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68672/0.69924. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68631/0.69965. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68597/0.69996. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68545/0.70032. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68494/0.70029. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68468/0.70060. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68508/0.70048. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68546/0.70083. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68463/0.70106. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68285/0.70138. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68335/0.70156. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68173/0.70258. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68317/0.70278. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68015/0.70367. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68199/0.70365. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68076/0.70356. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68044/0.70410. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68031/0.70441. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67881/0.70516. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67777/0.70467. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67609/0.70511. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67733/0.70492. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67654/0.70539. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67576/0.70581. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67250/0.70616. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67382/0.70664. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67364/0.70757. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67255/0.70765. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67265/0.70920. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67022/0.70868. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66968/0.70900. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66759/0.71092. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66824/0.71070. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66704/0.71120. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66665/0.71295. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66371/0.71206. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66322/0.71109. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66390/0.71216. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66261/0.71087. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66007/0.71457. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65735/0.71394. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65937/0.71496. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65827/0.71589. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65388/0.71648. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65805/0.71698. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65660/0.71504. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65550/0.71476. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65286/0.71828. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64999/0.71765. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65306/0.71786. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65200/0.71792. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64755/0.72042. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64425/0.72030. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64641/0.72060. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64529/0.72096. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64638/0.72045. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63989/0.71806. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64040/0.71977. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64125/0.72319. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64109/0.72366. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63632/0.72569. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63285/0.72677. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63508/0.72628. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63486/0.72403. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63596/0.72456. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62982/0.72398. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62683/0.72629. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63143/0.72744. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62395/0.72900. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62845/0.72917. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62399/0.72663. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62440/0.72761. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62147/0.72732. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62363/0.72718. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69580/0.69206. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69218/0.70006. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69002/0.70600. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68984/0.71101. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68863/0.71527. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68821/0.71807. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68835/0.71995. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68800/0.72177. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68777/0.72299. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68707/0.72394. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68748/0.72460. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68741/0.72462. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68685/0.72493. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68633/0.72558. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68585/0.72613. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68696/0.72630. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68622/0.72604. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68630/0.72581. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68597/0.72636. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68617/0.72647. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68581/0.72650. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68610/0.72612. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68530/0.72644. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68552/0.72625. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68514/0.72695. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68541/0.72690. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68513/0.72711. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68584/0.72688. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68492/0.72687. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68469/0.72620. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68473/0.72662. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68397/0.72671. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68392/0.72655. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68354/0.72660. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68303/0.72754. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68360/0.72778. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68320/0.72646. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68323/0.72744. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68320/0.72693. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68254/0.72668. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68257/0.72699. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68122/0.72783. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68153/0.72773. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68108/0.72758. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68017/0.72766. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67904/0.72654. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68044/0.72816. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67911/0.72753. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68044/0.72734. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67893/0.72621. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67907/0.72641. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67877/0.72614. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67846/0.72549. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67714/0.72566. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67815/0.72688. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67702/0.72632. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67508/0.72648. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67643/0.72671. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67560/0.72558. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67583/0.72537. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 0.67416/0.72519. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67562/0.72603. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67513/0.72474. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67373/0.72582. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67224/0.72500. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.67370/0.72473. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67065/0.72446. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67049/0.72377. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66968/0.72426. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67135/0.72411. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67125/0.72286. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66793/0.72286. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66854/0.72320. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66492/0.72382. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66460/0.72494. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66419/0.72546. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66908/0.72456. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66565/0.72448. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66281/0.72553. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66527/0.72502. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66208/0.72548. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66302/0.72575. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.66133/0.72380. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66271/0.72373. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 0.65978/0.72525. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65810/0.72491. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 0.66046/0.72721. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65898/0.72635. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65897/0.72772. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65693/0.72798. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.65696/0.72570. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65581/0.72817. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.65427/0.72968. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65068/0.72822. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65359/0.72768. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64850/0.72924. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64892/0.72847. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64892/0.72866. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65041/0.72809. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64747/0.73109. Took 0.20 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69775/0.69083. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69299/0.68796. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69098/0.68602. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69003/0.68470. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.68860/0.68359. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68876/0.68264. Took 0.22 sec\n",
      "Epoch 6, Loss(train/val) 0.68760/0.68190. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68691/0.68151. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68652/0.68117. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68654/0.68104. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68565/0.68115. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68455/0.68089. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68489/0.68077. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68570/0.68072. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68381/0.68053. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68426/0.68020. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68422/0.68017. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68365/0.68018. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68405/0.68007. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68359/0.68005. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68303/0.67991. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68250/0.67974. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68298/0.67967. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68364/0.67950. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68261/0.67942. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68210/0.67876. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68185/0.67858. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68203/0.67834. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68182/0.67833. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68230/0.67806. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68120/0.67796. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68175/0.67805. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68102/0.67790. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68157/0.67779. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67990/0.67791. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68011/0.67734. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68111/0.67727. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68106/0.67688. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68054/0.67695. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68077/0.67620. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67962/0.67694. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67940/0.67622. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67869/0.67596. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67940/0.67551. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67794/0.67585. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67746/0.67608. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67757/0.67619. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67842/0.67520. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67825/0.67449. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67588/0.67509. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67637/0.67466. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67570/0.67456. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67393/0.67463. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67650/0.67445. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67600/0.67433. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67529/0.67369. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67429/0.67353. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67408/0.67346. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67491/0.67436. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67358/0.67413. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67482/0.67381. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67314/0.67367. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67198/0.67371. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66969/0.67455. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66966/0.67436. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67008/0.67444. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67182/0.67423. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67117/0.67406. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67079/0.67495. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66897/0.67350. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66905/0.67446. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66852/0.67419. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66846/0.67396. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66593/0.67416. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66823/0.67496. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66603/0.67433. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66571/0.67526. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66627/0.67555. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66517/0.67509. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66504/0.67544. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.66298/0.67642. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66241/0.67691. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66276/0.67754. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66066/0.67640. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65878/0.67733. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66001/0.67845. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66173/0.67802. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65756/0.67848. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65749/0.67955. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65576/0.68055. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65569/0.68305. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65400/0.68103. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65102/0.68108. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65424/0.68118. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65371/0.68077. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65262/0.68267. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65071/0.68365. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65136/0.68305. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65122/0.68267. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64831/0.68116. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69478/0.69882. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69195/0.69920. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69076/0.69957. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69056/0.69995. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68978/0.70019. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68931/0.70063. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68870/0.70118. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68842/0.70201. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68716/0.70275. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68690/0.70334. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68725/0.70399. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68667/0.70456. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68569/0.70522. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68636/0.70595. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68540/0.70664. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68479/0.70767. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68463/0.70825. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68538/0.70852. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68416/0.70889. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68413/0.70924. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68413/0.70966. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68351/0.71020. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68231/0.71087. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68245/0.71158. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68259/0.71207. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68175/0.71293. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68208/0.71338. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68083/0.71400. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68143/0.71439. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68128/0.71469. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68069/0.71529. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68072/0.71566. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68030/0.71633. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68034/0.71695. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67870/0.71771. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67887/0.71842. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67846/0.71870. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67778/0.71955. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67729/0.72029. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67649/0.72085. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67666/0.72183. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67630/0.72262. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67453/0.72358. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67416/0.72429. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67581/0.72513. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67334/0.72590. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67430/0.72717. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67057/0.72860. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67270/0.72921. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67196/0.73015. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67028/0.73146. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66960/0.73295. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67027/0.73387. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66850/0.73447. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66814/0.73508. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66873/0.73627. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66822/0.73630. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66707/0.73703. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66527/0.73810. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66444/0.73931. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66390/0.73921. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66249/0.74022. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66055/0.74156. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65907/0.74307. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65802/0.74460. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65780/0.74586. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65676/0.74664. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65889/0.74725. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65499/0.74910. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65335/0.75056. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65444/0.75059. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65161/0.75159. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65513/0.75148. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65185/0.75230. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65282/0.75365. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64893/0.75454. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64923/0.75701. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64849/0.75742. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64861/0.75813. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64881/0.75863. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64859/0.75907. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64567/0.76054. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64371/0.76252. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64483/0.76283. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64243/0.76329. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63807/0.76538. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63767/0.76694. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64022/0.76673. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63888/0.76762. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63740/0.76973. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63619/0.77053. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63602/0.77048. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63672/0.77008. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63551/0.77143. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63556/0.77293. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62958/0.77299. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63376/0.77540. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63093/0.77438. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62868/0.78051. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62071/0.77946. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69200/0.70090. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69016/0.70260. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68960/0.70411. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68924/0.70554. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68842/0.70662. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68786/0.70793. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68755/0.70887. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68840/0.70946. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68747/0.71060. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68751/0.71103. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68734/0.71161. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68696/0.71220. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68582/0.71297. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68669/0.71336. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68589/0.71410. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68627/0.71471. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68586/0.71488. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68552/0.71554. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68499/0.71655. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68516/0.71725. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68477/0.71745. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68521/0.71772. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68331/0.71848. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68361/0.71907. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68425/0.71930. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68371/0.71986. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68346/0.72042. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68237/0.72097. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68364/0.72137. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68172/0.72181. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68271/0.72225. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68085/0.72242. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68182/0.72234. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68087/0.72326. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68098/0.72415. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68025/0.72489. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67988/0.72521. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67911/0.72585. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67931/0.72612. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67840/0.72659. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67844/0.72645. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67551/0.72703. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67797/0.72755. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67705/0.72774. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67824/0.72776. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67667/0.72738. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67537/0.72839. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67406/0.72876. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67426/0.72878. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67447/0.72960. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67460/0.73026. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67469/0.72992. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67493/0.72974. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67274/0.73062. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67258/0.73107. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67263/0.72998. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67259/0.73016. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67124/0.72985. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67125/0.73010. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67039/0.73116. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67093/0.73027. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66981/0.73005. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66794/0.73085. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66750/0.73175. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66774/0.73227. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66615/0.73241. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66701/0.73403. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66661/0.73418. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66648/0.73408. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66546/0.73397. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66334/0.73373. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66249/0.73473. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66145/0.73650. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66249/0.73617. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66532/0.73697. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66181/0.73659. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66151/0.73667. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66154/0.73683. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66192/0.73823. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65757/0.73959. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65956/0.74036. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65953/0.74129. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65467/0.74223. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65752/0.74168. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65486/0.74094. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65293/0.74247. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65547/0.74309. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65426/0.74076. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65099/0.74333. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65395/0.74218. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65508/0.74026. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65099/0.74014. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64959/0.74022. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65105/0.74191. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64884/0.74327. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65107/0.74287. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64749/0.74239. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64966/0.74265. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64855/0.74286. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64674/0.74043. Took 0.19 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69193/0.68734. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69054/0.68695. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68948/0.68768. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68938/0.68798. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68761/0.68876. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68739/0.68937. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68708/0.68998. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68721/0.69049. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68618/0.69126. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68643/0.69204. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68633/0.69240. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68580/0.69291. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68535/0.69368. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68565/0.69406. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68542/0.69450. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68365/0.69481. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68394/0.69549. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68381/0.69572. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68351/0.69612. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68316/0.69671. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68276/0.69754. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68288/0.69783. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68119/0.69762. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68234/0.69883. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68311/0.69894. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68161/0.69894. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68072/0.69977. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68180/0.69998. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.67997/0.70083. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67960/0.70136. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68093/0.70133. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67980/0.70249. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67948/0.70322. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67870/0.70424. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68021/0.70426. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67872/0.70476. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67748/0.70540. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67903/0.70540. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67719/0.70606. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67841/0.70637. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67688/0.70712. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67591/0.70748. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67517/0.70806. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67555/0.70809. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67451/0.70889. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67337/0.71026. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67478/0.71010. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67552/0.71117. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67406/0.71050. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67401/0.71192. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67250/0.71229. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67137/0.71260. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67004/0.71354. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67079/0.71335. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66908/0.71641. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67145/0.71396. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67036/0.71625. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66849/0.71634. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66683/0.71765. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66816/0.71836. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66789/0.71865. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66590/0.72037. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66676/0.72130. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66584/0.71969. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66535/0.72321. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66349/0.72296. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66495/0.72390. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66288/0.72238. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66206/0.72112. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65903/0.72342. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66282/0.72446. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66196/0.72262. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65931/0.72367. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65930/0.72401. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65814/0.72563. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65780/0.72631. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65522/0.72666. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65471/0.72713. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65641/0.72693. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65763/0.72662. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65143/0.72993. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65202/0.73045. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65420/0.72965. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65069/0.73039. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65279/0.73188. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65142/0.73181. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65207/0.73303. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65117/0.73349. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65007/0.73499. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65021/0.73421. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65013/0.73765. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64785/0.73568. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64669/0.73618. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64520/0.73731. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64573/0.73521. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64633/0.73618. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64363/0.74099. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64309/0.74273. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63720/0.74454. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64143/0.74549. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69249/0.70320. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69107/0.70383. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69004/0.70426. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.68977/0.70494. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68860/0.70546. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68835/0.70630. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68781/0.70706. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68742/0.70747. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68724/0.70840. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68718/0.70892. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68644/0.70936. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68583/0.70999. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68519/0.71068. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68526/0.71139. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68479/0.71180. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68468/0.71211. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68458/0.71273. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68447/0.71313. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68427/0.71335. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68231/0.71398. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68317/0.71357. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68206/0.71407. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68316/0.71477. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68191/0.71484. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68146/0.71523. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68075/0.71565. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68118/0.71586. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68055/0.71545. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67964/0.71560. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67901/0.71630. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67881/0.71594. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67851/0.71667. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67833/0.71677. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67746/0.71593. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67673/0.71615. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67566/0.71720. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67589/0.71734. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67513/0.71746. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67407/0.71735. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67350/0.71768. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67469/0.71739. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67164/0.71729. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67148/0.71901. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67098/0.71867. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67074/0.71820. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66935/0.71818. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66815/0.71794. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66887/0.71706. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66795/0.71729. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66586/0.71735. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66626/0.71737. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66464/0.71849. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66138/0.71902. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66193/0.71826. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66084/0.71949. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65991/0.71853. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65926/0.72087. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65790/0.72064. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65779/0.72036. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65654/0.71958. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66172/0.71994. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65719/0.71979. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65464/0.72189. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65311/0.71993. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65355/0.72000. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65140/0.71988. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65248/0.72053. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64803/0.71831. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65015/0.71889. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64692/0.72056. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64740/0.72033. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64488/0.72039. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64549/0.72298. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64035/0.71992. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64107/0.72091. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64097/0.72339. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64061/0.72304. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63902/0.72609. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63890/0.72572. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63999/0.72443. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63643/0.72545. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63296/0.72422. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63246/0.72452. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63272/0.72633. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63075/0.72558. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62683/0.72363. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63147/0.72433. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62780/0.72443. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62441/0.72546. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62656/0.72759. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62220/0.72866. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62314/0.72915. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62272/0.72711. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.61977/0.72606. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61879/0.72950. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61954/0.73107. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61407/0.72861. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61605/0.73010. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61810/0.72987. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61477/0.72947. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69179/0.69177. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69058/0.69138. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69098/0.69099. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69111/0.69061. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69075/0.69030. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69023/0.68988. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69039/0.68947. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68981/0.68888. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68959/0.68837. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68935/0.68767. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68857/0.68683. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68836/0.68594. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68731/0.68512. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68780/0.68415. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68755/0.68319. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68676/0.68214. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68533/0.68083. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68564/0.67978. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68504/0.67831. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68426/0.67673. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68385/0.67538. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68375/0.67417. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68276/0.67302. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68253/0.67231. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68379/0.67136. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68159/0.67060. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68051/0.66985. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68087/0.66926. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68052/0.66878. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67802/0.66805. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67946/0.66757. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67712/0.66743. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67890/0.66708. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67884/0.66688. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67757/0.66666. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67721/0.66625. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67650/0.66609. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67727/0.66663. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67615/0.66626. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67422/0.66622. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67445/0.66604. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67227/0.66633. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67215/0.66637. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67302/0.66676. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67223/0.66744. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.67135/0.66703. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67115/0.66757. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67164/0.66796. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66969/0.66840. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67042/0.66895. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67063/0.66910. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66989/0.66922. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66730/0.66871. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66724/0.67017. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66750/0.67055. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66806/0.67084. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66560/0.67118. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66508/0.67182. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66345/0.67219. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66433/0.67291. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66164/0.67319. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66363/0.67461. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66133/0.67375. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65894/0.67451. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66025/0.67385. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65953/0.67387. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65786/0.67532. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65818/0.67469. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65517/0.67711. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65633/0.67776. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65592/0.67904. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65772/0.67831. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65589/0.67849. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65546/0.67632. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65458/0.67509. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65373/0.67667. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65483/0.67550. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65418/0.67763. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65026/0.67774. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64997/0.67768. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64735/0.67888. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64960/0.68037. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64988/0.68016. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64870/0.68240. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64976/0.68100. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64646/0.68033. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64560/0.68132. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64332/0.68100. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64835/0.67914. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64549/0.68040. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64565/0.67626. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64241/0.68006. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64050/0.68134. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64301/0.68141. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.63934/0.68205. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64091/0.67698. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64061/0.67539. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63908/0.68198. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63567/0.67960. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64047/0.67998. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69562/0.69119. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69228/0.69238. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69057/0.69312. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69083/0.69365. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69037/0.69422. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69034/0.69484. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68907/0.69519. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68891/0.69567. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68945/0.69631. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68882/0.69678. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68845/0.69723. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68830/0.69781. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68848/0.69843. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68724/0.69893. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68759/0.69940. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68702/0.69991. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68694/0.70025. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68721/0.70041. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68630/0.70090. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68599/0.70112. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68638/0.70129. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68599/0.70171. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68663/0.70188. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68664/0.70177. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68717/0.70151. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68561/0.70203. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68560/0.70194. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68588/0.70198. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68479/0.70212. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68429/0.70224. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68392/0.70232. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68321/0.70274. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68412/0.70250. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68377/0.70249. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68322/0.70286. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68310/0.70265. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68399/0.70251. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.68272/0.70243. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68166/0.70282. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68218/0.70236. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68224/0.70218. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68169/0.70154. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68037/0.70204. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68139/0.70212. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68079/0.70186. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67980/0.70176. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68016/0.70136. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67952/0.70192. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67871/0.70110. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67797/0.70166. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67841/0.70149. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67846/0.70107. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67862/0.70008. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67739/0.70064. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67678/0.70011. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67689/0.69971. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67599/0.69967. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67647/0.69889. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67470/0.69932. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67382/0.69844. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67337/0.69928. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67432/0.69807. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67242/0.69796. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67312/0.69925. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67181/0.69797. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67350/0.69877. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67078/0.69802. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67003/0.69813. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67089/0.69735. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67030/0.69750. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66967/0.69685. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66842/0.69640. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67042/0.69682. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66786/0.69628. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66834/0.69656. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66676/0.69648. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66603/0.69725. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66618/0.69540. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66391/0.69690. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66508/0.69870. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66538/0.69753. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66373/0.69631. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66211/0.69657. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66381/0.69539. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66231/0.69536. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66048/0.69675. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66326/0.69547. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65918/0.69444. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65936/0.69602. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65953/0.69527. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65859/0.69555. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65954/0.69641. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65574/0.69495. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65768/0.69303. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65469/0.69472. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65788/0.69719. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65532/0.69523. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65457/0.69350. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65469/0.69126. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65442/0.69283. Took 0.19 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69434/0.69262. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69303/0.69162. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69189/0.69111. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69190/0.69081. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69059/0.69050. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68997/0.69035. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68997/0.69038. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69002/0.69051. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68924/0.69074. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68867/0.69081. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68877/0.69098. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68786/0.69139. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68834/0.69174. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68748/0.69204. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68753/0.69237. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68706/0.69263. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68699/0.69303. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68653/0.69347. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68640/0.69404. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68615/0.69435. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68572/0.69475. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68575/0.69517. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68509/0.69552. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68529/0.69607. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68543/0.69646. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68472/0.69691. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68386/0.69714. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68399/0.69756. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68377/0.69808. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68309/0.69862. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68428/0.69903. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68241/0.69969. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68219/0.70004. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68221/0.70052. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68275/0.70092. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68225/0.70117. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68184/0.70138. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68037/0.70164. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68088/0.70222. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68035/0.70238. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67972/0.70253. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67945/0.70307. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67896/0.70340. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68034/0.70390. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67934/0.70412. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67908/0.70428. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67891/0.70475. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67879/0.70466. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67803/0.70505. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67820/0.70516. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67802/0.70532. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67765/0.70573. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67665/0.70601. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67672/0.70613. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67534/0.70633. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67562/0.70662. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67527/0.70729. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67690/0.70776. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67490/0.70821. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67390/0.70846. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67465/0.70888. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67499/0.70920. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67301/0.70925. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67246/0.70977. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67225/0.71029. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67317/0.71081. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67141/0.71103. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67319/0.71145. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67329/0.71173. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67354/0.71191. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67270/0.71235. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67352/0.71233. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67190/0.71319. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67094/0.71359. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67193/0.71335. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67209/0.71353. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66967/0.71377. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66865/0.71460. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67143/0.71516. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66803/0.71542. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66900/0.71633. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66760/0.71640. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66744/0.71708. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66800/0.71749. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66656/0.71768. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66784/0.71810. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66646/0.71860. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66639/0.71919. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66650/0.71901. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66511/0.71884. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66301/0.72010. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66403/0.72094. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66458/0.72136. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66310/0.72225. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66232/0.72289. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66209/0.72338. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66162/0.72373. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66272/0.72394. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66328/0.72402. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66215/0.72417. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69994/0.69605. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69532/0.69586. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69328/0.69564. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69335/0.69553. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69217/0.69546. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69285/0.69545. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69215/0.69551. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69276/0.69554. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69239/0.69561. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69212/0.69568. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69167/0.69574. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69204/0.69581. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69127/0.69594. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69073/0.69607. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69100/0.69625. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69117/0.69642. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69094/0.69663. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69057/0.69686. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69048/0.69708. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.69016/0.69737. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68971/0.69769. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69078/0.69800. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.69092/0.69839. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68980/0.69879. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68973/0.69912. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68864/0.69951. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68925/0.69997. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68896/0.70046. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68930/0.70102. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68907/0.70152. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68831/0.70213. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68778/0.70274. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68764/0.70347. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68737/0.70412. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68661/0.70469. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68669/0.70541. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68634/0.70619. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68579/0.70706. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68586/0.70784. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68612/0.70841. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68650/0.70930. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68614/0.71028. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68485/0.71110. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68468/0.71166. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68417/0.71282. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68446/0.71346. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68420/0.71400. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68350/0.71450. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68285/0.71542. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68360/0.71597. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68164/0.71700. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68192/0.71742. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68187/0.71794. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68195/0.71857. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67986/0.71970. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67952/0.72045. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67995/0.72121. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67874/0.72183. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68110/0.72255. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67814/0.72302. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67944/0.72325. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67896/0.72377. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67753/0.72436. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67701/0.72580. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67729/0.72670. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67606/0.72754. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67463/0.72855. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67523/0.72950. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67448/0.73058. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67541/0.73157. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67747/0.73269. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67255/0.73372. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67124/0.73518. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67321/0.73490. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67147/0.73563. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67030/0.73638. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67265/0.73763. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67188/0.73777. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66964/0.73880. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66952/0.73991. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66800/0.74118. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66920/0.74116. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66744/0.74222. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67021/0.74378. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66772/0.74440. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66424/0.74607. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66537/0.74707. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66392/0.74820. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66480/0.74971. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66218/0.75147. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66205/0.75178. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66256/0.75244. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65963/0.75379. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66034/0.75537. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66221/0.75546. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65846/0.75764. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65973/0.75813. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65755/0.75872. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65734/0.75896. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65436/0.76059. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69614/0.69719. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69441/0.69730. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69509/0.69726. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69392/0.69681. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69366/0.69665. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69387/0.69651. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69270/0.69637. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69251/0.69642. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69229/0.69609. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69144/0.69597. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69151/0.69583. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69081/0.69555. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.69122/0.69541. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69141/0.69578. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69013/0.69576. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68996/0.69589. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.69016/0.69581. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68869/0.69631. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68889/0.69646. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68938/0.69685. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68899/0.69612. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68856/0.69602. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68868/0.69585. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68807/0.69628. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68693/0.69674. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68750/0.69729. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68790/0.69770. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68698/0.69784. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.68669/0.69823. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68550/0.69757. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68454/0.69769. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68580/0.69870. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68449/0.69962. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68595/0.70035. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 0.68334/0.70026. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68519/0.70097. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68350/0.70181. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68405/0.70286. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68405/0.70275. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68237/0.70351. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68128/0.70412. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68108/0.70536. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68056/0.70671. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67880/0.70737. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67936/0.70880. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67945/0.70910. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.67982/0.70891. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67697/0.71123. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67829/0.71060. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67875/0.71361. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67732/0.71460. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67595/0.71458. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67461/0.71614. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67462/0.71822. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67525/0.71931. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67246/0.72046. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67372/0.72224. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67321/0.72402. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67300/0.72487. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67182/0.72667. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66862/0.72741. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66902/0.72856. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67044/0.73041. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66830/0.73189. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66794/0.73393. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66631/0.73489. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66717/0.73648. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66726/0.73764. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66666/0.73715. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66435/0.74135. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66110/0.74047. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66419/0.74328. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66335/0.74605. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66388/0.74667. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66043/0.74984. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66047/0.75126. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66142/0.75356. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65761/0.75307. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65933/0.75544. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65842/0.75765. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65815/0.76112. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65799/0.76271. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65797/0.76299. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65304/0.76637. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65250/0.76529. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65360/0.76905. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65231/0.76724. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64950/0.76981. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65451/0.77122. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65106/0.77484. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65033/0.77644. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64838/0.77745. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64625/0.77776. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64564/0.78136. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64341/0.78170. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64557/0.78068. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64673/0.78299. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64601/0.78440. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64377/0.78252. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64213/0.78273. Took 0.19 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69346/0.69842. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69335/0.69643. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69275/0.69482. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69193/0.69352. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69197/0.69257. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69086/0.69176. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69109/0.69116. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69073/0.69102. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69007/0.69077. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69010/0.69070. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68963/0.69101. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68924/0.69080. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68839/0.69102. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68926/0.69178. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68790/0.69142. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68911/0.69180. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68872/0.69219. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68758/0.69287. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68842/0.69319. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68676/0.69303. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68641/0.69304. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68722/0.69344. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68645/0.69419. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68615/0.69420. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68565/0.69508. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68547/0.69546. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68565/0.69566. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68557/0.69583. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68437/0.69686. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68402/0.69674. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68421/0.69762. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68380/0.69761. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68354/0.69863. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68232/0.69990. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68396/0.70028. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68327/0.70146. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68169/0.70124. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68131/0.70145. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68022/0.70161. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68056/0.70282. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67841/0.70315. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67707/0.70324. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67772/0.70467. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67583/0.70501. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67675/0.70660. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67698/0.70643. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67611/0.70599. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67445/0.70740. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67316/0.70783. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67268/0.70861. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66999/0.70876. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67132/0.70916. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67115/0.70981. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66933/0.70997. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66760/0.70980. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66644/0.71117. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66708/0.70974. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66457/0.71046. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66531/0.71036. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66172/0.71144. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65996/0.71255. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65959/0.70980. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65844/0.71016. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65839/0.71082. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65578/0.71245. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65199/0.71189. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65419/0.71142. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65504/0.71161. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65079/0.71301. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64793/0.71135. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64849/0.71126. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64632/0.71226. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64415/0.71343. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64133/0.71329. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64468/0.71345. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64207/0.71393. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64101/0.71181. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63932/0.71112. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63629/0.71492. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63386/0.71700. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63477/0.71606. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63450/0.71414. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63314/0.71811. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63299/0.71928. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63311/0.72091. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62755/0.72106. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63149/0.71963. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62790/0.72430. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62435/0.72632. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62192/0.72795. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62364/0.72997. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62184/0.72936. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62344/0.72695. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61793/0.72917. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61872/0.73137. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62196/0.73216. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61551/0.73360. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61728/0.73268. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61645/0.73516. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61584/0.73572. Took 0.18 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69263/0.68642. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69197/0.68503. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69089/0.68429. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69116/0.68366. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69086/0.68330. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69027/0.68318. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69011/0.68330. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69025/0.68359. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68947/0.68383. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68992/0.68404. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68856/0.68389. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68889/0.68413. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68777/0.68429. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68799/0.68460. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68687/0.68500. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68704/0.68492. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68707/0.68543. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68623/0.68613. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68619/0.68643. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68601/0.68682. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68477/0.68737. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68470/0.68794. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68547/0.68826. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68415/0.68859. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68438/0.68952. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68428/0.69003. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68378/0.69122. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68425/0.69198. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68331/0.69206. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68160/0.69206. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68315/0.69379. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68144/0.69514. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68173/0.69528. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68094/0.69654. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68072/0.69829. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68057/0.69985. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67887/0.70010. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67844/0.70185. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67892/0.70229. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67932/0.70502. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67820/0.70620. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67740/0.70786. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67664/0.71020. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67741/0.71007. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67629/0.71071. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67575/0.71233. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67661/0.71386. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67400/0.71591. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67387/0.71830. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67218/0.71919. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67298/0.71974. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67213/0.72347. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67066/0.72337. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67146/0.72506. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67200/0.72688. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67105/0.72780. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67054/0.72924. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66890/0.73121. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66984/0.73261. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66915/0.73482. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66898/0.73679. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66766/0.73623. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66790/0.73697. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66688/0.73717. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66630/0.73711. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66540/0.74018. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66413/0.74074. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66451/0.74247. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66562/0.74313. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66431/0.74507. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66098/0.74528. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66154/0.74760. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66393/0.74493. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.66182/0.74653. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65942/0.74893. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65918/0.74955. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65852/0.74940. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66140/0.75031. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65803/0.74914. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65699/0.75145. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65620/0.75310. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65635/0.75249. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65403/0.75482. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65426/0.75644. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65519/0.75576. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65133/0.75642. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65306/0.75872. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65079/0.76070. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65036/0.76097. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64635/0.76028. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64728/0.76134. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64896/0.76048. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65146/0.76503. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64763/0.76385. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64560/0.76447. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64633/0.76506. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64314/0.76648. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64071/0.76703. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64058/0.76819. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64296/0.76889. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69467/0.69136. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69047/0.68925. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69000/0.68903. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69019/0.68895. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69022/0.68911. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68905/0.68918. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68960/0.68916. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68843/0.68910. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68877/0.68923. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68848/0.68937. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68774/0.68944. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68756/0.68952. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68793/0.68970. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68701/0.68998. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68793/0.69014. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68632/0.69016. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68648/0.69024. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68558/0.69041. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68527/0.69056. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68530/0.69078. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68539/0.69097. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68414/0.69110. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68536/0.69093. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68377/0.69095. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68250/0.69078. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68290/0.69087. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68172/0.69097. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68194/0.69092. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68142/0.69075. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68065/0.69075. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68054/0.69055. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67976/0.69033. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67942/0.69001. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67881/0.68981. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67828/0.68965. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67626/0.68981. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67879/0.68920. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67502/0.68848. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.67591/0.68840. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67438/0.68842. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67534/0.68852. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67350/0.68851. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67179/0.68844. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67146/0.68879. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67049/0.68918. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67216/0.68972. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66946/0.69000. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66743/0.68780. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66692/0.68968. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66376/0.68918. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.66435/0.68864. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66291/0.68750. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.66323/0.69066. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65841/0.69108. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.65956/0.69067. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65831/0.68911. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65637/0.68799. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65676/0.69212. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 0.65539/0.69150. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65105/0.69024. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 0.65229/0.69003. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65271/0.69430. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.64940/0.69289. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65086/0.69245. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64420/0.69590. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64575/0.69605. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 0.64343/0.69229. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63997/0.69521. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63917/0.69696. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64169/0.69834. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63900/0.69926. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.63565/0.69634. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63772/0.69789. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.63197/0.70163. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.63147/0.70138. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63027/0.69954. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63115/0.70298. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62699/0.70311. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62826/0.70148. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62847/0.70280. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62974/0.70704. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.62689/0.70341. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.62282/0.70469. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61956/0.70834. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62241/0.70917. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61862/0.70308. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61603/0.70537. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61914/0.70779. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61791/0.70906. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61192/0.71034. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.61256/0.71416. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.60997/0.71511. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.60905/0.71167. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.60639/0.71553. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.60468/0.72100. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60955/0.72317. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.60103/0.71289. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60267/0.71683. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60491/0.71851. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60001/0.71812. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69311/0.69015. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69077/0.69056. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69047/0.69127. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69025/0.69195. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68935/0.69267. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68929/0.69336. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68849/0.69404. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68845/0.69474. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68816/0.69547. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68745/0.69621. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68667/0.69696. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68690/0.69775. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68552/0.69854. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68614/0.69925. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68557/0.69996. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68507/0.70058. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68502/0.70124. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68507/0.70171. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68460/0.70214. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68380/0.70266. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68306/0.70303. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68343/0.70326. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68159/0.70365. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68253/0.70383. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68137/0.70433. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67971/0.70469. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68079/0.70499. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67894/0.70522. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68011/0.70557. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67804/0.70551. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67835/0.70562. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67817/0.70550. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67737/0.70531. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67581/0.70551. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67578/0.70609. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67404/0.70641. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67305/0.70674. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67256/0.70721. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67101/0.70763. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67154/0.70759. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66909/0.70824. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66843/0.70877. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.66926/0.70949. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66850/0.70936. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66430/0.71022. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66409/0.71140. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66126/0.71238. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66130/0.71368. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66211/0.71510. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.65828/0.71614. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65880/0.71882. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.65684/0.72028. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65806/0.72106. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65255/0.72275. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.65165/0.72495. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.65298/0.72533. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64920/0.72705. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64676/0.73037. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64364/0.73423. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.64022/0.73551. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.64000/0.73754. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64237/0.74095. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.64077/0.74234. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.64004/0.74068. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63766/0.74379. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.64249/0.74357. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63759/0.74356. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63474/0.74620. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63363/0.74816. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.62914/0.75417. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.62966/0.75758. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.62696/0.75705. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63308/0.75960. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62840/0.76006. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.62641/0.76466. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.62310/0.76428. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62153/0.76551. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62131/0.76906. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62209/0.77696. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62392/0.77447. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62166/0.77812. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.62131/0.77850. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61442/0.77451. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61671/0.78139. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.61709/0.77840. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61134/0.78354. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.62149/0.78170. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61234/0.78215. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61485/0.78390. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61063/0.78593. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61032/0.78438. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61387/0.78586. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.61199/0.79219. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61262/0.79362. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.60426/0.79307. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60470/0.79705. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60535/0.79741. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60615/0.79789. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60907/0.79461. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60744/0.79948. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69244/0.69319. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69024/0.69326. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68909/0.69327. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68958/0.69307. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68885/0.69281. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68846/0.69261. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68841/0.69255. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68818/0.69231. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68836/0.69214. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68756/0.69210. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68662/0.69211. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68694/0.69212. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68708/0.69213. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68661/0.69205. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.68669/0.69204. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68549/0.69241. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68570/0.69249. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68600/0.69262. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68593/0.69260. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68437/0.69287. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68461/0.69328. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68383/0.69361. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68337/0.69396. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68310/0.69422. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68326/0.69454. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68310/0.69539. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68207/0.69619. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68307/0.69686. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68254/0.69705. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68235/0.69758. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68110/0.69839. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67948/0.69927. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68038/0.69972. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67970/0.70031. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67939/0.70102. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67854/0.70258. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67757/0.70412. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67641/0.70613. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67509/0.70909. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67551/0.71047. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67475/0.71198. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67404/0.71423. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67177/0.71526. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67155/0.71932. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67252/0.72181. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67205/0.72160. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67145/0.71983. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67075/0.72357. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66973/0.72454. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66952/0.72469. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66845/0.72044. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67055/0.72614. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66586/0.72638. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66777/0.73071. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66566/0.72922. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66414/0.73510. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66702/0.72926. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66192/0.73255. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66504/0.73750. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66354/0.73516. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66122/0.73579. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66225/0.73984. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66102/0.73724. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66060/0.73931. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65960/0.73963. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66035/0.74253. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65656/0.73952. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65414/0.74201. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65506/0.74277. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65437/0.74076. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65687/0.74354. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65232/0.74350. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65219/0.74547. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65309/0.74503. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64915/0.74833. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65045/0.74496. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64953/0.75036. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64766/0.74741. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64186/0.75062. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64460/0.74647. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64779/0.74688. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64665/0.74593. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64494/0.74729. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64395/0.74775. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64088/0.74928. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64123/0.74774. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64012/0.74550. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63808/0.74965. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63778/0.74815. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63278/0.75186. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63552/0.74991. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63914/0.74876. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63689/0.75429. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63351/0.75186. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63509/0.75140. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63314/0.75026. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63035/0.75619. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62810/0.75687. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63136/0.75983. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62496/0.75856. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69043/0.69840. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68909/0.69915. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68847/0.69920. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68765/0.69919. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68757/0.69921. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68744/0.69928. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68623/0.69936. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68614/0.69951. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68530/0.69967. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68540/0.70032. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68544/0.70025. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68458/0.70084. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68512/0.70097. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68437/0.70168. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68503/0.70205. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68434/0.70245. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68454/0.70245. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68391/0.70276. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68384/0.70295. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68299/0.70316. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68366/0.70369. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68358/0.70409. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68338/0.70484. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68237/0.70481. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68306/0.70553. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68268/0.70550. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68271/0.70590. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68221/0.70621. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68269/0.70720. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68217/0.70709. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68278/0.70784. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68184/0.70815. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68190/0.70819. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68223/0.70902. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68201/0.70924. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68155/0.70910. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68105/0.70944. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67968/0.71069. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67976/0.71147. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68032/0.71148. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68063/0.71175. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68045/0.71204. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67935/0.71230. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67967/0.71321. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67864/0.71317. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67766/0.71434. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67930/0.71467. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67791/0.71541. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67929/0.71679. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67866/0.71664. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67718/0.71676. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67732/0.71768. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67665/0.71890. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67567/0.72044. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67397/0.72104. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67434/0.72228. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67785/0.72278. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67496/0.72255. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67556/0.72352. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67520/0.72349. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67442/0.72491. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.67330/0.72557. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67156/0.72727. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67284/0.72610. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67212/0.72693. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.67209/0.72790. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67359/0.72778. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67177/0.72863. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67082/0.73039. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66998/0.73027. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67097/0.73088. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67157/0.73108. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66934/0.73099. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67011/0.73241. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66914/0.73354. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66891/0.73468. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.67171/0.73541. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66990/0.73537. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66701/0.73552. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66692/0.73558. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66748/0.73700. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66682/0.73785. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66640/0.73710. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66792/0.73759. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66594/0.73869. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66626/0.73833. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66561/0.73968. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66480/0.74102. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66541/0.74269. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66261/0.74514. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.66287/0.74674. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66481/0.74665. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66403/0.74667. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66150/0.74415. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66009/0.74428. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66101/0.74845. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66199/0.74848. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65859/0.75062. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66008/0.75226. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65894/0.75112. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.68971/0.70108. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68890/0.70102. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68773/0.70104. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68777/0.70130. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68720/0.70158. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68659/0.70196. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68655/0.70254. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68619/0.70258. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68502/0.70300. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68602/0.70332. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68527/0.70366. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68546/0.70396. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68492/0.70441. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68485/0.70460. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68420/0.70476. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68391/0.70501. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68407/0.70540. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68286/0.70550. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68340/0.70611. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68312/0.70602. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68337/0.70615. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68273/0.70627. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68247/0.70692. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68100/0.70726. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68152/0.70762. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68148/0.70792. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68056/0.70796. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68036/0.70839. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68052/0.70891. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68018/0.70903. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67812/0.71002. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67870/0.70952. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67801/0.70849. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67665/0.70885. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67545/0.70962. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67589/0.70900. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67518/0.70807. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67469/0.70925. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67375/0.70874. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67572/0.70852. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67263/0.70971. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67354/0.70960. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67270/0.70982. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67284/0.70899. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66958/0.70994. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67207/0.71096. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67151/0.71095. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66934/0.70937. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66958/0.70848. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67027/0.70890. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66709/0.71027. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66760/0.71054. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66827/0.71353. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66706/0.71176. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66754/0.71152. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66409/0.71258. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66562/0.71055. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66387/0.71229. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66225/0.71281. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66087/0.71387. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66195/0.71065. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66369/0.71216. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66220/0.71271. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66275/0.71269. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66050/0.71379. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66114/0.71415. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65957/0.71470. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65581/0.71483. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66047/0.71439. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65914/0.71531. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65620/0.71668. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65506/0.71709. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65825/0.71713. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65490/0.71736. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65340/0.71718. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65182/0.71900. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64957/0.71771. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65210/0.71877. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65185/0.72034. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65203/0.72093. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64930/0.72081. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64879/0.72058. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64801/0.72179. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64613/0.72495. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64754/0.72371. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64499/0.72516. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64433/0.72454. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64753/0.72225. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64515/0.72521. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64190/0.72338. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64111/0.72281. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64563/0.72294. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64430/0.72607. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64125/0.72470. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63889/0.72506. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63691/0.72541. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63732/0.72517. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63853/0.72578. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64037/0.72671. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63913/0.72876. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69318/0.69258. Took 0.30 sec\n",
      "Epoch 1, Loss(train/val) 0.69268/0.68974. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69239/0.68880. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69156/0.68868. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69189/0.68837. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69165/0.68819. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69106/0.68790. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69063/0.68782. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69028/0.68766. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69072/0.68771. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69058/0.68795. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69035/0.68851. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69018/0.68831. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.69003/0.68836. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68987/0.68827. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69053/0.68906. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68985/0.68986. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68866/0.68953. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68870/0.68929. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68890/0.69008. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68768/0.69072. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68826/0.69070. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68638/0.69161. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68708/0.69219. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68714/0.69336. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68537/0.69396. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68626/0.69384. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68418/0.69448. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68421/0.69354. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68446/0.69426. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68364/0.69607. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68176/0.69653. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68131/0.69823. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68178/0.69866. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68404/0.69927. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68065/0.69916. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68032/0.70021. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68045/0.69988. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67905/0.70072. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67976/0.70025. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67840/0.70114. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67577/0.70214. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67537/0.70275. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67464/0.70286. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67509/0.70387. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67687/0.70485. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67349/0.70598. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67404/0.70687. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67223/0.70672. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67250/0.70860. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67182/0.70775. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67114/0.70948. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66962/0.70797. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67086/0.70863. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66955/0.70899. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67017/0.70931. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66770/0.71202. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66835/0.71046. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66477/0.71174. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66596/0.71152. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66516/0.70972. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66300/0.71260. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66489/0.71155. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66373/0.71302. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66116/0.71390. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66095/0.71581. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66074/0.71607. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65686/0.71873. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66063/0.71493. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65817/0.71666. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65758/0.71813. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66003/0.71748. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65605/0.71772. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65528/0.71891. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65171/0.72157. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65377/0.72411. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65260/0.72368. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65269/0.71985. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65191/0.71860. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65069/0.72086. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64908/0.72001. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65146/0.71831. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64971/0.72095. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64061/0.72069. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64857/0.71972. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64490/0.72209. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64600/0.72133. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64760/0.71991. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64437/0.71735. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64107/0.72443. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64246/0.72263. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64407/0.72507. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64225/0.72665. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64173/0.72802. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63989/0.72243. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63936/0.72441. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63913/0.72540. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63781/0.72793. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63986/0.72082. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63523/0.73081. Took 0.18 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69282/0.70279. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69264/0.70195. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69263/0.70142. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69205/0.70102. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69150/0.70066. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69186/0.70023. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69092/0.69974. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69114/0.69942. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69127/0.69916. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69112/0.69902. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69175/0.69879. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69130/0.69879. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.69065/0.69839. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69097/0.69816. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68938/0.69780. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69052/0.69783. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.69012/0.69778. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69098/0.69753. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.69117/0.69757. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.69024/0.69728. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68961/0.69709. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68994/0.69689. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68927/0.69680. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68889/0.69589. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68893/0.69495. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68781/0.69498. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68850/0.69513. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68988/0.69474. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68773/0.69429. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68574/0.69391. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68727/0.69373. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68534/0.69382. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68678/0.69498. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68509/0.69368. Took 0.22 sec\n",
      "Epoch 34, Loss(train/val) 0.68452/0.69350. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68389/0.69291. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.68255/0.69260. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68368/0.69343. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68311/0.69328. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68222/0.69223. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68046/0.69303. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68111/0.69294. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68138/0.69248. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67907/0.69358. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67841/0.69263. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67828/0.69115. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67671/0.69192. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67586/0.69196. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67608/0.69200. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67245/0.69157. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67255/0.69267. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67241/0.69264. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67180/0.69454. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67015/0.69314. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66746/0.69237. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66533/0.69293. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66490/0.69219. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66301/0.69213. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66387/0.69469. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.66448/0.69121. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66315/0.69394. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66027/0.69266. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.65693/0.69675. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 0.65933/0.69486. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.65527/0.69510. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65339/0.69543. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65164/0.69908. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65148/0.69637. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65098/0.69717. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 0.65035/0.69821. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65166/0.69667. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 0.64520/0.69989. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.64482/0.69884. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64431/0.69625. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.64234/0.69785. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64027/0.70231. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63738/0.70379. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64085/0.70104. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64238/0.70127. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63525/0.70334. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.63492/0.70409. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63569/0.70476. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.63445/0.70671. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63064/0.70509. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62969/0.70718. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.63222/0.70712. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63317/0.70660. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62782/0.71109. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62975/0.70989. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.62673/0.71050. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62593/0.71158. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62707/0.70946. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62582/0.70980. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62351/0.71144. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62206/0.71372. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61277/0.71547. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62228/0.71537. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61469/0.71703. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61959/0.71605. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61368/0.71727. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69449/0.69364. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69344/0.69378. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69243/0.69378. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69186/0.69373. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69198/0.69376. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69094/0.69375. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69129/0.69370. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69072/0.69387. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69090/0.69376. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69103/0.69372. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69030/0.69358. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68980/0.69354. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68928/0.69372. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68934/0.69356. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68789/0.69347. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68866/0.69341. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68836/0.69319. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68695/0.69289. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68769/0.69302. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68627/0.69301. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68630/0.69304. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68648/0.69286. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68475/0.69231. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68423/0.69205. Took 0.22 sec\n",
      "Epoch 24, Loss(train/val) 0.68434/0.69260. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68303/0.69229. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68290/0.69247. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68250/0.69278. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68203/0.69301. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68168/0.69309. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67895/0.69290. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67815/0.69316. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67774/0.69319. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67899/0.69421. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67603/0.69446. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67568/0.69431. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67701/0.69462. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67303/0.69495. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67419/0.69679. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67416/0.69777. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67315/0.69810. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67092/0.69912. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67057/0.70002. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67070/0.70052. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66804/0.70167. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66955/0.70238. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66798/0.70269. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66759/0.70355. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66462/0.70435. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66534/0.70640. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66465/0.70735. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66122/0.70858. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66102/0.71012. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66453/0.71125. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65923/0.71114. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66085/0.71267. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.65907/0.71400. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65704/0.71493. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65987/0.71592. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65323/0.71766. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65299/0.71863. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65438/0.71953. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65381/0.71986. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65299/0.72072. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65379/0.72122. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65145/0.72288. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64764/0.72629. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65171/0.72585. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64741/0.72854. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64809/0.72837. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64716/0.72727. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64632/0.72944. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.64725/0.73011. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64307/0.73030. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64466/0.73264. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64442/0.73437. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64243/0.73284. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64147/0.73491. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63895/0.73577. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63850/0.73679. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.63902/0.73988. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64234/0.74098. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63547/0.74220. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63430/0.74067. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63576/0.74143. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63715/0.74386. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63394/0.74226. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63501/0.74367. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63251/0.74447. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63406/0.74653. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63013/0.75015. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62915/0.74866. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62919/0.74769. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62933/0.74729. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62575/0.74801. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62857/0.74770. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62491/0.74709. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62098/0.74748. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62064/0.75009. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62312/0.75444. Took 0.19 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69434/0.69491. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69410/0.69409. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69306/0.69363. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69277/0.69324. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69320/0.69298. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69263/0.69283. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69229/0.69266. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69183/0.69230. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69195/0.69206. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69148/0.69173. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69113/0.69157. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69118/0.69121. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.69132/0.69089. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69021/0.69050. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68997/0.69000. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68982/0.68990. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68893/0.69002. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68924/0.68934. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68884/0.68923. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68857/0.68898. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68794/0.68863. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68843/0.68851. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68752/0.68837. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68711/0.68840. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68572/0.68860. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68616/0.68879. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68515/0.68846. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68502/0.68854. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68467/0.68906. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68428/0.68895. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68330/0.68918. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68315/0.68952. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68264/0.68955. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68129/0.69030. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68062/0.69061. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67990/0.69125. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68073/0.69236. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67883/0.69249. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67762/0.69295. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67923/0.69372. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67742/0.69427. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67773/0.69603. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67716/0.69615. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67562/0.69584. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67535/0.69656. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67301/0.69745. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67448/0.69751. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67274/0.69763. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67236/0.69849. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67097/0.69966. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67120/0.69999. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67221/0.70043. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67007/0.70153. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66944/0.70241. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66804/0.70373. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66666/0.70527. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66699/0.70576. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66491/0.70751. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66445/0.70611. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66595/0.70719. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66499/0.70685. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66482/0.70559. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66489/0.70906. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66161/0.71128. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66092/0.70962. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65727/0.70927. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65947/0.71113. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65930/0.71205. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65523/0.71362. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65845/0.71569. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65570/0.71622. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65525/0.71515. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65613/0.71516. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65614/0.71748. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65483/0.71575. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65026/0.71514. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65240/0.71711. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65135/0.71620. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65089/0.71737. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65037/0.71796. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65025/0.72077. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64625/0.72211. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64665/0.72166. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64421/0.72261. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64312/0.72312. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64509/0.72455. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64567/0.72332. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64383/0.72548. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64267/0.72663. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64151/0.72625. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63944/0.72495. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63988/0.72815. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64029/0.72932. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63781/0.72912. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63393/0.73419. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63547/0.73580. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63411/0.73584. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63373/0.73314. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63458/0.73718. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63230/0.73495. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69563/0.69390. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69337/0.69312. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69383/0.69274. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69234/0.69242. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69296/0.69209. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69248/0.69174. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69255/0.69135. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69226/0.69092. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69143/0.69066. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69260/0.69031. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69216/0.68994. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69141/0.68970. Took 0.22 sec\n",
      "Epoch 12, Loss(train/val) 0.69124/0.68944. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69141/0.68910. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.69173/0.68860. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69135/0.68828. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.69102/0.68805. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.69156/0.68752. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.69075/0.68729. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.69064/0.68696. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.69024/0.68657. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69001/0.68614. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.69017/0.68569. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68942/0.68519. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.69025/0.68503. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68967/0.68472. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68809/0.68440. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68813/0.68369. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68813/0.68324. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68732/0.68276. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68697/0.68212. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68597/0.68145. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.68788/0.68086. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68596/0.68045. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68605/0.67986. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68550/0.67946. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68570/0.67924. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68475/0.67820. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68452/0.67764. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68401/0.67673. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68307/0.67662. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68278/0.67604. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68220/0.67600. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68124/0.67491. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68335/0.67482. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68151/0.67427. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68246/0.67382. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68184/0.67371. Took 0.22 sec\n",
      "Epoch 48, Loss(train/val) 0.68093/0.67332. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68237/0.67352. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67985/0.67339. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67811/0.67322. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.68099/0.67279. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67724/0.67329. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67902/0.67358. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67851/0.67360. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67720/0.67332. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67811/0.67385. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67517/0.67364. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67628/0.67340. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67624/0.67390. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67467/0.67411. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.67710/0.67439. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67326/0.67562. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67270/0.67595. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67261/0.67583. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67194/0.67590. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67311/0.67613. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67192/0.67804. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67048/0.67806. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66948/0.67899. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66977/0.67975. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66829/0.68081. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66719/0.68091. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66784/0.68196. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66488/0.68343. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66702/0.68463. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66653/0.68511. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66520/0.68482. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66520/0.68535. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66338/0.68623. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66315/0.68752. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66283/0.68820. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66355/0.68955. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66187/0.69139. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66178/0.69031. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65819/0.69170. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65915/0.69276. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.66004/0.69275. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65617/0.69323. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.65536/0.69519. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65622/0.69565. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65396/0.69832. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65302/0.69829. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65730/0.69488. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65487/0.69863. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65233/0.69960. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64964/0.70208. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65123/0.70180. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65255/0.70224. Took 0.19 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69538/0.69550. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69307/0.69384. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69360/0.69286. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69353/0.69309. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69242/0.69294. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69231/0.69347. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69235/0.69364. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69168/0.69385. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69111/0.69396. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69164/0.69441. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69142/0.69485. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69156/0.69473. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69081/0.69513. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69166/0.69576. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69056/0.69621. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69064/0.69630. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69044/0.69687. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69044/0.69708. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69076/0.69739. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.69058/0.69799. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68958/0.69857. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68883/0.69859. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68874/0.69905. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68934/0.69891. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68856/0.69963. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68784/0.70009. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68709/0.70044. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68760/0.70213. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68724/0.70221. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68660/0.70301. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68564/0.70344. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68629/0.70432. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68417/0.70453. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68448/0.70514. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68332/0.70570. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68347/0.70780. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68187/0.70775. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68142/0.70885. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67947/0.71017. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67990/0.71107. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68169/0.71238. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67811/0.71365. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67764/0.71424. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67627/0.71595. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67407/0.71716. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67710/0.71788. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67557/0.71809. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67348/0.71780. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67073/0.71995. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67141/0.72082. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66935/0.72126. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66833/0.72227. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66773/0.72293. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66501/0.72236. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66626/0.72215. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66709/0.72342. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66096/0.72409. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66371/0.72514. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66336/0.72680. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65999/0.72641. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65982/0.72611. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65799/0.72490. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65832/0.72592. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65811/0.72838. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65264/0.72729. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65415/0.72997. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64887/0.73024. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65145/0.72838. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64806/0.72982. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64701/0.73090. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64590/0.73115. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64605/0.73245. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64093/0.73231. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64218/0.73838. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64364/0.73337. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64021/0.73389. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63648/0.73858. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64379/0.74061. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63671/0.73885. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64059/0.74255. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.63142/0.74242. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63536/0.74171. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63357/0.74601. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62951/0.74591. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62710/0.74509. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63073/0.74134. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62732/0.74746. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62720/0.74531. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62368/0.74783. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.62713/0.74908. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62585/0.74859. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62967/0.74778. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62115/0.74608. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62102/0.74957. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62085/0.75061. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62440/0.75357. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62396/0.75343. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61984/0.75937. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61502/0.75310. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61994/0.74785. Took 0.18 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69325/0.68739. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69244/0.68396. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69198/0.68333. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69186/0.68355. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69167/0.68297. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69175/0.68245. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69164/0.68270. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69034/0.68203. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69041/0.68220. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69108/0.68225. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69022/0.68203. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69027/0.68182. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69078/0.68209. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69001/0.68178. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69030/0.68230. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69015/0.68297. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68922/0.68254. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68949/0.68278. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68964/0.68209. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68843/0.68233. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68851/0.68295. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68850/0.68320. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68811/0.68290. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68886/0.68340. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68932/0.68330. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68867/0.68329. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68734/0.68354. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68767/0.68312. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68789/0.68368. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68831/0.68361. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68720/0.68270. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68761/0.68418. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68689/0.68446. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68563/0.68456. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68721/0.68452. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68542/0.68508. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68566/0.68495. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68533/0.68427. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68484/0.68464. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68512/0.68402. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68448/0.68455. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68449/0.68431. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68505/0.68553. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68337/0.68525. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68222/0.68428. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68219/0.68569. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68383/0.68461. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68520/0.68594. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68105/0.68558. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68198/0.68446. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68007/0.68649. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68171/0.68613. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68127/0.68764. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68034/0.68725. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67711/0.68621. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.68009/0.68741. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67728/0.68603. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67922/0.68762. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67772/0.68716. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67745/0.68536. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67783/0.68715. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67605/0.68835. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67499/0.68769. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67556/0.68954. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67307/0.69012. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67387/0.68964. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67199/0.69102. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66853/0.69119. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67114/0.69192. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67099/0.69176. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66997/0.69472. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66949/0.69352. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66762/0.69481. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66715/0.69584. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66619/0.69541. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66861/0.69712. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66784/0.69746. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66605/0.69963. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66353/0.69827. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66473/0.70161. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66073/0.70247. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66054/0.70544. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66126/0.70319. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66091/0.70717. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66104/0.70981. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65921/0.70708. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65794/0.70908. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65384/0.70956. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65491/0.71194. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65412/0.71531. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65104/0.71527. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65393/0.71504. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64854/0.72004. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65015/0.71891. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64554/0.72264. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64448/0.72237. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64731/0.72304. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64685/0.72673. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64267/0.72971. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64090/0.73302. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69234/0.68873. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69143/0.68837. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69197/0.68858. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69151/0.68885. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69131/0.68914. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69114/0.68931. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69148/0.68959. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69037/0.68973. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69083/0.69008. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69038/0.69030. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69025/0.69044. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68986/0.69065. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68992/0.69065. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69043/0.69083. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68944/0.69105. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68985/0.69112. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68948/0.69124. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68885/0.69128. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68859/0.69092. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68923/0.69126. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68832/0.69129. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68737/0.69125. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68780/0.69170. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68721/0.69148. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68786/0.69177. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68707/0.69186. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68626/0.69178. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68602/0.69180. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68528/0.69182. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68534/0.69202. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68484/0.69197. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68344/0.69121. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68344/0.69122. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68360/0.69068. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68297/0.69093. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68217/0.69044. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68154/0.69031. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68278/0.68989. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68071/0.69011. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68020/0.68997. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67902/0.69001. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67893/0.68971. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67712/0.68972. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67714/0.68968. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67612/0.68957. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67738/0.68902. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67611/0.68938. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67470/0.68951. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67346/0.68890. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67246/0.68938. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67093/0.69049. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67213/0.69083. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67003/0.69234. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67051/0.69175. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66980/0.69190. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66714/0.69223. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66814/0.69155. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66571/0.69226. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66457/0.69269. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66393/0.69410. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66313/0.69358. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66069/0.69534. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66212/0.69628. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65979/0.69683. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65808/0.69775. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65544/0.69822. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65518/0.69926. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.65332/0.70027. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65089/0.70108. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65250/0.70180. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65087/0.70274. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64939/0.70222. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64651/0.70481. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64544/0.70662. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64570/0.70555. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64478/0.70430. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64468/0.70553. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63915/0.70620. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64142/0.70695. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64188/0.70744. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63739/0.70783. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63777/0.70912. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63618/0.71035. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63843/0.71084. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63313/0.70964. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63534/0.71438. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62975/0.71657. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63070/0.71261. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62501/0.71839. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62862/0.71965. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62682/0.72245. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62382/0.72215. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62706/0.72239. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62099/0.72603. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.62471/0.72530. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62085/0.72465. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61845/0.72521. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61747/0.73183. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.61711/0.72827. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61566/0.72720. Took 0.20 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69358/0.69553. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69151/0.69670. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69180/0.69682. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69166/0.69694. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69113/0.69686. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69111/0.69684. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69115/0.69691. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69084/0.69703. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69103/0.69686. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69050/0.69684. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69056/0.69686. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69049/0.69665. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68980/0.69652. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69025/0.69654. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68880/0.69686. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68895/0.69672. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68923/0.69675. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68891/0.69689. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68831/0.69674. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68868/0.69673. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68911/0.69667. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68805/0.69665. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68819/0.69649. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68800/0.69680. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68757/0.69648. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68767/0.69652. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68771/0.69611. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68743/0.69624. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68699/0.69638. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68824/0.69611. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68661/0.69583. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68609/0.69569. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68510/0.69517. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68584/0.69496. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 0.68618/0.69486. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.68483/0.69493. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68403/0.69464. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68426/0.69514. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68319/0.69478. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68382/0.69421. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.68316/0.69428. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68338/0.69395. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68255/0.69416. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68267/0.69336. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67936/0.69236. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67994/0.69317. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68013/0.69209. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68065/0.69241. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68064/0.69171. Took 0.22 sec\n",
      "Epoch 49, Loss(train/val) 0.67899/0.69129. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67846/0.69118. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.67795/0.69026. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67931/0.69052. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67764/0.69069. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67560/0.69096. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67660/0.69046. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67433/0.68958. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67564/0.69020. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67418/0.69090. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67373/0.69109. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67371/0.69042. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67304/0.69001. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66955/0.68986. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67058/0.68908. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66966/0.68966. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67043/0.69057. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66746/0.69010. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66837/0.69059. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66623/0.69115. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66778/0.69161. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66534/0.68926. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66431/0.69066. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66633/0.69216. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66710/0.69082. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66429/0.69142. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66349/0.69239. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66156/0.69251. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66142/0.69089. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66132/0.69064. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65871/0.69303. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66275/0.69215. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65861/0.69255. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65866/0.69357. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65799/0.69228. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65565/0.69274. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65572/0.69193. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65665/0.69529. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65697/0.69350. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65360/0.69548. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65674/0.69419. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65543/0.69576. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65339/0.69382. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65218/0.69272. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65256/0.69327. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65038/0.69431. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64872/0.69740. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65064/0.69536. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64911/0.69940. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64991/0.69705. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64910/0.69843. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69382/0.70404. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69088/0.70184. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68951/0.70333. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68844/0.70444. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68888/0.70509. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68873/0.70540. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68830/0.70627. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68796/0.70671. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68811/0.70724. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68795/0.70754. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68754/0.70841. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68779/0.70867. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68731/0.70852. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68765/0.70900. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68693/0.70926. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68763/0.70912. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68746/0.70981. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68669/0.71007. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68701/0.71016. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68603/0.71106. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68632/0.71086. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68585/0.71119. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68559/0.71173. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68532/0.71229. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68602/0.71201. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68460/0.71280. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68449/0.71249. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68455/0.71267. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68489/0.71326. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68383/0.71387. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68451/0.71404. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68387/0.71447. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68250/0.71441. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68226/0.71475. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68224/0.71477. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68161/0.71526. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68147/0.71546. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68252/0.71569. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68046/0.71577. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68002/0.71635. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68023/0.71634. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67877/0.71740. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67874/0.71750. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67951/0.71761. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67791/0.71796. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67634/0.71833. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67584/0.71808. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67662/0.71861. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67531/0.72024. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67558/0.71938. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67311/0.71993. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67408/0.71994. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67291/0.72088. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67173/0.72169. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67210/0.71971. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67048/0.72154. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67006/0.72140. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66825/0.72175. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66903/0.72264. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66638/0.72362. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66653/0.72346. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66679/0.72529. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66607/0.72434. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66590/0.72453. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66436/0.72470. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66373/0.72432. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66030/0.72652. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65942/0.72664. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66039/0.72614. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65870/0.72836. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65831/0.72997. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66039/0.72765. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65682/0.73118. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65704/0.72949. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65579/0.73244. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65437/0.72919. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65394/0.73308. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65243/0.73279. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65213/0.73400. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65302/0.73266. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65136/0.73633. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64876/0.73689. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64932/0.73566. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64775/0.73476. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64270/0.73797. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64398/0.73926. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64875/0.73906. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64251/0.74033. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64328/0.73741. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64124/0.73956. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64242/0.74148. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63640/0.74179. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63785/0.74441. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63815/0.74563. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63774/0.74789. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63506/0.74651. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63448/0.74753. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63651/0.74653. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63349/0.75080. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63205/0.75154. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69226/0.69854. Took 0.39 sec\n",
      "Epoch 1, Loss(train/val) 0.68903/0.69823. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68802/0.69797. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68808/0.69764. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68688/0.69724. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68646/0.69713. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68579/0.69698. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68486/0.69689. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68593/0.69684. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68416/0.69688. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68381/0.69712. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68433/0.69699. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68263/0.69688. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68264/0.69704. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68237/0.69710. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68069/0.69664. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68088/0.69686. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67965/0.69688. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68091/0.69677. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68002/0.69676. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67846/0.69688. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67857/0.69736. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67754/0.69813. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67709/0.69883. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67554/0.69916. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67512/0.69941. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67342/0.70052. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67482/0.70182. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67472/0.70177. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67331/0.70243. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.66993/0.70469. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67086/0.70479. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66833/0.70508. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66890/0.70603. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66781/0.70842. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66863/0.70769. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66594/0.70932. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66442/0.70990. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66261/0.71061. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66271/0.71170. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66076/0.71277. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66137/0.71421. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66069/0.71796. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.65762/0.71766. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65585/0.71933. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65589/0.72103. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65344/0.72028. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65374/0.72354. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65364/0.72364. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.65206/0.72461. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65035/0.72760. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65387/0.73040. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.64768/0.72878. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64843/0.72806. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64817/0.73514. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64646/0.73467. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64348/0.73674. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64571/0.73392. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64185/0.73326. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.64642/0.73676. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.63858/0.73854. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64218/0.73772. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64106/0.74374. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.64191/0.74197. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63609/0.73759. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64062/0.74520. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63762/0.74256. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63462/0.74586. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63588/0.74377. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 0.63268/0.74412. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63485/0.74943. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63217/0.74743. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63213/0.75440. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62933/0.75054. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63049/0.75343. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.62793/0.75121. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62446/0.75244. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62730/0.74816. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62551/0.75086. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62252/0.75150. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62206/0.75329. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.62057/0.75542. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62468/0.75686. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61627/0.75057. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62058/0.75271. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61777/0.75099. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61557/0.75318. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61942/0.75008. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61177/0.75502. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61329/0.75526. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61013/0.75718. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61444/0.75362. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61121/0.75666. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.61278/0.76023. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60763/0.75884. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60731/0.75853. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.60417/0.75939. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60510/0.75934. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60750/0.75896. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.60356/0.75477. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69383/0.69441. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68910/0.69675. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68791/0.69773. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68815/0.69796. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68763/0.69814. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68715/0.69838. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68678/0.69860. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68714/0.69870. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68657/0.69880. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68631/0.69898. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68623/0.69904. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68618/0.69906. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68409/0.69943. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68532/0.69959. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68576/0.69992. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68346/0.70019. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68454/0.70045. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68330/0.70092. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68325/0.70137. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68370/0.70141. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68285/0.70199. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68279/0.70227. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68160/0.70303. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68269/0.70311. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68150/0.70373. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67980/0.70424. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68111/0.70404. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68091/0.70526. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67967/0.70485. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67940/0.70528. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67876/0.70520. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67919/0.70627. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67870/0.70692. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67880/0.70667. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67786/0.70693. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67615/0.70802. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67686/0.70832. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67517/0.70850. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67614/0.70921. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67702/0.70919. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67591/0.70975. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67581/0.71025. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67294/0.71063. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67111/0.71070. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67428/0.71178. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67237/0.71173. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67064/0.71259. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67292/0.71334. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67141/0.71418. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66974/0.71380. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67091/0.71357. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66878/0.71447. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66707/0.71561. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66656/0.71618. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66615/0.71579. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66425/0.71702. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66477/0.71858. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66426/0.71867. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66338/0.71796. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66405/0.71938. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 0.65869/0.72057. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65941/0.72125. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65689/0.72214. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65818/0.72275. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65852/0.72490. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65500/0.72694. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65598/0.72764. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65437/0.72947. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65284/0.72875. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65227/0.73096. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65168/0.72990. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65033/0.73302. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64841/0.73223. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64611/0.73686. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64626/0.74068. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64730/0.74206. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64544/0.74353. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64338/0.74657. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64440/0.74705. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64094/0.74737. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64153/0.75012. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64139/0.74956. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63829/0.75403. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63294/0.75179. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63492/0.75878. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63092/0.75813. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63581/0.75837. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63219/0.76122. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62979/0.76267. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62829/0.76497. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62624/0.76655. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62576/0.76953. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62192/0.77323. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62470/0.77620. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62280/0.77521. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62203/0.77953. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61943/0.77598. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62000/0.78251. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61927/0.78466. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61170/0.78128. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69176/0.71064. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68985/0.71251. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69055/0.71306. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68943/0.71326. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68874/0.71430. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68856/0.71452. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68895/0.71504. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68775/0.71553. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68731/0.71577. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68651/0.71658. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68716/0.71760. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68604/0.71763. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68625/0.71803. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68672/0.71812. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68575/0.71811. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68577/0.71847. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68571/0.71893. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68537/0.71933. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68509/0.71930. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68509/0.71940. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68491/0.71975. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68523/0.72018. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68443/0.72109. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68503/0.72034. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68387/0.72084. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68330/0.72107. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68349/0.72135. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68265/0.72260. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68264/0.72342. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68262/0.72277. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68311/0.72316. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68213/0.72311. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68218/0.72327. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68153/0.72332. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68203/0.72412. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68079/0.72515. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68078/0.72609. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68019/0.72646. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67982/0.72603. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68036/0.72643. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67906/0.72667. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67850/0.72828. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67879/0.72809. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67826/0.72999. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67848/0.73002. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67727/0.72957. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67656/0.73095. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67739/0.73210. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67548/0.73215. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67554/0.73373. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67631/0.73390. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67444/0.73550. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67485/0.73532. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67298/0.73527. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67388/0.73739. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67355/0.73770. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67330/0.73836. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67281/0.73992. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67208/0.73999. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67215/0.74121. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67100/0.74020. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67139/0.74337. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67052/0.74319. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66947/0.74475. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67197/0.74693. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66730/0.74724. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66971/0.74979. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66947/0.74979. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66823/0.75024. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66705/0.75236. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66684/0.75264. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66585/0.75494. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66674/0.75540. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66617/0.75835. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66514/0.75749. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66528/0.75737. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66423/0.75947. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66269/0.76087. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66277/0.76114. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66214/0.76246. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66294/0.76396. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66205/0.76393. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66196/0.76528. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66244/0.76545. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66259/0.76951. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65886/0.77014. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65905/0.77008. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65718/0.77303. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65700/0.77273. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65638/0.77423. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65583/0.77408. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65572/0.77778. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65703/0.77910. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65792/0.77906. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65625/0.77859. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65879/0.78050. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65342/0.78275. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65332/0.78339. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65179/0.78276. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64944/0.78683. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69478/0.69743. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69095/0.69663. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68964/0.69671. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68914/0.69695. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68922/0.69715. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68870/0.69749. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68886/0.69772. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68816/0.69820. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68777/0.69880. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68765/0.69923. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68714/0.69965. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68751/0.70011. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68630/0.70068. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68597/0.70125. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68597/0.70171. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68537/0.70250. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68609/0.70320. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68555/0.70356. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68492/0.70420. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68468/0.70470. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68532/0.70508. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68389/0.70568. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68352/0.70616. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68329/0.70643. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68368/0.70683. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68292/0.70737. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68367/0.70775. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68337/0.70824. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68206/0.70864. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68164/0.70915. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68195/0.70948. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68125/0.70986. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68209/0.71048. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68098/0.71073. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68009/0.71038. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68013/0.71024. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67943/0.71047. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68061/0.71032. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67962/0.71024. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67897/0.71089. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67812/0.71132. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67855/0.71241. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67827/0.71275. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67850/0.71250. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67696/0.71278. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67787/0.71298. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67691/0.71357. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67630/0.71336. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67603/0.71374. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67454/0.71473. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67556/0.71572. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67462/0.71547. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67480/0.71531. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67442/0.71540. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67503/0.71683. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67136/0.71740. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67198/0.71762. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67226/0.71864. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67194/0.71878. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67036/0.71843. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67169/0.71779. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67044/0.71800. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67168/0.71724. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66913/0.71863. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66886/0.71940. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66980/0.71901. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66707/0.71934. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66616/0.72044. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66703/0.72139. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66621/0.72204. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66614/0.72228. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66805/0.72324. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66270/0.72350. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66272/0.72361. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66344/0.72377. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66207/0.72242. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66263/0.72377. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66200/0.72228. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66191/0.72113. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66229/0.72277. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66116/0.72410. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65746/0.72544. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66009/0.72565. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65849/0.72756. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65996/0.72629. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65569/0.72480. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65358/0.72658. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65503/0.72823. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65479/0.72956. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65385/0.72740. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65158/0.72783. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64979/0.72911. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65124/0.72946. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64808/0.72751. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64778/0.73066. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64761/0.72937. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64644/0.73271. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64749/0.73156. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64472/0.73026. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64038/0.73261. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69340/0.69548. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68996/0.69785. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68938/0.69939. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68796/0.70032. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68776/0.70091. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68714/0.70082. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68645/0.70057. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68730/0.70048. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68674/0.70040. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68574/0.70071. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68518/0.70061. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68424/0.70092. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68421/0.70095. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68425/0.70115. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68370/0.70133. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68307/0.70196. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68380/0.70176. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68315/0.70206. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68118/0.70233. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68137/0.70299. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68074/0.70373. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68314/0.70298. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68130/0.70315. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68104/0.70289. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68050/0.70275. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67926/0.70360. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68060/0.70388. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68034/0.70266. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68010/0.70239. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68019/0.70307. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67835/0.70313. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68001/0.70280. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68129/0.70185. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67945/0.70236. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67936/0.70228. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.67887/0.70251. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67809/0.70267. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67885/0.70290. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67839/0.70248. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67794/0.70242. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67707/0.70284. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67639/0.70248. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67754/0.70197. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67745/0.70217. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67775/0.70179. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67639/0.70275. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67649/0.70241. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67781/0.70236. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67753/0.70228. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67569/0.70140. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67727/0.70154. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67565/0.70137. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67631/0.70187. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67440/0.70121. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67388/0.70183. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67539/0.70195. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67381/0.70183. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67500/0.70221. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67382/0.70204. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67401/0.70200. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67287/0.70277. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67343/0.70282. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67276/0.70285. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67321/0.70378. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67256/0.70349. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.67206/0.70403. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67036/0.70397. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67187/0.70346. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67221/0.70418. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67408/0.70443. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67130/0.70477. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67085/0.70454. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67031/0.70594. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66911/0.70512. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67001/0.70568. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66747/0.70582. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66802/0.70609. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66676/0.70715. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66615/0.70838. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66672/0.70691. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66792/0.70737. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66524/0.70820. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66705/0.70909. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66530/0.71081. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66428/0.70955. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66660/0.71016. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66481/0.71134. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66340/0.71113. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66226/0.71368. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66435/0.71279. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66140/0.71291. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66346/0.71254. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66007/0.71345. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66037/0.71407. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66143/0.71506. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65913/0.71618. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65764/0.71562. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65839/0.71743. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65721/0.71855. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65641/0.71841. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69338/0.69018. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69146/0.68903. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69073/0.68816. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69026/0.68744. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68972/0.68702. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68917/0.68668. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68907/0.68639. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68749/0.68606. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68834/0.68604. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68650/0.68584. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68605/0.68556. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68637/0.68534. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68561/0.68528. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68454/0.68531. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68398/0.68549. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68339/0.68561. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68357/0.68566. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68209/0.68567. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68198/0.68590. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68237/0.68627. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68136/0.68610. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68058/0.68644. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67988/0.68642. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68091/0.68635. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68082/0.68698. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67863/0.68682. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67828/0.68685. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67707/0.68686. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67659/0.68692. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67489/0.68716. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67642/0.68776. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67456/0.68755. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67366/0.68742. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67143/0.68731. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67238/0.68779. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.66963/0.68805. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67154/0.68807. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66892/0.68914. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66838/0.68742. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66857/0.68939. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66769/0.68904. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66738/0.69016. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66561/0.68929. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66389/0.68803. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66349/0.68887. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66403/0.68882. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66330/0.68911. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66131/0.68877. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66069/0.68827. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66014/0.68811. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65647/0.68821. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65916/0.68792. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65778/0.68847. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65844/0.68918. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65818/0.68888. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65322/0.68885. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65699/0.68791. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65308/0.68820. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65251/0.68717. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65410/0.68845. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65134/0.68859. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65422/0.68729. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65019/0.68738. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.64554/0.68769. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65077/0.68672. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64618/0.68647. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64882/0.68668. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64369/0.68715. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64393/0.68787. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64632/0.68728. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64291/0.68769. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64281/0.68798. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64164/0.68586. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.63996/0.68660. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64350/0.68654. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64048/0.68616. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63479/0.68525. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.63749/0.68508. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63296/0.68538. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63295/0.68628. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63597/0.68531. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63396/0.68678. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63083/0.68609. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62913/0.68516. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63038/0.68634. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63129/0.68574. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62951/0.68310. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62638/0.68541. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62471/0.68278. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.62585/0.68539. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62770/0.68694. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62225/0.68514. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61998/0.68817. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62056/0.68653. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62373/0.68403. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62344/0.68732. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61921/0.68738. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61841/0.68853. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62052/0.68807. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61609/0.68762. Took 0.18 sec\n",
      "ACC: 0.6354166666666666\n",
      "Epoch 0, Loss(train/val) 0.68955/0.67634. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68943/0.67628. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68835/0.67699. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68840/0.67819. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68758/0.67863. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68673/0.67930. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68671/0.67966. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68728/0.68056. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68658/0.68159. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68610/0.68222. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68601/0.68275. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68538/0.68311. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68538/0.68422. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68541/0.68502. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68498/0.68492. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68528/0.68576. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68470/0.68711. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68406/0.68652. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68410/0.68721. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68421/0.68843. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68272/0.68843. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68357/0.68898. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68312/0.68940. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68274/0.69015. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68245/0.69067. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68183/0.69127. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68157/0.69151. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68127/0.69185. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68211/0.69323. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68257/0.69390. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68053/0.69307. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68176/0.69328. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68031/0.69460. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67983/0.69586. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67987/0.69637. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67927/0.69686. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68061/0.69645. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67882/0.69638. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67786/0.69901. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67730/0.69736. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67825/0.69877. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67721/0.69917. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67837/0.69911. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67613/0.69932. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67613/0.70071. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67687/0.70070. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67612/0.70140. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67617/0.70159. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67511/0.70312. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67635/0.70304. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67555/0.70496. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67542/0.70470. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67306/0.70450. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67260/0.70498. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67227/0.70503. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67253/0.70607. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67247/0.70607. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67134/0.70699. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67097/0.70752. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67182/0.70718. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67048/0.70788. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66960/0.70806. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66891/0.70915. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66781/0.70919. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66760/0.71043. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66890/0.70946. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66763/0.71149. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66830/0.71026. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66597/0.70954. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66640/0.71167. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66544/0.71123. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66380/0.71172. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66657/0.71443. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66440/0.71303. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66310/0.71555. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66364/0.71399. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66063/0.71490. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66224/0.71807. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66131/0.71529. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66072/0.71529. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65870/0.71753. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65830/0.71606. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65919/0.71486. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65825/0.71485. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65797/0.71795. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66007/0.71771. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65927/0.71744. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65498/0.71965. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65822/0.71873. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65633/0.72015. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65412/0.72130. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65431/0.72148. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65380/0.72033. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65015/0.72413. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65304/0.72358. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65194/0.72158. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65027/0.72372. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65152/0.72317. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65174/0.72453. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64873/0.72728. Took 0.18 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69157/0.70779. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68923/0.71199. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68982/0.71227. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68832/0.71258. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.68746/0.71283. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68851/0.71295. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68782/0.71271. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68681/0.71247. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68778/0.71209. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68649/0.71279. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68666/0.71236. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68600/0.71237. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68568/0.71444. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68576/0.71419. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68477/0.71397. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68526/0.71463. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68483/0.71551. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68564/0.71517. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68459/0.71623. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68425/0.71613. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68496/0.71593. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68377/0.71748. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68318/0.71653. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68339/0.71866. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68265/0.71875. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68429/0.71919. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68332/0.71893. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68296/0.71929. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68189/0.71944. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68114/0.72089. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68186/0.72234. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68141/0.72216. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68223/0.72144. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68152/0.72266. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68103/0.72401. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68078/0.72265. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68145/0.72407. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68129/0.72594. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67966/0.72595. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68007/0.72634. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68094/0.72564. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67920/0.72679. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68033/0.72727. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67890/0.72703. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68009/0.72813. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67987/0.72849. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67871/0.72958. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67927/0.72953. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67851/0.73077. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67869/0.73065. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 0.67875/0.73108. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67801/0.73315. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67833/0.73258. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67606/0.73310. Took 0.21 sec\n",
      "Epoch 54, Loss(train/val) 0.67655/0.73312. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67770/0.73552. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67795/0.73464. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67657/0.73576. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67534/0.73666. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67534/0.73809. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67698/0.73722. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67450/0.73841. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.67495/0.73796. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67462/0.73978. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67574/0.73983. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67337/0.74019. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67388/0.74178. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67370/0.74032. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67272/0.74285. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67220/0.74301. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67390/0.74349. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67082/0.74272. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.67169/0.74523. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67069/0.74707. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67167/0.74304. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66994/0.74581. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66899/0.74925. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66977/0.74736. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66920/0.75120. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66813/0.75163. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66629/0.75072. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66570/0.75147. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66881/0.75397. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66633/0.75252. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66741/0.75422. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66762/0.75298. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66521/0.75557. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66268/0.75525. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66551/0.75547. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66484/0.75486. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66231/0.75737. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66265/0.76051. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.66274/0.76144. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66311/0.75913. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66146/0.76202. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66053/0.75991. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65967/0.76311. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65873/0.76216. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65907/0.76422. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65716/0.76754. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69390/0.69749. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69266/0.69598. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69176/0.69450. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69154/0.69336. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69104/0.69251. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69007/0.69169. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69055/0.69089. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68999/0.69026. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68957/0.68980. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68960/0.68935. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68940/0.68898. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68886/0.68871. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68888/0.68834. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68952/0.68811. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68905/0.68776. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68798/0.68751. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68820/0.68746. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68903/0.68751. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68869/0.68739. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68805/0.68745. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68846/0.68727. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68792/0.68736. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68783/0.68735. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68727/0.68754. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68799/0.68760. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68718/0.68764. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68729/0.68758. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68642/0.68739. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68746/0.68727. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68687/0.68717. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68661/0.68747. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68666/0.68779. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68614/0.68773. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68571/0.68778. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68548/0.68808. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68520/0.68826. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68593/0.68823. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68655/0.68837. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68524/0.68839. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68573/0.68823. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68586/0.68835. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68441/0.68840. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68446/0.68821. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68448/0.68864. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68376/0.68866. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68368/0.68882. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68455/0.68911. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68236/0.68924. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68279/0.68917. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68382/0.68967. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68293/0.69016. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68175/0.69073. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68126/0.69124. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68209/0.69146. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68267/0.69133. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.68122/0.69114. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68098/0.69087. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.68069/0.69201. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68134/0.69252. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67806/0.69331. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.68006/0.69408. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67980/0.69485. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67856/0.69562. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67858/0.69610. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67828/0.69690. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67869/0.69748. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67619/0.69896. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67704/0.69959. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67653/0.70065. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67729/0.70137. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67629/0.70273. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67505/0.70256. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.67696/0.70240. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67509/0.70317. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67125/0.70368. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67533/0.70551. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67245/0.70663. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.67451/0.70620. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.67146/0.70654. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.67197/0.70815. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67039/0.70951. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67086/0.71085. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67032/0.71196. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67037/0.71138. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66813/0.71224. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66796/0.71250. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.67016/0.71358. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66956/0.71442. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66838/0.71452. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66969/0.71515. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66647/0.71657. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66649/0.71690. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66366/0.71837. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66319/0.71947. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66701/0.71910. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66342/0.72059. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66167/0.72255. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66277/0.72419. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66192/0.72373. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66316/0.72378. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69423/0.68880. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69351/0.68875. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69310/0.68926. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69187/0.68944. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69222/0.68960. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69215/0.68997. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69146/0.69024. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69073/0.69062. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69082/0.69088. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69073/0.69105. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68960/0.69169. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68974/0.69233. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68960/0.69267. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68982/0.69274. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68881/0.69270. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68865/0.69290. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68912/0.69356. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68851/0.69376. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68941/0.69413. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68849/0.69436. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68859/0.69483. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68826/0.69476. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68846/0.69438. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68712/0.69532. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68762/0.69506. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68728/0.69570. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68790/0.69607. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68823/0.69656. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68700/0.69661. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68740/0.69655. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68600/0.69708. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68772/0.69696. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68632/0.69772. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68620/0.69839. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68523/0.69887. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68587/0.69887. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68480/0.69878. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68487/0.69929. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68452/0.70004. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68464/0.70046. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68397/0.70006. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68426/0.70028. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68372/0.70094. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68365/0.70090. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68323/0.70134. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68324/0.70270. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68205/0.70384. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68232/0.70366. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68251/0.70397. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68134/0.70267. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68328/0.70361. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68124/0.70338. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68234/0.70312. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68253/0.70340. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67929/0.70414. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68102/0.70439. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.68058/0.70517. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.68009/0.70496. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67886/0.70530. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.68052/0.70600. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67922/0.70582. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67887/0.70615. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.68004/0.70632. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.68029/0.70690. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67948/0.70613. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67962/0.70624. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67680/0.70678. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67914/0.70611. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67694/0.70642. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67960/0.70646. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67838/0.70699. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67582/0.70777. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67793/0.70804. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67634/0.70858. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67650/0.70897. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67539/0.70945. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67367/0.70935. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67492/0.71025. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67434/0.71004. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67312/0.71026. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67366/0.70986. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67267/0.71149. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.67339/0.71151. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67225/0.71238. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.67423/0.71106. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.67422/0.71141. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.67216/0.71163. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.67326/0.71108. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.67303/0.71055. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.67332/0.71070. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.67170/0.71118. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.67201/0.71164. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.67058/0.71119. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.67060/0.71203. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.67014/0.71254. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66991/0.71246. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.67067/0.71253. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.67254/0.71286. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66864/0.71208. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.67078/0.71190. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69417/0.69483. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69373/0.69427. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69328/0.69396. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69284/0.69362. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69265/0.69333. Took 0.22 sec\n",
      "Epoch 5, Loss(train/val) 0.69243/0.69304. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69234/0.69269. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69201/0.69253. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69170/0.69234. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69111/0.69215. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69130/0.69183. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69026/0.69158. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.68951/0.69127. Took 0.22 sec\n",
      "Epoch 13, Loss(train/val) 0.68940/0.69075. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68927/0.69040. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68879/0.69018. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68773/0.68972. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68757/0.68929. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68704/0.68906. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68708/0.68861. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68608/0.68837. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68623/0.68796. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68483/0.68746. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68591/0.68722. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68511/0.68702. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68397/0.68701. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68471/0.68655. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68402/0.68634. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68320/0.68602. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68309/0.68610. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68171/0.68577. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 0.68247/0.68571. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.68101/0.68544. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68034/0.68529. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 0.68076/0.68512. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.67941/0.68461. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.67930/0.68429. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.67942/0.68460. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.67837/0.68499. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67876/0.68488. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.67831/0.68477. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67668/0.68486. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67776/0.68531. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67498/0.68439. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 0.67672/0.68492. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67492/0.68496. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67517/0.68539. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67515/0.68482. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67349/0.68511. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67208/0.68562. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66922/0.68544. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67206/0.68611. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.67080/0.68680. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66983/0.68633. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66904/0.68709. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66956/0.68614. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66936/0.68724. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.66657/0.68641. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66861/0.68803. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.66584/0.68739. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66708/0.68911. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66341/0.68829. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66418/0.68816. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66513/0.68917. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66299/0.68947. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66045/0.69013. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66123/0.69061. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66011/0.69004. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65881/0.69015. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65846/0.69004. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65754/0.68994. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65641/0.69058. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65412/0.69137. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65538/0.69109. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65448/0.69196. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65303/0.69303. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65285/0.69277. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64857/0.69278. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65156/0.69256. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65164/0.69332. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64878/0.69364. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65095/0.69439. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64694/0.69378. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64780/0.69278. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64343/0.69344. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64336/0.69493. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64432/0.69648. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63992/0.69671. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64179/0.69808. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63764/0.69953. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63870/0.70059. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63749/0.70141. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63595/0.70168. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63715/0.70153. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63453/0.70244. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62988/0.70199. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62880/0.70329. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63324/0.70504. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62609/0.70581. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62673/0.70596. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69292/0.68835. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69281/0.68905. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69219/0.68964. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69167/0.68993. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69147/0.69022. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69152/0.69049. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69158/0.69103. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69163/0.69141. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69001/0.69194. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68984/0.69229. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68922/0.69260. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68900/0.69302. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68832/0.69357. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68787/0.69418. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68700/0.69497. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68704/0.69483. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68588/0.69585. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68685/0.69584. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68499/0.69642. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68566/0.69694. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68448/0.69748. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68422/0.69876. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68309/0.69902. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68289/0.69957. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68283/0.70010. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68004/0.70055. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68036/0.70062. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68011/0.70175. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67808/0.70157. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67731/0.70168. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67607/0.70221. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67664/0.70223. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67651/0.70302. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67385/0.70301. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67407/0.70247. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67572/0.70314. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67320/0.70292. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67079/0.70305. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67174/0.70319. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67018/0.70211. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67129/0.70252. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66970/0.70210. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66711/0.70245. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66763/0.70328. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66756/0.70248. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66579/0.70010. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66465/0.69999. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66496/0.70023. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66338/0.69913. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66466/0.69915. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66421/0.69843. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66047/0.69816. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66098/0.69816. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66043/0.69718. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65842/0.69731. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65885/0.69698. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65958/0.69666. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65856/0.69722. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65456/0.69569. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65594/0.69541. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65577/0.69511. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65527/0.69449. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65107/0.69398. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65212/0.69392. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65047/0.69313. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64997/0.69470. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65000/0.69385. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64732/0.69300. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64865/0.69255. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64608/0.69329. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64293/0.69216. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64893/0.69258. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64149/0.69104. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64312/0.69255. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64296/0.69086. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63960/0.69029. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64112/0.68981. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63717/0.69083. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63848/0.69348. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63746/0.69161. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63505/0.68964. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63584/0.68920. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63460/0.68637. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63545/0.68535. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62922/0.68863. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63313/0.68884. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63230/0.68983. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62939/0.69262. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63173/0.69197. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62795/0.69243. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62732/0.69219. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62443/0.69199. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62849/0.69157. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62629/0.69221. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62465/0.69141. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62214/0.69176. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62048/0.69058. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.61795/0.69028. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62265/0.68848. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62174/0.69054. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69273/0.69404. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69152/0.69375. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69092/0.69354. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69050/0.69339. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69052/0.69319. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69007/0.69293. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68905/0.69279. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68844/0.69273. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68890/0.69256. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68862/0.69231. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68718/0.69212. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68780/0.69198. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68718/0.69197. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68583/0.69205. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68570/0.69219. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68607/0.69195. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68524/0.69213. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68439/0.69221. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68489/0.69207. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68290/0.69226. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68342/0.69217. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68329/0.69198. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68249/0.69157. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68193/0.69142. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68147/0.69131. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67987/0.69099. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68031/0.69092. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67861/0.69042. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67796/0.69023. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67754/0.68958. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67617/0.68898. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67682/0.68864. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67702/0.68871. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67496/0.68877. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67361/0.68748. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67447/0.68738. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67468/0.68689. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67347/0.68672. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67267/0.68661. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67346/0.68586. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67042/0.68558. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67042/0.68526. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66883/0.68521. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66874/0.68386. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66946/0.68427. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66711/0.68346. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66616/0.68342. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66613/0.68418. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66372/0.68365. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66430/0.68283. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66219/0.68280. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.66556/0.68261. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66346/0.68262. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66063/0.68256. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66183/0.68186. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66160/0.68218. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65945/0.68279. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65835/0.68191. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65702/0.68143. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.65748/0.68194. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65650/0.68074. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65505/0.68077. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65401/0.68278. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65237/0.68145. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65205/0.68174. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65173/0.68267. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65088/0.68126. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64940/0.68158. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65124/0.68302. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64843/0.68286. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64895/0.68213. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64784/0.68362. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64661/0.68262. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64251/0.68226. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64498/0.68263. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64429/0.68368. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64376/0.68391. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63978/0.68313. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64008/0.68358. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63873/0.68521. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63942/0.68451. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64178/0.68336. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63751/0.68202. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63629/0.68444. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63597/0.68354. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63793/0.68544. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63025/0.68544. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63155/0.68650. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63230/0.68469. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63286/0.68489. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62691/0.68703. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63265/0.68561. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63386/0.68682. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62929/0.68729. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62794/0.68625. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62782/0.68540. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63175/0.68442. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62585/0.68684. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62354/0.68663. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62620/0.68460. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69248/0.68943. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69030/0.68913. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69007/0.68946. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68945/0.68984. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68943/0.69032. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68777/0.69095. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68798/0.69143. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68666/0.69207. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68612/0.69284. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68638/0.69356. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68682/0.69417. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68576/0.69497. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68461/0.69571. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68524/0.69651. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68309/0.69726. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68335/0.69832. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68376/0.69924. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68361/0.70028. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68331/0.70102. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68137/0.70169. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68154/0.70265. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68090/0.70346. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68114/0.70434. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68069/0.70501. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68078/0.70618. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68019/0.70743. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67907/0.70832. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67959/0.70894. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67775/0.71005. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67761/0.71135. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67763/0.71210. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67686/0.71326. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67628/0.71465. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67544/0.71592. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67748/0.71666. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67591/0.71768. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67398/0.71901. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67433/0.72048. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67347/0.72184. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67414/0.72303. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67342/0.72368. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67201/0.72515. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67049/0.72631. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66970/0.72808. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67264/0.72928. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67177/0.73049. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67107/0.73175. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66889/0.73429. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66811/0.73502. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66923/0.73665. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66679/0.73840. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66659/0.74124. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66586/0.74294. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66505/0.74376. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66586/0.74490. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66540/0.74603. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66375/0.74748. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66323/0.74908. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66176/0.75072. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66034/0.75151. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66094/0.75388. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66119/0.75576. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66072/0.75706. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66173/0.75784. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65961/0.75974. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65814/0.75983. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65724/0.76184. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65699/0.76331. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65537/0.76489. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65531/0.76563. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65638/0.76666. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65361/0.76856. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65626/0.76879. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65201/0.77033. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64964/0.77180. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65039/0.77325. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65024/0.77521. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65096/0.77694. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64904/0.77899. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64728/0.77867. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64642/0.77957. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64473/0.78248. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64734/0.78410. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64661/0.78270. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64525/0.78275. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64549/0.78598. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64256/0.78815. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64317/0.78905. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64406/0.79030. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63982/0.79099. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64249/0.79188. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63566/0.79547. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64018/0.79747. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63876/0.79785. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63777/0.79718. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63717/0.79876. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63851/0.79807. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63741/0.79994. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63389/0.80062. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63443/0.80099. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69291/0.68698. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69086/0.68823. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68917/0.68958. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68799/0.69096. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68815/0.69217. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68673/0.69356. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68570/0.69494. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68492/0.69645. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68384/0.69802. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68382/0.69938. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68238/0.70067. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68255/0.70203. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68243/0.70317. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68248/0.70425. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68144/0.70523. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68196/0.70631. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68152/0.70703. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68021/0.70790. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.67894/0.70888. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67931/0.70938. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67973/0.70988. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67924/0.71044. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67905/0.71112. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67734/0.71164. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67824/0.71218. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67777/0.71233. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67629/0.71263. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67634/0.71320. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67581/0.71415. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67677/0.71448. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67466/0.71492. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67348/0.71573. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67252/0.71700. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67306/0.71758. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67244/0.71827. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67303/0.71896. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67001/0.71888. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67031/0.71949. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66993/0.72017. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66830/0.72076. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66859/0.72106. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66692/0.72096. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66854/0.72104. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66732/0.72104. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66678/0.72140. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66432/0.72229. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66390/0.72255. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66218/0.72313. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66198/0.72297. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66257/0.72366. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66064/0.72394. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.65878/0.72535. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65713/0.72661. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65962/0.72690. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65818/0.72769. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65391/0.72642. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65301/0.72801. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65263/0.72865. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65466/0.72909. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65392/0.73007. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65342/0.72977. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65138/0.73259. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.64818/0.73302. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65026/0.73318. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65079/0.73529. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64730/0.73648. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64652/0.73757. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64739/0.73757. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64590/0.73886. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64439/0.73977. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64633/0.74001. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64267/0.73880. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64098/0.73934. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64335/0.74147. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64125/0.74174. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63856/0.74298. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63974/0.74476. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63892/0.74299. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63571/0.74449. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63700/0.74122. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63703/0.74276. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63282/0.74679. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63866/0.74702. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63446/0.74521. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63698/0.74733. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63078/0.74911. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63111/0.74928. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63230/0.74459. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63015/0.74558. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63162/0.74911. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63134/0.74624. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62818/0.74844. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63371/0.75085. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62709/0.75272. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62804/0.75077. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62959/0.75348. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62812/0.75538. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62552/0.75060. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62614/0.75335. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62483/0.75230. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69323/0.68845. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69137/0.68813. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69103/0.68823. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68958/0.68846. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68949/0.68874. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68831/0.68904. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68820/0.68934. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68794/0.68970. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68767/0.69009. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68682/0.69064. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68661/0.69131. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68635/0.69175. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68542/0.69230. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68452/0.69290. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68556/0.69317. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68402/0.69351. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68429/0.69386. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68494/0.69442. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68360/0.69493. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68341/0.69553. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68238/0.69622. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68292/0.69660. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68169/0.69704. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68115/0.69772. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68088/0.69814. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68102/0.69881. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67937/0.69899. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67938/0.69920. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67897/0.69974. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67763/0.70059. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67675/0.70102. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67724/0.70118. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67646/0.70142. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67597/0.70189. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67596/0.70208. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67470/0.70288. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67390/0.70338. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67251/0.70266. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67213/0.70226. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67207/0.70277. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67257/0.70342. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67244/0.70487. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67127/0.70618. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67088/0.70640. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67151/0.70650. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67029/0.70766. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67017/0.70746. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66884/0.70878. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66716/0.70966. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66493/0.71028. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66577/0.71075. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66650/0.71156. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66396/0.71147. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66337/0.71329. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66185/0.71437. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66447/0.71526. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66194/0.71651. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66154/0.71758. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65961/0.71891. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66018/0.71906. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65871/0.72154. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65840/0.72180. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65820/0.72212. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65713/0.72291. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65816/0.72194. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65703/0.72325. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65383/0.72352. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65702/0.72439. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65077/0.72501. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65202/0.72729. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65276/0.72854. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65070/0.72886. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64998/0.73035. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65272/0.72793. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64990/0.72941. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65073/0.73053. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64723/0.72982. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64683/0.73294. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64589/0.73190. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64580/0.73481. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64361/0.73518. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64511/0.73485. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64460/0.73627. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64416/0.73602. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64229/0.73819. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63905/0.73743. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63824/0.74005. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64143/0.74061. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63852/0.74319. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63741/0.74271. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63898/0.74615. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63460/0.74459. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63629/0.74576. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63634/0.74335. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63343/0.74444. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63192/0.74644. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62977/0.74862. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62798/0.75263. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63055/0.74872. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62652/0.74877. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69130/0.69606. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68962/0.69869. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68928/0.70025. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68893/0.70092. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68877/0.70128. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68890/0.70181. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68865/0.70185. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68797/0.70227. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68779/0.70250. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68737/0.70322. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68671/0.70363. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68620/0.70430. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68563/0.70443. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68611/0.70505. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68545/0.70530. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68504/0.70637. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68513/0.70744. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68458/0.70788. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68502/0.70843. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68469/0.70879. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68406/0.70954. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68413/0.71021. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68429/0.71133. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68358/0.71184. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68329/0.71231. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68293/0.71264. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68389/0.71240. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68347/0.71295. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68274/0.71366. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68198/0.71457. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68347/0.71471. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68304/0.71431. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68199/0.71442. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68255/0.71509. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68260/0.71571. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68250/0.71606. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68172/0.71682. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68129/0.71689. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68096/0.71690. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68092/0.71685. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67994/0.71842. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68117/0.71827. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68042/0.71902. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67931/0.71949. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68121/0.71950. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67924/0.71977. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67860/0.71908. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67850/0.71950. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67751/0.72049. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68018/0.72019. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67713/0.72081. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67720/0.72122. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67744/0.72150. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67737/0.72180. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67702/0.72182. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67732/0.72138. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67803/0.72162. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67607/0.72283. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67741/0.72245. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67596/0.72189. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67438/0.72283. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67350/0.72277. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67520/0.72305. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67157/0.72311. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67263/0.72269. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.67334/0.72266. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67281/0.72251. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67294/0.72178. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66960/0.72379. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67334/0.72062. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67172/0.72106. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67103/0.72196. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66999/0.72355. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.67014/0.72265. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66985/0.72244. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66985/0.72282. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66912/0.72291. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66796/0.72110. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66703/0.72283. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66725/0.72217. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66402/0.72452. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66953/0.72478. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66564/0.72298. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66295/0.72453. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66434/0.72286. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66253/0.72535. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66344/0.72497. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.66323/0.72430. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66182/0.72346. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65961/0.72177. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66109/0.72699. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66019/0.72594. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66132/0.72632. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66033/0.72670. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65952/0.72626. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65755/0.72648. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65687/0.72586. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65938/0.72461. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65525/0.72519. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65507/0.72521. Took 0.18 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69272/0.68845. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68915/0.68343. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68786/0.68201. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68795/0.68227. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68648/0.68299. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68638/0.68383. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68575/0.68487. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68587/0.68622. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68503/0.68824. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68468/0.68960. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68476/0.69049. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68367/0.69155. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68446/0.69290. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68349/0.69400. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68338/0.69480. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68204/0.69512. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68209/0.69649. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68277/0.69706. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68217/0.69824. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68060/0.69807. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68245/0.69842. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68206/0.69876. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68166/0.69782. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68163/0.69792. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68130/0.69811. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68156/0.69821. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68040/0.69817. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68095/0.69871. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68036/0.69795. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68014/0.69768. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68094/0.69760. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67939/0.69782. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68004/0.69818. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67902/0.69757. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67884/0.69743. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67883/0.69704. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67879/0.69706. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67816/0.69712. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67810/0.69700. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67816/0.69653. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67731/0.69640. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67737/0.69663. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67614/0.69575. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67806/0.69577. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67498/0.69572. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67562/0.69483. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67553/0.69474. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67502/0.69311. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67509/0.69269. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67507/0.69295. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67513/0.69326. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67473/0.69257. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67441/0.69244. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67343/0.69295. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67388/0.69179. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67330/0.69287. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67090/0.69124. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67342/0.69207. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67274/0.69055. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67170/0.69200. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67107/0.69117. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66969/0.69011. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66883/0.68907. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67018/0.68876. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67007/0.68909. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66765/0.69113. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66695/0.68756. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66541/0.68983. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66762/0.68874. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66662/0.68866. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66659/0.68784. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66541/0.68989. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66481/0.68971. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66485/0.68943. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66195/0.68887. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66698/0.68949. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66361/0.69104. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66203/0.69200. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66157/0.69120. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66130/0.69328. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66157/0.69360. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66006/0.69141. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66080/0.69038. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66099/0.69219. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65705/0.69288. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65767/0.69564. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65696/0.69566. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65678/0.69457. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65259/0.69737. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65491/0.69890. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65372/0.69979. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65373/0.69488. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65241/0.69755. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65281/0.69988. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65188/0.70004. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65020/0.70091. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65079/0.70307. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64925/0.70414. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64940/0.70234. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64585/0.70873. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69690/0.68619. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68996/0.68512. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68913/0.68507. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68787/0.68493. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68869/0.68497. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68677/0.68495. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68624/0.68506. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68550/0.68517. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68499/0.68523. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68408/0.68546. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68477/0.68540. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68409/0.68560. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68209/0.68586. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68318/0.68614. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68328/0.68645. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68363/0.68659. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68273/0.68678. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68215/0.68701. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68173/0.68728. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68241/0.68730. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68220/0.68734. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68105/0.68749. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68111/0.68750. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68032/0.68768. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67949/0.68784. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68144/0.68775. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68092/0.68775. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68084/0.68776. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68066/0.68798. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67873/0.68786. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67964/0.68778. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68033/0.68782. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67882/0.68800. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68053/0.68811. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67890/0.68828. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68037/0.68847. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67873/0.68874. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67747/0.68857. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67692/0.68851. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67739/0.68879. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67816/0.68867. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67726/0.68905. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67889/0.68911. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67681/0.68936. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67628/0.68943. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67753/0.68960. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67560/0.68969. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67556/0.68998. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67523/0.69026. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67669/0.69061. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67660/0.69057. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67476/0.69081. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67542/0.69120. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67379/0.69125. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67416/0.69132. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67069/0.69180. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67187/0.69234. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67189/0.69291. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67227/0.69267. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66993/0.69311. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66984/0.69440. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66942/0.69469. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66988/0.69506. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66947/0.69507. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66915/0.69574. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66793/0.69610. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66702/0.69564. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66573/0.69513. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66862/0.69575. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66509/0.69672. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66670/0.69663. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66520/0.69672. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66405/0.69667. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66637/0.69711. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66098/0.69746. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66210/0.69626. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66252/0.69778. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66075/0.69657. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66163/0.69627. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65891/0.69784. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65938/0.69808. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65797/0.69859. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66070/0.69780. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65687/0.69850. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65692/0.69900. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65598/0.69865. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65559/0.69960. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65559/0.69808. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65312/0.70056. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65295/0.70160. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65451/0.70018. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65015/0.70045. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65350/0.70152. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65228/0.70074. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65269/0.69938. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64608/0.70291. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64823/0.70178. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64967/0.70131. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64969/0.70198. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64783/0.70231. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69205/0.69068. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68816/0.69080. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68772/0.69133. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68648/0.69158. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68623/0.69168. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68604/0.69183. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68534/0.69206. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68512/0.69251. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68385/0.69286. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68414/0.69313. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68381/0.69357. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68324/0.69405. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68264/0.69482. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68215/0.69592. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68194/0.69633. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68086/0.69701. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68199/0.69733. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68102/0.69817. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68040/0.69847. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68169/0.69863. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67976/0.69908. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68132/0.69948. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67957/0.70008. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67968/0.70035. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67914/0.70083. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67981/0.70081. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67794/0.70129. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67780/0.70166. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67739/0.70187. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67821/0.70191. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67772/0.70196. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67791/0.70205. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67687/0.70231. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67816/0.70259. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67630/0.70273. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67604/0.70311. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67646/0.70313. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67558/0.70319. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67511/0.70323. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67494/0.70354. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67553/0.70345. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67408/0.70289. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67322/0.70276. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67328/0.70218. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67290/0.70234. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67318/0.70217. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67315/0.70146. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67078/0.70190. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67361/0.70164. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67016/0.70241. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67092/0.70315. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67061/0.70321. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67019/0.70348. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66926/0.70404. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67099/0.70391. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66941/0.70383. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66803/0.70399. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66676/0.70472. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66618/0.70458. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66570/0.70531. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66652/0.70604. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66449/0.70561. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66417/0.70575. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66341/0.70696. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66335/0.70755. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66183/0.70706. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66303/0.70699. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66006/0.70696. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66003/0.70804. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66023/0.70824. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65942/0.70818. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65862/0.70939. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65888/0.70912. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65588/0.71047. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65809/0.71066. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65563/0.71095. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65454/0.71073. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65435/0.71242. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65358/0.71242. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65349/0.71187. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65277/0.71057. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64819/0.71191. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65006/0.71253. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65071/0.71235. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64768/0.71280. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64661/0.71489. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64463/0.71663. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64695/0.71504. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64461/0.71603. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64203/0.71537. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.64159/0.71710. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64200/0.71768. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64280/0.72102. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63964/0.72230. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.63894/0.72207. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64138/0.72273. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63454/0.72116. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63369/0.72335. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63265/0.72253. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63452/0.72099. Took 0.21 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69320/0.69756. Took 0.36 sec\n",
      "Epoch 1, Loss(train/val) 0.69206/0.69770. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69128/0.69751. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69085/0.69724. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69041/0.69733. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69077/0.69740. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69036/0.69746. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69025/0.69771. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69050/0.69779. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68993/0.69787. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69024/0.69805. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68911/0.69840. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68872/0.69854. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68950/0.69885. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68958/0.69914. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68813/0.69935. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68950/0.69967. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68916/0.69992. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68888/0.70000. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68872/0.70014. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68809/0.70055. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68785/0.70089. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68796/0.70101. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68711/0.70156. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68764/0.70197. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68739/0.70264. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68683/0.70296. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68765/0.70317. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68631/0.70389. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68612/0.70443. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68607/0.70483. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68552/0.70515. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68560/0.70588. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68570/0.70651. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68441/0.70691. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68492/0.70732. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68517/0.70776. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68386/0.70827. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68424/0.70886. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68416/0.70940. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68327/0.70989. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68328/0.71073. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68208/0.71114. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68244/0.71186. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68269/0.71254. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68069/0.71295. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68134/0.71358. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68081/0.71396. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68082/0.71417. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67948/0.71461. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68021/0.71526. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67971/0.71549. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67770/0.71536. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67961/0.71551. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67767/0.71566. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67744/0.71560. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67683/0.71598. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67589/0.71628. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67602/0.71659. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67363/0.71614. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67456/0.71618. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67528/0.71656. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67365/0.71560. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67358/0.71575. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67252/0.71557. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66886/0.71637. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67075/0.71556. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66789/0.71586. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66857/0.71498. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66617/0.71358. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66391/0.71287. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66290/0.71233. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66306/0.71135. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66207/0.71047. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65996/0.70883. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65672/0.70791. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65790/0.70701. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65705/0.70637. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65452/0.70425. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.65286/0.70406. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65365/0.70341. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64975/0.70420. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65051/0.70249. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64806/0.70215. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64549/0.70237. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64337/0.70140. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64420/0.70332. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64123/0.70203. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64148/0.70077. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63938/0.70215. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63702/0.70131. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63686/0.70182. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63620/0.70108. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63590/0.70146. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63554/0.70047. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63572/0.70036. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63364/0.70236. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62648/0.70264. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62721/0.70395. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62712/0.70541. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69357/0.68831. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69324/0.68876. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69230/0.68915. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69199/0.68959. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69213/0.69006. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69229/0.69051. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69145/0.69088. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69187/0.69114. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69156/0.69149. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69099/0.69187. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69098/0.69244. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69061/0.69285. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69106/0.69312. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69048/0.69314. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69008/0.69327. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69009/0.69363. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69004/0.69398. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.69020/0.69441. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68975/0.69466. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68972/0.69505. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68959/0.69533. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68852/0.69566. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68905/0.69589. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68876/0.69594. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68838/0.69618. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68807/0.69628. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68840/0.69630. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68687/0.69669. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68703/0.69684. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68548/0.69715. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68560/0.69776. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68572/0.69751. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68592/0.69699. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68517/0.69715. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68434/0.69762. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68423/0.69790. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68338/0.69740. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68316/0.69793. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68342/0.69792. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68180/0.69770. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68232/0.69795. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68102/0.69766. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68121/0.69803. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68058/0.69801. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67848/0.69756. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67999/0.69760. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67962/0.69704. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67753/0.69679. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67818/0.69662. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67714/0.69728. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67515/0.69713. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67418/0.69724. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67310/0.69831. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67402/0.69733. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67378/0.69765. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67187/0.69861. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67079/0.69905. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67144/0.69989. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67016/0.69987. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66853/0.69997. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67105/0.69913. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66650/0.70051. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66714/0.70143. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66560/0.70121. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66549/0.70189. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66418/0.70265. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66298/0.70246. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66324/0.70235. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66261/0.70347. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65846/0.70490. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65594/0.70421. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65670/0.70533. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65465/0.70593. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65376/0.70478. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65152/0.70743. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65403/0.70750. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65450/0.70738. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65119/0.70731. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65066/0.71061. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64725/0.70983. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64320/0.71135. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64414/0.71367. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.63921/0.71367. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64378/0.71453. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64304/0.71457. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63542/0.71574. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63629/0.71508. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63579/0.71699. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63456/0.71847. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63538/0.72066. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63391/0.72014. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63382/0.71861. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63170/0.71996. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62498/0.72057. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62576/0.71908. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62519/0.72055. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62733/0.72439. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62515/0.72551. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62389/0.72724. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62073/0.72912. Took 0.20 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69219/0.69527. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69106/0.69868. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69122/0.69977. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69045/0.70022. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68970/0.70117. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68996/0.70227. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68938/0.70278. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68818/0.70330. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68890/0.70382. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68817/0.70464. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68760/0.70580. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68798/0.70731. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68698/0.70725. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68665/0.70794. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68661/0.70800. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68672/0.70853. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68559/0.70894. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68619/0.70905. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68629/0.70931. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68583/0.70893. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68487/0.71022. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68681/0.71004. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68595/0.70988. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68497/0.71027. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68473/0.71051. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68513/0.71041. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68484/0.71149. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68528/0.71201. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68453/0.71189. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68315/0.71253. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68398/0.71180. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68465/0.71167. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68435/0.71220. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68463/0.71261. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68376/0.71272. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68348/0.71205. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68238/0.71275. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68310/0.71226. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68169/0.71300. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68206/0.71313. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68124/0.71348. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68386/0.71274. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68208/0.71302. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68083/0.71281. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68223/0.71292. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68038/0.71203. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67987/0.71179. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68171/0.71152. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67988/0.71185. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68124/0.71153. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67986/0.71100. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67963/0.71104. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67992/0.71103. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67948/0.71079. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67906/0.71168. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67915/0.71139. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67734/0.71133. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.67780/0.71232. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67806/0.71148. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67589/0.71167. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67846/0.71254. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67624/0.71090. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67616/0.71091. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67564/0.71077. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67636/0.71133. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.67422/0.71007. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67348/0.71133. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67485/0.71128. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67384/0.71084. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67586/0.71125. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67459/0.71133. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67504/0.71091. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67218/0.71093. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67187/0.71061. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67386/0.71146. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67023/0.71019. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67241/0.71021. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67138/0.70894. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.67000/0.70802. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.67255/0.70981. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.67067/0.70943. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66885/0.70970. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.67043/0.71097. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67087/0.70954. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66611/0.70912. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.66969/0.71007. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66778/0.70705. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66660/0.70845. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66886/0.70831. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66245/0.71007. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66520/0.71168. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66657/0.71029. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66370/0.70686. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66369/0.70796. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.66334/0.70913. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.66503/0.70473. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.66390/0.70820. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.66216/0.70759. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65986/0.70658. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.66288/0.70773. Took 0.19 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69658/0.68439. Took 0.28 sec\n",
      "Epoch 1, Loss(train/val) 0.69138/0.68145. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68994/0.68269. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69000/0.68441. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68861/0.68608. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68780/0.68805. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68766/0.68999. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68708/0.69119. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68663/0.69268. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68615/0.69444. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68499/0.69601. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68467/0.69717. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68476/0.69850. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68467/0.70015. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68381/0.70094. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68199/0.70206. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68275/0.70342. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68348/0.70416. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68245/0.70498. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68292/0.70615. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68345/0.70582. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68113/0.70618. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68055/0.70785. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67994/0.70718. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68007/0.70726. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67960/0.70891. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67992/0.70886. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68055/0.70806. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67894/0.70961. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67918/0.71028. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67902/0.71057. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67890/0.70877. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67711/0.70976. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67725/0.71037. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67618/0.71091. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67522/0.71109. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67397/0.71276. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67517/0.71338. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67524/0.71221. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67292/0.71250. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67444/0.71340. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67232/0.71455. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67204/0.71524. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67143/0.71468. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67008/0.71536. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66993/0.71436. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66963/0.71665. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66849/0.71557. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66881/0.71627. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66818/0.71742. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66640/0.71691. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66615/0.71893. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66207/0.71880. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66563/0.72023. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66266/0.71818. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66158/0.72133. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66097/0.72087. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65890/0.72301. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65596/0.72410. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65921/0.72175. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65645/0.72109. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65716/0.72416. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.65659/0.72544. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65252/0.72387. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65520/0.72442. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65139/0.72529. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64991/0.72658. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65023/0.72599. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.64764/0.72681. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64727/0.72848. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64552/0.72979. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64512/0.72816. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.64274/0.72941. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64208/0.73033. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64220/0.73065. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64013/0.73375. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63957/0.73428. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63848/0.73822. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63918/0.73811. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63659/0.73682. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63037/0.74059. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63105/0.74085. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63098/0.74027. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63223/0.73911. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62962/0.74228. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62576/0.74471. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.62728/0.74426. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62686/0.74788. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62285/0.74474. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.62287/0.74783. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62047/0.74916. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62150/0.75488. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.61922/0.75306. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61746/0.75115. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.61742/0.75085. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.61357/0.75401. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61372/0.75242. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.61399/0.75292. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.61729/0.75670. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61271/0.75274. Took 0.20 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69522/0.68226. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69051/0.68224. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68910/0.68254. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68763/0.68275. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68604/0.68317. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68502/0.68336. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68409/0.68391. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68312/0.68441. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68206/0.68562. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68188/0.68554. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68126/0.68577. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68064/0.68557. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68046/0.68644. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67981/0.68632. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.67999/0.68659. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67945/0.68602. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67968/0.68610. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67984/0.68582. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67811/0.68609. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67883/0.68546. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67851/0.68534. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67885/0.68543. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67829/0.68467. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67810/0.68441. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67832/0.68418. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67720/0.68471. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67828/0.68444. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67827/0.68428. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67719/0.68444. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67633/0.68377. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67657/0.68395. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67626/0.68377. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67735/0.68357. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67658/0.68338. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67648/0.68294. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67572/0.68306. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67672/0.68320. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67584/0.68282. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67595/0.68294. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67492/0.68273. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67465/0.68271. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67549/0.68233. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67542/0.68291. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67452/0.68293. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67396/0.68371. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67363/0.68280. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67351/0.68263. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67450/0.68276. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67284/0.68275. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67293/0.68201. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67224/0.68331. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67322/0.68222. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67320/0.68188. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.67214/0.68275. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67139/0.68231. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.67036/0.68252. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 0.67104/0.68212. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67155/0.68275. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66986/0.68245. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66910/0.68301. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66872/0.68367. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67126/0.68263. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66915/0.68201. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66650/0.68301. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67001/0.68190. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66608/0.68210. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66766/0.68133. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66749/0.68304. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66656/0.68235. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66625/0.68441. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66749/0.68247. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66495/0.68310. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66563/0.68375. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66507/0.68380. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66540/0.68418. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66486/0.68364. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66284/0.68422. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66266/0.68407. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66313/0.68526. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66262/0.68443. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65968/0.68565. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66277/0.68442. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66038/0.68611. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66129/0.68526. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65835/0.68595. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65824/0.68608. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66022/0.68468. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65934/0.68802. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65599/0.68555. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65667/0.68873. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65699/0.68719. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65704/0.68944. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65366/0.68986. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65688/0.68889. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65600/0.69249. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65247/0.69218. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65429/0.69152. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65273/0.69184. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65102/0.69059. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65471/0.68986. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69037/0.68122. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68802/0.68037. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68719/0.68101. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68591/0.68143. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68558/0.68210. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68471/0.68269. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68392/0.68340. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68357/0.68396. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68227/0.68474. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68142/0.68604. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68135/0.68740. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68109/0.68787. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68066/0.68913. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68029/0.68987. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68004/0.69060. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67983/0.69102. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67977/0.69147. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67995/0.69209. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67810/0.69262. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.67947/0.69335. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67809/0.69359. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67760/0.69416. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67755/0.69545. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67656/0.69571. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67621/0.69654. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67734/0.69701. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67588/0.69762. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67643/0.69806. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67607/0.69827. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67587/0.69840. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67469/0.69865. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67511/0.69923. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67526/0.69926. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67514/0.69952. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67525/0.69901. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67570/0.69908. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67318/0.69901. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67371/0.69918. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67122/0.69939. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67250/0.69983. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67314/0.70089. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67103/0.70033. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67118/0.70143. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67219/0.70224. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67179/0.70217. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67023/0.70128. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67074/0.70232. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67065/0.70253. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67073/0.70264. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66959/0.70150. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66953/0.70152. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66986/0.70292. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66767/0.70100. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66582/0.70313. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66815/0.70222. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66780/0.70348. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66735/0.70225. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66703/0.70199. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66617/0.70260. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66510/0.70326. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66505/0.70406. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66385/0.70443. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66468/0.70383. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66483/0.70566. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66236/0.70571. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66209/0.70513. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66087/0.70509. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66191/0.70564. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65803/0.70641. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65777/0.70546. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65986/0.70580. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65734/0.70556. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65787/0.70542. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65699/0.70550. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65599/0.70604. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65437/0.70632. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65609/0.70684. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65313/0.70570. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65171/0.70803. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65383/0.70773. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65019/0.70758. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65276/0.70667. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64901/0.70694. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64881/0.70787. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64901/0.70819. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64520/0.70853. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64478/0.70884. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64559/0.70989. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64488/0.70979. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64298/0.71042. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64313/0.71150. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64652/0.71020. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63957/0.71078. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63677/0.71489. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63903/0.71309. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63728/0.71181. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63563/0.71484. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63274/0.71601. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63511/0.71446. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63321/0.71438. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69048/0.70327. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68753/0.70164. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68589/0.69907. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68422/0.69757. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68332/0.69609. Took 0.22 sec\n",
      "Epoch 5, Loss(train/val) 0.68155/0.69438. Took 0.22 sec\n",
      "Epoch 6, Loss(train/val) 0.68066/0.69305. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68035/0.69187. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.67941/0.69085. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.67860/0.69010. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.67830/0.68998. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.67806/0.68963. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.67684/0.68913. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.67741/0.68875. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.67781/0.68889. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.67690/0.68825. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.67611/0.68844. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67647/0.68883. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.67567/0.68880. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67485/0.68837. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.67487/0.68863. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67472/0.68855. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.67522/0.68834. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67460/0.68846. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.67387/0.68838. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67417/0.68853. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67405/0.68906. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67272/0.68896. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67399/0.68946. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67380/0.68939. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67261/0.68957. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67201/0.68974. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67165/0.69029. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67198/0.68983. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67071/0.68972. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67100/0.68972. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67104/0.69068. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67032/0.69061. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67014/0.69045. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.66998/0.69043. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66965/0.69052. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66945/0.69055. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66848/0.69038. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66799/0.68956. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.66946/0.68918. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66889/0.68940. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.66551/0.68916. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66719/0.69011. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66626/0.69038. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66707/0.68956. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66681/0.68913. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66503/0.69020. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66488/0.69024. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66529/0.68988. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66377/0.68920. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66383/0.68914. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66224/0.68904. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66336/0.68920. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66198/0.68910. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65893/0.68881. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.65808/0.68908. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65945/0.68827. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65807/0.68846. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65772/0.68860. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65870/0.68792. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65697/0.68758. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65444/0.68839. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65644/0.68779. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65583/0.68780. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65310/0.68828. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65323/0.68782. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65256/0.68829. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65024/0.68692. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64944/0.68723. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65077/0.68685. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64852/0.68607. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64646/0.68670. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64759/0.68695. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64888/0.68578. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64408/0.68635. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64629/0.68730. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64180/0.68689. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.64108/0.68647. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64021/0.68776. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63755/0.68724. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63749/0.68842. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63956/0.68757. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63738/0.68578. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63719/0.68624. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63655/0.68592. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63365/0.68673. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63656/0.68684. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63283/0.68559. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63047/0.68751. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63276/0.68729. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62682/0.68722. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62811/0.68906. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62712/0.69068. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.62618/0.68976. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62747/0.68940. Took 0.19 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69106/0.69265. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68748/0.69200. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68660/0.69251. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68508/0.69318. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68514/0.69402. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68511/0.69501. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68344/0.69619. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68340/0.69720. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68179/0.69848. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68169/0.69986. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68058/0.70122. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.67972/0.70267. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68006/0.70407. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67891/0.70514. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.67804/0.70626. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67897/0.70694. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67756/0.70785. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67654/0.70919. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67673/0.71028. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67605/0.71125. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67756/0.71139. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67627/0.71234. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67695/0.71266. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67644/0.71303. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67586/0.71342. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67586/0.71361. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67601/0.71373. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67495/0.71415. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67477/0.71438. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67539/0.71482. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67430/0.71496. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67612/0.71526. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67489/0.71537. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67459/0.71556. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67396/0.71615. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67421/0.71626. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67422/0.71658. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67315/0.71663. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67218/0.71703. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67268/0.71764. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67271/0.71825. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67227/0.71898. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67301/0.71896. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67183/0.71928. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67032/0.71960. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67221/0.71996. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67197/0.72014. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67083/0.71999. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66861/0.72085. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67045/0.72114. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66932/0.72170. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66951/0.72219. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66956/0.72217. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66968/0.72243. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66831/0.72334. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66896/0.72392. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66821/0.72483. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66793/0.72522. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66940/0.72535. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66870/0.72518. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66775/0.72546. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66619/0.72609. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66766/0.72681. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66545/0.72747. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66623/0.72815. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66396/0.72924. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66658/0.73053. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66292/0.73085. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66377/0.73174. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66223/0.73218. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66300/0.73207. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66300/0.73277. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66175/0.73319. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66254/0.73324. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66069/0.73342. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66003/0.73447. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66083/0.73444. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65785/0.73576. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66079/0.73605. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66048/0.73568. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66140/0.73527. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65792/0.73693. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65885/0.73710. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65677/0.73781. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65896/0.73882. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65570/0.73870. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65434/0.73905. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65655/0.73876. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65491/0.73960. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65161/0.74029. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65331/0.74091. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65025/0.74321. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65151/0.74470. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64882/0.74494. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65000/0.74415. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65397/0.74411. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64874/0.74464. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65001/0.74456. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64670/0.74545. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64884/0.74525. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.68954/0.68962. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68701/0.69130. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68568/0.69163. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68396/0.69161. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68383/0.69219. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68200/0.69276. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68243/0.69319. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68184/0.69391. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68006/0.69476. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.67919/0.69554. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.67951/0.69700. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.67931/0.69733. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.67783/0.69816. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67760/0.69972. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.67701/0.69991. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67781/0.70011. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67661/0.70119. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67686/0.70102. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67721/0.70188. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67550/0.70235. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67588/0.70277. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67545/0.70331. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67513/0.70325. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67526/0.70378. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67516/0.70405. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67517/0.70387. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67313/0.70405. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67293/0.70473. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67337/0.70550. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67262/0.70581. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67232/0.70658. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67167/0.70667. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67318/0.70695. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67090/0.70673. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66962/0.70736. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67038/0.70689. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66986/0.70812. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66916/0.70833. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66832/0.70823. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66909/0.70897. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67045/0.70943. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66783/0.70874. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66907/0.70937. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66741/0.70990. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66715/0.70990. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66702/0.71027. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66476/0.71166. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66680/0.71228. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66367/0.71281. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66570/0.71308. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66258/0.71276. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66308/0.71476. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66363/0.71389. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66464/0.71482. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66247/0.71450. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66113/0.71510. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66218/0.71452. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66265/0.71577. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66146/0.71533. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66133/0.71532. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66260/0.71448. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65818/0.71507. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65804/0.71663. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66035/0.71605. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65745/0.71607. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65821/0.71744. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65487/0.71762. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65646/0.71693. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65500/0.71634. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65623/0.71720. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65373/0.71646. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65348/0.71766. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65315/0.71741. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65213/0.71933. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65333/0.71860. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65071/0.71862. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65230/0.71858. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64840/0.72040. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65304/0.71938. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64756/0.71996. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64433/0.72071. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64735/0.72288. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64854/0.71992. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64799/0.72310. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64567/0.72066. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64525/0.72079. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64164/0.72207. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64322/0.72168. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64530/0.72060. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64165/0.72015. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64111/0.72099. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63939/0.72147. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64006/0.72104. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63729/0.72316. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63813/0.72309. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63278/0.72301. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63559/0.72081. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63574/0.72352. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63429/0.72229. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63532/0.72248. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.68957/0.69135. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68900/0.69161. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68778/0.69220. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68697/0.69296. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68718/0.69375. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68619/0.69440. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68653/0.69521. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68575/0.69603. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68438/0.69691. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68492/0.69773. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68428/0.69860. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68328/0.70002. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68345/0.70211. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68191/0.70344. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68121/0.70521. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68203/0.70678. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68092/0.70759. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68172/0.70894. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68219/0.70970. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68063/0.71071. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.67971/0.71239. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67982/0.71347. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68067/0.71426. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.67929/0.71574. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.67959/0.71621. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.67896/0.71721. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67995/0.71759. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67785/0.71780. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.67740/0.71928. Took 0.22 sec\n",
      "Epoch 29, Loss(train/val) 0.67810/0.71986. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67793/0.72027. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67766/0.72065. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67742/0.72189. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.67720/0.72203. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67763/0.72255. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67693/0.72276. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67600/0.72343. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67635/0.72391. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.67468/0.72500. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67709/0.72490. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67504/0.72602. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67479/0.72637. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67449/0.72695. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67507/0.72773. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67301/0.72869. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.67381/0.72945. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67413/0.72916. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67359/0.72966. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67360/0.73030. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67282/0.73064. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67096/0.73125. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67307/0.73168. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67289/0.73231. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67195/0.73220. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67055/0.73406. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67090/0.73402. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67061/0.73360. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67072/0.73443. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67021/0.73440. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66955/0.73626. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67000/0.73657. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66928/0.73762. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66737/0.73725. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66667/0.73917. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66791/0.73928. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66664/0.73922. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66339/0.74036. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66409/0.74236. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66415/0.74289. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66665/0.74421. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66254/0.74521. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66229/0.74742. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66163/0.74761. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66093/0.74890. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66052/0.75100. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66129/0.75124. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66013/0.75229. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66102/0.75175. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65783/0.75233. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65633/0.75265. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65890/0.75401. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65890/0.75434. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65897/0.75480. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65563/0.75537. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65801/0.75686. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65674/0.75793. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65458/0.76051. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65279/0.75935. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65018/0.76162. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65488/0.76243. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65207/0.76135. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65009/0.76333. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64796/0.76528. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65005/0.76509. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64731/0.76547. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64822/0.76806. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64797/0.76833. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64540/0.76981. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64589/0.77107. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64488/0.77173. Took 0.20 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.68982/0.69083. Took 0.34 sec\n",
      "Epoch 1, Loss(train/val) 0.68812/0.69001. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68790/0.68947. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68638/0.68903. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68625/0.68869. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68583/0.68818. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68504/0.68776. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68451/0.68732. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68565/0.68681. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68407/0.68636. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68412/0.68586. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68391/0.68545. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68413/0.68518. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68281/0.68488. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68315/0.68464. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68169/0.68435. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68198/0.68413. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68166/0.68384. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68020/0.68376. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68011/0.68348. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68055/0.68321. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68040/0.68279. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67920/0.68256. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67852/0.68257. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67840/0.68273. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67843/0.68256. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67814/0.68265. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67693/0.68215. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67699/0.68210. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67604/0.68176. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67538/0.68131. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67445/0.68060. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67476/0.68040. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.67418/0.68048. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67237/0.68020. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67263/0.68027. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67071/0.68044. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66951/0.68021. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66697/0.68008. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66801/0.67992. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66823/0.67926. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.66863/0.67912. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66812/0.67952. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66613/0.67934. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66197/0.67844. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66543/0.67784. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66159/0.67727. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 0.65876/0.67713. Took 0.30 sec\n",
      "Epoch 48, Loss(train/val) 0.65744/0.67672. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65668/0.67645. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.65621/0.67584. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65342/0.67539. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.65148/0.67475. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.64985/0.67471. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64972/0.67534. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64650/0.67471. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64500/0.67452. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64639/0.67327. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64069/0.67402. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.64074/0.67325. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.63990/0.67438. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.63559/0.67479. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63325/0.67412. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63552/0.67495. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63336/0.67636. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63231/0.67556. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.62934/0.67599. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.62728/0.67484. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.62789/0.67635. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.62506/0.67781. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.62225/0.67719. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62387/0.67671. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62034/0.67841. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62211/0.67996. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.61768/0.68013. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.61826/0.68106. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.61828/0.68007. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.61522/0.67818. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.61590/0.67889. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61542/0.68267. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.61036/0.67872. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61373/0.67988. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61044/0.67958. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.60753/0.68309. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.61163/0.68250. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.60810/0.68345. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.60887/0.68326. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.60225/0.68724. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.60172/0.68928. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.60310/0.68711. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60250/0.68468. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.60009/0.68900. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.59752/0.69037. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.59419/0.68944. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.59843/0.68788. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.59421/0.68930. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59511/0.68846. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.59166/0.69358. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.58985/0.69137. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59481/0.69261. Took 0.20 sec\n",
      "ACC: 0.625\n",
      "Epoch 0, Loss(train/val) 0.69021/0.68804. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68767/0.68525. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68727/0.68442. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68666/0.68409. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68651/0.68406. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68665/0.68421. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68584/0.68431. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68630/0.68467. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68599/0.68497. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68516/0.68513. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68426/0.68553. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68464/0.68574. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68458/0.68642. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68377/0.68712. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68298/0.68763. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68289/0.68808. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68281/0.68881. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68186/0.68943. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68200/0.69024. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68181/0.69097. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68087/0.69196. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68038/0.69223. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68117/0.69303. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67944/0.69383. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67953/0.69478. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67849/0.69604. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67980/0.69680. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67756/0.69749. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67838/0.69878. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67719/0.69999. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67603/0.70134. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67656/0.70219. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67534/0.70310. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67633/0.70402. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67535/0.70482. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67385/0.70541. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67446/0.70650. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67354/0.70817. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67304/0.70988. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67371/0.71001. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67171/0.71083. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67267/0.71177. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67346/0.71290. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67034/0.71394. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67048/0.71434. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67009/0.71609. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67123/0.71690. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67101/0.71727. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67218/0.71776. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67063/0.71965. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66906/0.71960. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66862/0.72068. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67023/0.72139. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66920/0.72086. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66830/0.72185. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66814/0.72203. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66845/0.72327. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66675/0.72229. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66609/0.72310. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66741/0.72444. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66657/0.72504. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66506/0.72535. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66635/0.72689. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66547/0.72693. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66686/0.72642. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66361/0.72720. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66331/0.72827. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66430/0.72725. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66339/0.72817. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66104/0.72676. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66186/0.72953. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66146/0.72924. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66289/0.73005. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66115/0.73156. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66145/0.73091. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66040/0.73253. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65857/0.73327. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65985/0.72986. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65776/0.73128. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65641/0.73210. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65641/0.73189. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65615/0.73363. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65400/0.73451. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65678/0.73486. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65475/0.73596. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65323/0.73527. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65447/0.73561. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.65397/0.73490. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65276/0.73813. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65541/0.74059. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65304/0.73999. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64967/0.74120. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65029/0.73928. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65130/0.73924. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64912/0.74042. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64987/0.73941. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64842/0.74116. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64959/0.74025. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64701/0.74339. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64469/0.74263. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69417/0.68965. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68737/0.68498. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68549/0.68378. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68489/0.68380. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68511/0.68413. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68329/0.68456. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68394/0.68512. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68312/0.68570. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68250/0.68643. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68251/0.68703. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68194/0.68765. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68228/0.68825. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68156/0.68891. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68186/0.68962. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68112/0.69056. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68151/0.69117. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68055/0.69159. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68161/0.69203. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.67990/0.69237. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67905/0.69259. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67861/0.69309. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67918/0.69320. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67837/0.69315. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67850/0.69365. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67842/0.69386. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67695/0.69426. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67639/0.69427. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67546/0.69427. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67637/0.69397. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67544/0.69324. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67465/0.69322. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67161/0.69284. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67047/0.69293. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67076/0.69250. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.66983/0.69230. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66944/0.69233. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.66798/0.69203. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66738/0.69109. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66642/0.68998. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66555/0.68940. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66440/0.68776. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66162/0.68811. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66015/0.68778. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.65985/0.68652. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.65835/0.68669. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65659/0.68729. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65594/0.68738. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65542/0.68665. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65268/0.68803. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65359/0.68906. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65154/0.68871. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.64849/0.68773. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.64773/0.69003. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64943/0.68948. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64591/0.68825. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64553/0.69021. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64489/0.69361. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64340/0.69509. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64313/0.69469. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64243/0.69388. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64102/0.69540. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64106/0.69835. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64042/0.69909. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.63802/0.70011. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63755/0.69705. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63437/0.69896. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63661/0.69979. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63582/0.70105. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63538/0.70484. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63975/0.70229. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63530/0.70296. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63182/0.70530. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63279/0.70628. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63439/0.70423. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63144/0.70734. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63260/0.70547. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62951/0.70603. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.62766/0.70476. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62915/0.70792. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62626/0.70745. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62295/0.70956. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62169/0.70954. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62129/0.71015. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62416/0.70752. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62267/0.70955. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62097/0.71147. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62197/0.70973. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62544/0.70921. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62055/0.71039. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62144/0.71386. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61989/0.70908. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.61557/0.71164. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61866/0.71164. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61525/0.71664. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61678/0.71153. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61825/0.71272. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.61401/0.71516. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61424/0.71512. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60692/0.71534. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.60823/0.71526. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69549/0.69293. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68467/0.69489. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68214/0.69719. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68120/0.69879. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68125/0.69936. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68038/0.69993. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68013/0.70034. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68032/0.70043. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.67935/0.70047. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.67869/0.70062. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.67912/0.70050. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.67904/0.70057. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.67881/0.70081. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67847/0.70055. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.67751/0.70083. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.67784/0.70091. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67779/0.70084. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67675/0.70087. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67641/0.70065. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67569/0.70068. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67625/0.70105. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67433/0.70110. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67510/0.70135. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67415/0.70120. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.67348/0.70109. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67230/0.70109. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67190/0.70128. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67055/0.70083. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67011/0.70058. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.66896/0.70051. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.66704/0.70093. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.66796/0.70051. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.66641/0.70099. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.66389/0.70118. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.66247/0.70134. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.66024/0.70327. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.66015/0.70268. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.65847/0.70391. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.65862/0.70544. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.65896/0.70448. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.65693/0.70663. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.65589/0.70756. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.65861/0.70731. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.65613/0.70668. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65160/0.70856. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65102/0.70711. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.65157/0.70871. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.64887/0.70891. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.64865/0.71061. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.64908/0.71256. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.64608/0.71412. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.64634/0.71359. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.64462/0.71627. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.64433/0.71538. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64471/0.71603. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.64206/0.71553. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64057/0.71837. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64008/0.71914. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.64035/0.71910. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.63614/0.72000. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.63457/0.72061. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.63582/0.72052. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.63420/0.72207. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.63201/0.72234. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63399/0.72337. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.63152/0.72455. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63069/0.72498. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63066/0.72351. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63109/0.72581. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.62961/0.72743. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.62796/0.72869. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.62763/0.72864. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62775/0.73259. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62733/0.73258. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62511/0.73160. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.62637/0.73273. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62308/0.73376. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62086/0.73344. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62129/0.73473. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62154/0.73414. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62014/0.73728. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61883/0.73772. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.62046/0.73555. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61822/0.73718. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.61692/0.73750. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61616/0.73958. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61691/0.73934. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61355/0.73970. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61305/0.74240. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61514/0.74196. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.61404/0.74019. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61409/0.74126. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.61103/0.74295. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.60832/0.74121. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.60816/0.74234. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60744/0.74468. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.60820/0.74517. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60564/0.74497. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60429/0.74805. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60615/0.74750. Took 0.20 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.68684/0.69806. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68265/0.70331. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68146/0.70545. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68094/0.70621. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68015/0.70665. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.67953/0.70711. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.67913/0.70776. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.67892/0.70815. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.67868/0.70841. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.67843/0.70867. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.67822/0.70912. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.67770/0.70982. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.67823/0.71036. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67759/0.71000. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.67695/0.71030. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.67623/0.71070. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67643/0.71168. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67633/0.71172. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.67724/0.71167. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67568/0.71188. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67559/0.71207. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67604/0.71240. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67536/0.71251. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67501/0.71275. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67401/0.71283. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67422/0.71278. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67383/0.71348. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67334/0.71347. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67444/0.71353. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67326/0.71347. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67236/0.71370. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67385/0.71392. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67305/0.71380. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67236/0.71370. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67238/0.71341. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67314/0.71401. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67159/0.71323. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67145/0.71334. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67025/0.71479. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67056/0.71441. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67029/0.71420. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67010/0.71472. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67041/0.71354. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67031/0.71382. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66992/0.71377. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66959/0.71344. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66903/0.71362. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66760/0.71440. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66842/0.71379. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66811/0.71411. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66744/0.71433. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66727/0.71421. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66797/0.71430. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66585/0.71470. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66586/0.71341. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66478/0.71310. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66373/0.71285. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66354/0.71402. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66374/0.71383. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66388/0.71427. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66307/0.71423. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66428/0.71410. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66426/0.71457. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66170/0.71487. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66088/0.71554. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66066/0.71472. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65910/0.71472. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66001/0.71454. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65877/0.71513. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65865/0.71590. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65941/0.71626. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65718/0.71660. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65721/0.71691. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65587/0.71756. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65473/0.71900. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65304/0.71746. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65342/0.71579. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65119/0.71776. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65069/0.71847. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65083/0.71777. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64959/0.71852. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64841/0.71735. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64960/0.71914. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64806/0.71925. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64679/0.71850. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64118/0.71980. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64542/0.72145. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64187/0.72001. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64163/0.72283. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63923/0.72085. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64004/0.72442. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64008/0.72997. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63742/0.72956. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63351/0.72758. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63478/0.72765. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63789/0.72817. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63407/0.72783. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62841/0.73354. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63067/0.73502. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62959/0.73532. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69593/0.68938. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68150/0.69280. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.67978/0.69317. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.67937/0.69311. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.67925/0.69303. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.67887/0.69308. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.67791/0.69345. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.67700/0.69368. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.67690/0.69383. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.67631/0.69393. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.67566/0.69426. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.67523/0.69455. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.67504/0.69462. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.67508/0.69464. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.67477/0.69490. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.67453/0.69500. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67351/0.69481. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67421/0.69444. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67426/0.69454. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67366/0.69431. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67318/0.69447. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67413/0.69435. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67271/0.69462. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67402/0.69432. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67217/0.69478. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67153/0.69484. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67115/0.69494. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67243/0.69484. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67169/0.69433. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67203/0.69435. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67114/0.69437. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67144/0.69497. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67106/0.69458. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67053/0.69439. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67007/0.69517. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66982/0.69506. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67015/0.69520. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.66958/0.69561. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.66986/0.69502. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67028/0.69595. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66852/0.69613. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67011/0.69638. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66956/0.69683. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66807/0.69683. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.66831/0.69773. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66800/0.69805. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66899/0.69793. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66900/0.69799. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66897/0.69827. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66692/0.69802. Took 0.22 sec\n",
      "Epoch 50, Loss(train/val) 0.66790/0.69804. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66701/0.69841. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66593/0.69898. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66586/0.69949. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66517/0.69963. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66340/0.69990. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66489/0.69939. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66585/0.69992. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66629/0.69985. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66371/0.70036. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66289/0.70045. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66431/0.70140. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66338/0.70133. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66399/0.70258. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66283/0.70311. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66341/0.70340. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66330/0.70349. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66377/0.70400. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66165/0.70411. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66268/0.70506. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66142/0.70583. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65991/0.70619. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66191/0.70696. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66353/0.70624. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65959/0.70656. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66025/0.70705. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65868/0.70723. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65964/0.70772. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65787/0.70785. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65869/0.70776. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65882/0.70889. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65733/0.70906. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65867/0.70949. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65691/0.71041. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65512/0.71094. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65661/0.71063. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65720/0.71167. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65530/0.71123. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65426/0.71065. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65512/0.71242. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65454/0.71523. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65430/0.71333. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65216/0.71470. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65142/0.71464. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65188/0.71530. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65221/0.71672. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65051/0.71586. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64939/0.71667. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64765/0.71700. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65068/0.71594. Took 0.19 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.70032/0.68080. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68429/0.67265. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68197/0.67095. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68076/0.67036. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68155/0.66981. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.67970/0.66919. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68059/0.66911. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.67878/0.66891. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.67899/0.66897. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.67758/0.66873. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.67745/0.66893. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.67653/0.66923. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.67776/0.66957. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67647/0.66978. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.67597/0.67020. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67566/0.67055. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67589/0.67114. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67366/0.67184. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67475/0.67277. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67459/0.67347. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67410/0.67392. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67352/0.67506. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67209/0.67607. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67267/0.67699. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67232/0.67791. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67171/0.67846. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67110/0.67894. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.66976/0.68031. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.66831/0.68111. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.66843/0.68255. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.66874/0.68380. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.66825/0.68452. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66656/0.68510. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.66763/0.68590. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66517/0.68668. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66657/0.68768. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66740/0.68854. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66537/0.68969. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66487/0.69092. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66466/0.69152. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66515/0.69253. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66399/0.69239. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66283/0.69420. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66168/0.69560. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66176/0.69643. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66078/0.69783. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65840/0.69871. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66133/0.69926. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65879/0.70141. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65933/0.70064. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66030/0.70231. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65872/0.70302. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65758/0.70349. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65791/0.70349. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65673/0.70474. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65653/0.70622. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65460/0.70656. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65301/0.70869. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65336/0.70951. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65478/0.71006. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65327/0.71030. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65401/0.71116. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65314/0.71078. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65457/0.71086. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65129/0.71005. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65123/0.71238. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65110/0.71256. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64969/0.71371. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64845/0.71388. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64792/0.71496. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64588/0.71320. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64792/0.71583. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64849/0.71427. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64734/0.71544. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64482/0.71492. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64369/0.71664. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64416/0.71586. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64388/0.71497. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64262/0.71636. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64121/0.71627. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64050/0.71674. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63885/0.71786. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63869/0.71920. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.63939/0.71704. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63985/0.71847. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63868/0.71693. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63565/0.71649. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63767/0.71594. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63664/0.71391. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63343/0.71518. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63068/0.71435. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63007/0.71571. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63415/0.71605. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63273/0.71452. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62812/0.71266. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62679/0.71285. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62892/0.71389. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62649/0.71413. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63104/0.71000. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62730/0.71164. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.68895/0.68608. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68528/0.68672. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68454/0.68761. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68440/0.68840. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68396/0.68900. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68305/0.68961. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68299/0.69020. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68304/0.69084. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68217/0.69138. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68236/0.69198. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68155/0.69263. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68105/0.69302. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68130/0.69366. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68052/0.69441. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68036/0.69503. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67913/0.69559. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67851/0.69603. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67891/0.69652. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.67897/0.69695. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.67680/0.69750. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67712/0.69824. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67661/0.69910. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67562/0.70007. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67552/0.70122. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67496/0.70247. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67435/0.70287. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67427/0.70386. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67318/0.70461. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67296/0.70561. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67268/0.70658. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67115/0.70827. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67076/0.70942. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67137/0.71032. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66962/0.71089. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66889/0.71170. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66802/0.71322. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66613/0.71430. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66595/0.71583. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66569/0.71659. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66549/0.71755. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66346/0.71914. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66426/0.72028. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66213/0.72111. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66178/0.72180. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65998/0.72271. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65954/0.72337. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65672/0.72325. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65958/0.72275. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65620/0.72471. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65571/0.72697. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65448/0.72656. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65595/0.72633. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65108/0.72696. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65195/0.72755. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64975/0.72681. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64913/0.72780. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64756/0.72874. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64825/0.72973. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64321/0.73127. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64502/0.73159. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64395/0.73184. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64403/0.73191. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64215/0.73242. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63943/0.73276. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63891/0.73353. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.63912/0.73422. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.63808/0.73241. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63694/0.73481. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63674/0.73450. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63422/0.73562. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63038/0.74012. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62984/0.73624. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63243/0.73672. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62667/0.73676. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62630/0.74169. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.62542/0.73976. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62184/0.74170. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62435/0.73930. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62391/0.73899. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.61888/0.74219. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61748/0.74070. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62174/0.74100. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61922/0.73821. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.61114/0.74027. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61631/0.74098. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61320/0.74066. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.60990/0.74257. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.60870/0.74262. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61019/0.74282. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61044/0.74229. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60937/0.74040. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.60517/0.74526. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60810/0.74524. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.60350/0.74262. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60321/0.74538. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60455/0.74438. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60181/0.74786. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60347/0.74512. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60020/0.74802. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.59986/0.74441. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.68888/0.68359. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68509/0.68169. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68406/0.68126. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68403/0.68099. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68449/0.68088. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68367/0.68066. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68276/0.68038. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68344/0.68020. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68250/0.67994. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68184/0.67971. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68204/0.67953. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68118/0.67916. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68104/0.67878. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68032/0.67839. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68031/0.67804. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68083/0.67785. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68093/0.67769. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68030/0.67772. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68036/0.67771. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67873/0.67760. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67833/0.67736. Took 0.23 sec\n",
      "Epoch 21, Loss(train/val) 0.67921/0.67744. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67735/0.67750. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67867/0.67759. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67823/0.67765. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67820/0.67764. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67725/0.67757. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67716/0.67784. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67745/0.67835. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67620/0.67870. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67663/0.67921. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67481/0.67954. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67632/0.68004. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67547/0.68088. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67408/0.68122. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67345/0.68191. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67402/0.68209. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67286/0.68275. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67287/0.68310. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67157/0.68409. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67260/0.68505. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67115/0.68568. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67176/0.68636. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67202/0.68686. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67090/0.68806. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67087/0.68832. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66915/0.68920. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 0.66751/0.69017. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66917/0.68975. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66769/0.69122. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66772/0.69178. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66829/0.69258. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66836/0.69280. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66607/0.69374. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66538/0.69447. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66558/0.69616. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66437/0.69726. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66282/0.69803. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66433/0.70048. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66095/0.70183. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66129/0.70354. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.66114/0.70418. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66438/0.70470. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65981/0.70581. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66192/0.70650. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66202/0.70679. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66197/0.70909. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66042/0.71048. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65869/0.71096. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65603/0.71188. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65759/0.71268. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65755/0.71412. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65723/0.71600. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65759/0.71728. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65667/0.71819. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.65448/0.72047. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65489/0.72080. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65396/0.72161. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65396/0.72214. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65417/0.72372. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65387/0.72568. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65306/0.72603. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65383/0.72626. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65170/0.72831. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65148/0.72939. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65043/0.73096. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65049/0.73258. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64833/0.73329. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64971/0.73402. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64706/0.73508. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64873/0.73662. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64625/0.73841. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64767/0.74108. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64711/0.74009. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64569/0.74052. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64686/0.74363. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64629/0.74317. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64329/0.74401. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64226/0.74501. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64466/0.74675. Took 0.18 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69484/0.68447. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68670/0.68393. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68495/0.68376. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68483/0.68382. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68395/0.68394. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68324/0.68429. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68276/0.68489. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68262/0.68541. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68229/0.68593. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68122/0.68684. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68111/0.68753. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68132/0.68830. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.67947/0.68918. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67903/0.68999. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.67942/0.69096. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67813/0.69201. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67815/0.69310. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.67801/0.69429. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67729/0.69520. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67768/0.69622. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67730/0.69717. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67740/0.69801. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67619/0.69888. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67663/0.69955. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67579/0.70023. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67406/0.70192. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67430/0.70298. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67342/0.70380. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67334/0.70469. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67249/0.70568. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67277/0.70651. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67331/0.70703. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67092/0.70764. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67209/0.70840. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67124/0.70948. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67134/0.70995. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67344/0.71038. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66927/0.71092. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66955/0.71175. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67004/0.71169. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67033/0.71202. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67056/0.71175. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66988/0.71219. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66929/0.71286. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66813/0.71353. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66921/0.71347. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66770/0.71326. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66781/0.71391. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66771/0.71425. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66784/0.71423. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66621/0.71492. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66597/0.71603. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66493/0.71630. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66450/0.71597. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66478/0.71674. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66435/0.71720. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66340/0.71787. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66401/0.71798. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66377/0.71885. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66382/0.71921. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66311/0.71901. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66139/0.72019. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66264/0.72103. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66337/0.72113. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66269/0.72087. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66182/0.72148. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66090/0.72194. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66195/0.72189. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66023/0.72201. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65930/0.72274. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66030/0.72425. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66019/0.72498. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65810/0.72423. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65890/0.72465. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65783/0.72410. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65918/0.72415. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65938/0.72447. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65698/0.72445. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65842/0.72370. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65700/0.72437. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65753/0.72424. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65744/0.72402. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65811/0.72504. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65493/0.72504. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65350/0.72574. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65538/0.72582. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65478/0.72678. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65427/0.72715. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65389/0.72634. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65356/0.72720. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65301/0.72683. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65096/0.72828. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.65158/0.72891. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65284/0.72926. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65225/0.72955. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65016/0.72978. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64910/0.72967. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64773/0.73024. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64612/0.73095. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64964/0.73135. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69496/0.69356. Took 0.39 sec\n",
      "Epoch 1, Loss(train/val) 0.69375/0.69320. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69344/0.69302. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69301/0.69285. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69204/0.69285. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69221/0.69294. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69195/0.69311. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69255/0.69329. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69111/0.69334. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69042/0.69349. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69005/0.69381. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69010/0.69404. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68972/0.69436. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68930/0.69482. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68900/0.69540. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.69024/0.69590. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68838/0.69616. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68890/0.69652. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68881/0.69710. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68905/0.69743. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68929/0.69754. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68825/0.69804. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68780/0.69875. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68863/0.69908. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68795/0.69947. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68848/0.69988. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68883/0.70047. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68695/0.70120. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68698/0.70199. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68682/0.70259. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68537/0.70326. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68635/0.70374. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68509/0.70387. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68600/0.70431. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68635/0.70445. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68602/0.70452. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68510/0.70473. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68515/0.70466. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68494/0.70482. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68607/0.70426. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68497/0.70446. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68293/0.70472. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68501/0.70540. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68417/0.70519. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68298/0.70531. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68412/0.70563. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.68389/0.70592. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68194/0.70600. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68298/0.70635. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68239/0.70655. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68190/0.70666. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.68267/0.70703. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68014/0.70769. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.68120/0.70820. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.68133/0.70777. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67937/0.70776. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.68001/0.70888. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67934/0.70845. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67826/0.70964. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67749/0.70998. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67776/0.71006. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67672/0.70913. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.67652/0.70962. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67545/0.71091. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67566/0.70992. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67524/0.71115. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67639/0.71140. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67450/0.71123. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.67464/0.71041. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67450/0.71002. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67452/0.71040. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67046/0.71158. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.67272/0.71273. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.67186/0.71165. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66979/0.71187. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.67129/0.71029. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66863/0.71149. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66766/0.71239. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66723/0.71299. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66784/0.71224. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.66702/0.71355. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66673/0.71207. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66489/0.71420. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66576/0.71250. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66678/0.71142. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66657/0.70901. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66336/0.71170. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66283/0.71268. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66012/0.71416. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65904/0.71147. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66284/0.71150. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65667/0.71390. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.65843/0.71390. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65633/0.71517. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65889/0.71431. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65532/0.71334. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65808/0.71477. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65637/0.71537. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65379/0.71717. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65390/0.71443. Took 0.19 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69488/0.69462. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69365/0.69385. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69266/0.69358. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69236/0.69314. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69256/0.69326. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69179/0.69310. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69171/0.69320. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69129/0.69349. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69146/0.69331. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69032/0.69318. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69060/0.69328. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.69057/0.69360. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69053/0.69347. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69021/0.69356. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68911/0.69319. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68994/0.69341. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68947/0.69334. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68896/0.69389. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68935/0.69366. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68869/0.69342. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68880/0.69434. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68796/0.69398. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68815/0.69419. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68820/0.69459. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68801/0.69464. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68796/0.69459. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68769/0.69472. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68660/0.69491. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68692/0.69533. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68573/0.69525. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68607/0.69444. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68685/0.69453. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68479/0.69494. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68528/0.69420. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68447/0.69484. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68346/0.69411. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68434/0.69392. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68393/0.69487. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68206/0.69481. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68222/0.69422. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68295/0.69525. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68239/0.69448. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68226/0.69418. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68081/0.69307. Took 0.22 sec\n",
      "Epoch 44, Loss(train/val) 0.68001/0.69407. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68004/0.69425. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67717/0.69394. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67805/0.69557. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67767/0.69479. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67722/0.69537. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67665/0.69547. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67662/0.69521. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.67506/0.69482. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67700/0.69381. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67563/0.69557. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67408/0.69546. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67437/0.69558. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67201/0.69555. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67388/0.69563. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66951/0.69598. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67057/0.69499. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66980/0.69642. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67063/0.69638. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66949/0.69764. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66874/0.69804. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66738/0.69733. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 0.66723/0.69782. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66404/0.69712. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66168/0.69895. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66416/0.70017. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66298/0.69912. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66201/0.69861. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66087/0.69841. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65657/0.69963. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65750/0.70066. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65733/0.70000. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65525/0.70055. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65175/0.70301. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65418/0.70419. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65279/0.70611. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64980/0.70524. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65051/0.70495. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64946/0.70515. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64824/0.70667. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65100/0.70599. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64518/0.70571. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64285/0.70693. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64436/0.70678. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64119/0.70775. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64046/0.70839. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63801/0.71068. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63713/0.70943. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.63851/0.71183. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63706/0.71432. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63144/0.71314. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63015/0.71300. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63431/0.71402. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63104/0.71507. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63163/0.71456. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62541/0.71649. Took 0.19 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69546/0.69484. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69393/0.69408. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69398/0.69371. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69359/0.69331. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69291/0.69295. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69212/0.69269. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69194/0.69235. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69285/0.69225. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.69202/0.69193. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69185/0.69162. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69214/0.69136. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69144/0.69136. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69278/0.69113. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69076/0.69083. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69092/0.69069. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.69021/0.69072. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69074/0.69052. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68985/0.69043. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68942/0.69047. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68941/0.69045. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68973/0.69053. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68960/0.69062. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68880/0.69072. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68851/0.69078. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68781/0.69111. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68818/0.69133. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68773/0.69152. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68779/0.69153. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68744/0.69199. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68724/0.69240. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68734/0.69261. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68795/0.69339. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68658/0.69389. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68661/0.69448. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68489/0.69488. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68474/0.69552. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68582/0.69577. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68566/0.69588. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68479/0.69640. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68301/0.69686. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68175/0.69792. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68418/0.69864. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68209/0.69923. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68316/0.69986. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68205/0.70027. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68164/0.70147. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68138/0.70240. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67946/0.70330. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67975/0.70442. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68081/0.70514. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67817/0.70641. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67819/0.70759. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67692/0.70839. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67554/0.70942. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67627/0.71038. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67541/0.71146. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67891/0.71228. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67638/0.71285. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67625/0.71421. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67572/0.71518. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67392/0.71653. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67294/0.71826. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67384/0.71846. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67180/0.71978. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67453/0.72049. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.67165/0.72171. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67302/0.72256. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67197/0.72356. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67152/0.72516. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66970/0.72614. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66732/0.72772. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66944/0.72917. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66474/0.73191. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66789/0.73202. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66555/0.73307. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66735/0.73474. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66499/0.73635. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66725/0.73715. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66504/0.73843. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66541/0.73953. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65810/0.74255. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66236/0.74392. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66294/0.74303. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66008/0.74480. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66023/0.74677. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65989/0.74758. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65848/0.75015. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65927/0.75255. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65827/0.75182. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65655/0.75443. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65481/0.75715. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65481/0.75769. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65270/0.75939. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65161/0.76245. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65402/0.76362. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64891/0.76665. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64838/0.76704. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65061/0.76962. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64935/0.77102. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64500/0.77389. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69370/0.69468. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69291/0.69455. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69337/0.69449. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69373/0.69442. Took 0.22 sec\n",
      "Epoch 4, Loss(train/val) 0.69334/0.69434. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69276/0.69430. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69281/0.69421. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69267/0.69416. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.69310/0.69413. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69267/0.69411. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69131/0.69411. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69210/0.69410. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.69149/0.69409. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69171/0.69405. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.69121/0.69404. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69134/0.69408. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.69120/0.69405. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.69178/0.69403. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.69088/0.69392. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.69059/0.69380. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.69046/0.69375. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69027/0.69377. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68933/0.69371. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68957/0.69362. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.68976/0.69363. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68850/0.69375. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68803/0.69375. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68702/0.69375. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68955/0.69360. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68651/0.69358. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68754/0.69351. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68625/0.69353. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68603/0.69346. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68593/0.69310. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68380/0.69273. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68403/0.69273. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68492/0.69271. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68455/0.69308. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68374/0.69278. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68332/0.69275. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68302/0.69269. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68299/0.69260. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68290/0.69266. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68034/0.69243. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67953/0.69256. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67932/0.69243. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67950/0.69261. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67840/0.69282. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67842/0.69297. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67680/0.69247. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67660/0.69269. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67574/0.69337. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67604/0.69373. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67425/0.69371. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67427/0.69413. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67416/0.69444. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67320/0.69457. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67126/0.69448. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66833/0.69536. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66913/0.69668. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66865/0.69635. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66859/0.69668. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66661/0.69809. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66744/0.69749. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66483/0.69845. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66373/0.70060. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66542/0.70122. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66791/0.70164. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66237/0.70213. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66531/0.70320. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66098/0.70267. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65745/0.70532. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65820/0.70399. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65873/0.70329. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65885/0.70640. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65755/0.70622. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65563/0.70792. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65499/0.70666. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65184/0.70729. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64886/0.70910. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65276/0.70862. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65134/0.71058. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64890/0.71184. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65119/0.71074. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64600/0.71162. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64580/0.71214. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64633/0.71322. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63990/0.71434. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64191/0.71609. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64175/0.71683. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63664/0.71600. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63838/0.71603. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63893/0.71621. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63096/0.71629. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63039/0.71921. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62899/0.72052. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63182/0.72187. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62905/0.72236. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62874/0.72276. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62936/0.72064. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69398/0.70205. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69236/0.70422. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69157/0.70579. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69109/0.70677. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69053/0.70761. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68944/0.70858. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68924/0.70969. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68852/0.71062. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68851/0.71150. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68821/0.71211. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68736/0.71293. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68684/0.71341. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68637/0.71388. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68651/0.71435. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68640/0.71493. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68578/0.71555. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68531/0.71558. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68451/0.71686. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68304/0.71737. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68346/0.71827. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68299/0.71905. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68224/0.71929. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68216/0.72024. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68171/0.72128. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68124/0.72159. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68095/0.72244. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68075/0.72354. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67935/0.72442. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67997/0.72510. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67830/0.72580. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67814/0.72648. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67813/0.72731. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67700/0.72804. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67538/0.72862. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67651/0.72913. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67502/0.73031. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.67598/0.73114. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67508/0.73171. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67458/0.73224. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67246/0.73377. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67214/0.73558. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67163/0.73560. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67349/0.73652. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67198/0.73770. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67118/0.73895. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66894/0.74057. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66951/0.74122. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66965/0.74120. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66989/0.74316. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66820/0.74284. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66951/0.74419. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66700/0.74485. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66694/0.74598. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66534/0.74633. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66355/0.74789. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66570/0.74885. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66420/0.74937. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66339/0.74989. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66379/0.75073. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66158/0.75181. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66316/0.75038. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66422/0.75102. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 0.66228/0.75139. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66297/0.75184. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66287/0.75241. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65882/0.75329. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 0.65960/0.75436. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66064/0.75398. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65783/0.75298. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65712/0.75267. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65655/0.75510. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65730/0.75683. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65746/0.75729. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65710/0.75627. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65611/0.75532. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65399/0.75740. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65485/0.75636. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65638/0.75729. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65220/0.75820. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65258/0.75845. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65289/0.75677. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65134/0.75963. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65366/0.75861. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65037/0.75762. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 0.65117/0.76121. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65086/0.76020. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64743/0.76184. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64742/0.76215. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64808/0.76141. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64983/0.76211. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64702/0.76202. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64532/0.76358. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64750/0.76338. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64582/0.76197. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64812/0.76372. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64537/0.76043. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64654/0.76007. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64552/0.76095. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64362/0.76305. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64503/0.76656. Took 0.19 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69765/0.69012. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69480/0.69210. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69373/0.69347. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69181/0.69510. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69151/0.69673. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68951/0.69814. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69031/0.69937. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68913/0.69977. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68838/0.70126. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68802/0.70211. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68797/0.70280. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68757/0.70388. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68647/0.70453. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68559/0.70500. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68552/0.70559. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68585/0.70588. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68389/0.70608. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68467/0.70671. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68387/0.70643. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68406/0.70571. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68437/0.70583. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68173/0.70538. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68261/0.70568. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68322/0.70539. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68132/0.70544. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68147/0.70570. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68157/0.70552. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68091/0.70557. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68036/0.70527. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67995/0.70525. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67923/0.70525. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67967/0.70530. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67894/0.70517. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67910/0.70583. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67930/0.70534. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67915/0.70509. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67749/0.70488. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67779/0.70573. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67759/0.70465. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67818/0.70456. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67576/0.70480. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67720/0.70497. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67720/0.70456. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67651/0.70525. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67531/0.70524. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67648/0.70536. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67554/0.70374. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67519/0.70403. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67427/0.70470. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67706/0.70445. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67610/0.70517. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67532/0.70455. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67463/0.70558. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67496/0.70574. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67402/0.70515. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67415/0.70516. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67311/0.70552. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67454/0.70576. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67247/0.70579. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67392/0.70568. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67310/0.70634. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67413/0.70677. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67332/0.70626. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67194/0.70601. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67325/0.70678. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67245/0.70658. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67145/0.70750. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67120/0.70594. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67183/0.70698. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67146/0.70789. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67129/0.70800. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67153/0.70848. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67105/0.70858. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67120/0.71048. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66881/0.71003. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67084/0.71047. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66963/0.71109. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67002/0.71021. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66891/0.71107. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67090/0.71137. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66711/0.71175. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66861/0.71128. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66743/0.71178. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66812/0.71227. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66658/0.71380. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66694/0.71446. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66757/0.71454. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66872/0.71325. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66649/0.71529. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66585/0.71697. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66636/0.71578. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66675/0.71571. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66543/0.71593. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66462/0.71689. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66376/0.71715. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66428/0.71824. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66667/0.71798. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.66666/0.71798. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66436/0.71628. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66176/0.71854. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69330/0.69210. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69323/0.69231. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69253/0.69259. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69188/0.69278. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69219/0.69299. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69224/0.69330. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69165/0.69353. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69142/0.69387. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69217/0.69413. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69116/0.69442. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69039/0.69484. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68945/0.69533. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68965/0.69607. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68945/0.69679. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68879/0.69749. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68791/0.69834. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68805/0.69938. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68657/0.70045. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68642/0.70141. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68648/0.70239. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68501/0.70337. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68489/0.70455. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68458/0.70533. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68406/0.70599. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68411/0.70695. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68352/0.70766. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68245/0.70789. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68401/0.70833. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68328/0.70887. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68206/0.70887. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68321/0.70910. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68104/0.70953. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68042/0.70946. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68097/0.70983. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68033/0.71022. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67858/0.71120. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67788/0.71211. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67922/0.71181. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67734/0.71196. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67667/0.71237. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67738/0.71346. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67673/0.71365. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67647/0.71321. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67286/0.71346. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67203/0.71434. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67193/0.71501. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67226/0.71496. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67270/0.71540. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67077/0.71579. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66941/0.71639. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66963/0.71644. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67125/0.71900. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66766/0.71856. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66795/0.71975. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66759/0.72093. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66919/0.72046. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66347/0.71976. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66268/0.72233. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66396/0.72255. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66345/0.72411. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66013/0.72279. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66033/0.72387. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65899/0.72361. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65918/0.72600. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65541/0.72780. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65615/0.72726. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65460/0.72798. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65310/0.73110. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65063/0.73148. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64876/0.73241. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65056/0.73514. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64838/0.73648. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65083/0.73758. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64534/0.73820. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64734/0.73896. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64190/0.73942. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64151/0.73959. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65193/0.74222. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64097/0.74095. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64231/0.74102. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64099/0.74311. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63571/0.74445. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63704/0.74500. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63386/0.74446. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63752/0.74541. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63607/0.74731. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63626/0.74630. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63241/0.74814. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63349/0.74689. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63130/0.74988. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.62844/0.74710. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62909/0.75023. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62545/0.75087. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.62513/0.75157. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62734/0.75219. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62089/0.75349. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62113/0.75694. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62373/0.75568. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62769/0.75643. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62308/0.75766. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69322/0.70254. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68915/0.70409. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68902/0.70579. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68808/0.70719. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68671/0.70859. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68648/0.70985. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68585/0.71079. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68582/0.71190. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68495/0.71277. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68488/0.71334. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68568/0.71361. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68520/0.71406. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68373/0.71431. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68368/0.71474. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68308/0.71475. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68166/0.71495. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68296/0.71522. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68269/0.71548. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68163/0.71570. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68134/0.71604. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68128/0.71624. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68206/0.71606. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68197/0.71634. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68040/0.71610. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68003/0.71623. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67990/0.71618. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67965/0.71616. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67908/0.71630. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67963/0.71655. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67965/0.71671. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67934/0.71654. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67885/0.71656. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67889/0.71625. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67825/0.71613. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67958/0.71631. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67927/0.71617. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67817/0.71625. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67730/0.71597. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67705/0.71658. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67661/0.71698. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67682/0.71720. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67588/0.71719. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67577/0.71733. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67518/0.71743. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67507/0.71726. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67303/0.71779. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67384/0.71804. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67239/0.71813. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67360/0.71787. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67296/0.71761. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67260/0.71863. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67384/0.71803. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67147/0.71855. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66963/0.71884. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67014/0.71832. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67010/0.71892. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66961/0.72013. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66916/0.72005. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66781/0.71941. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66845/0.72026. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66754/0.72003. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66704/0.72072. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66584/0.72166. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66544/0.72301. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66347/0.72261. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66441/0.72437. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66383/0.72501. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66263/0.72604. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66511/0.72578. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66191/0.72667. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66138/0.72767. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66109/0.72730. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65711/0.72845. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65995/0.72991. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66070/0.73047. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65876/0.73314. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65991/0.73278. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65624/0.73227. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65984/0.73480. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65848/0.73489. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65521/0.73660. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65570/0.73648. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65481/0.73918. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65502/0.73999. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 0.65310/0.74148. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65263/0.74214. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65035/0.74441. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65046/0.74609. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65283/0.74589. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.64927/0.74687. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.64834/0.74886. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.64602/0.75024. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64537/0.75066. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.64437/0.75100. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64669/0.75188. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64358/0.75234. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64598/0.75374. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64471/0.75588. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64285/0.75522. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.64080/0.75874. Took 0.20 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69132/0.69118. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69056/0.69085. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69036/0.69016. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68964/0.68946. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68926/0.68849. Took 0.23 sec\n",
      "Epoch 5, Loss(train/val) 0.68892/0.68730. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68814/0.68621. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68865/0.68493. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68694/0.68340. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68590/0.68205. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68625/0.68135. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68526/0.67981. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68479/0.67900. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68554/0.67794. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68487/0.67720. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68464/0.67664. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68370/0.67601. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68383/0.67532. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68302/0.67443. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68393/0.67449. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68194/0.67399. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68336/0.67388. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68227/0.67296. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68236/0.67285. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68217/0.67294. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68048/0.67218. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68098/0.67271. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68233/0.67272. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68154/0.67250. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68062/0.67234. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68040/0.67250. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68051/0.67237. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67958/0.67271. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67911/0.67344. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67890/0.67324. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67951/0.67299. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67810/0.67377. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67713/0.67339. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67755/0.67370. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67786/0.67415. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67643/0.67355. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67690/0.67454. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67559/0.67444. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67544/0.67444. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67201/0.67486. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67299/0.67467. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67448/0.67454. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67285/0.67542. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67155/0.67528. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67112/0.67598. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67106/0.67692. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67079/0.67759. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67030/0.67697. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66905/0.67705. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66960/0.67724. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66827/0.67697. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66580/0.67735. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66664/0.67689. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66608/0.67796. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66380/0.67966. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66369/0.67802. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66273/0.67836. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65851/0.67789. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65894/0.67732. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65728/0.67653. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65760/0.67532. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65796/0.67578. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65598/0.67883. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65783/0.67758. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65411/0.67855. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65026/0.67757. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65111/0.67841. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64775/0.67686. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64818/0.67957. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65266/0.68069. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64685/0.67893. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64959/0.68055. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64278/0.67846. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64465/0.67844. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64211/0.68087. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64011/0.68060. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64016/0.68042. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63604/0.67947. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63818/0.67880. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63766/0.67827. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63540/0.67734. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63668/0.67867. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63555/0.67954. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63126/0.68035. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63139/0.68215. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62966/0.68238. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62947/0.68296. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62780/0.68414. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63093/0.68353. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62799/0.68422. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62167/0.68427. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62424/0.68535. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62234/0.68563. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62630/0.68458. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62117/0.68460. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69306/0.69831. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69170/0.69755. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69134/0.69771. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69060/0.69779. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68961/0.69808. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68994/0.69837. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68985/0.69864. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68911/0.69898. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68856/0.69973. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68859/0.69979. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68790/0.70014. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68824/0.70049. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68858/0.70082. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68710/0.70095. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68734/0.70141. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68747/0.70172. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68766/0.70193. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68774/0.70184. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68749/0.70190. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68662/0.70229. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68641/0.70242. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68502/0.70286. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68630/0.70293. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68592/0.70320. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68400/0.70314. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68486/0.70355. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68560/0.70352. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68414/0.70350. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68452/0.70382. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68459/0.70428. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68460/0.70424. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68459/0.70460. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68384/0.70502. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68384/0.70518. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68336/0.70533. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68369/0.70534. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68288/0.70568. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68299/0.70597. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68201/0.70581. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68193/0.70598. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68254/0.70619. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68181/0.70629. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68248/0.70642. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68224/0.70677. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68071/0.70642. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68115/0.70641. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68181/0.70680. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67977/0.70715. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67902/0.70742. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67973/0.70778. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67976/0.70775. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67962/0.70829. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67894/0.70830. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67884/0.70835. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67893/0.70889. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67728/0.70898. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67818/0.70904. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67751/0.70904. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67684/0.70985. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67801/0.70987. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67660/0.71016. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67528/0.71083. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67554/0.71100. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67579/0.71146. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67630/0.71133. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67583/0.71114. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67517/0.71159. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67625/0.71138. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67502/0.71229. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67361/0.71252. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67480/0.71249. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67458/0.71277. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67394/0.71237. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67365/0.71293. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67387/0.71362. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67298/0.71361. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67178/0.71347. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67197/0.71411. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67126/0.71364. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.67134/0.71392. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67162/0.71443. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67168/0.71439. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67152/0.71473. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66933/0.71553. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.67044/0.71578. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66782/0.71532. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66832/0.71514. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66831/0.71550. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66870/0.71558. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66688/0.71511. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66619/0.71586. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66742/0.71604. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66494/0.71596. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66882/0.71626. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66652/0.71613. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.66629/0.71623. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.66551/0.71751. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66616/0.71688. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66587/0.71720. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66361/0.71787. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69688/0.68803. Took 0.39 sec\n",
      "Epoch 1, Loss(train/val) 0.69345/0.68968. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69318/0.69102. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69231/0.69224. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69104/0.69343. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69085/0.69457. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68951/0.69581. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68984/0.69703. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68927/0.69830. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68924/0.69953. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68774/0.70072. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68845/0.70179. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68735/0.70279. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68702/0.70403. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68750/0.70499. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68674/0.70599. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68579/0.70694. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68568/0.70749. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68515/0.70833. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68558/0.70880. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68488/0.70918. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68376/0.70983. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68473/0.71014. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68319/0.71059. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68392/0.71076. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68260/0.71114. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68301/0.71185. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68278/0.71187. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68163/0.71216. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68145/0.71214. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68012/0.71241. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68047/0.71263. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68038/0.71220. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67876/0.71172. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67897/0.71157. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67817/0.71131. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67771/0.71149. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67812/0.71109. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67681/0.71070. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67725/0.71002. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67588/0.71006. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67546/0.70997. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67626/0.70866. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67544/0.70840. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67244/0.70800. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67382/0.70754. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67400/0.70727. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67283/0.70616. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67050/0.70555. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67026/0.70537. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67129/0.70433. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66991/0.70394. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67000/0.70315. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66875/0.70247. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66762/0.70215. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66584/0.70132. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66750/0.70077. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66631/0.69986. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66640/0.69901. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66626/0.69798. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66452/0.69716. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66479/0.69661. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66273/0.69564. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66120/0.69493. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66509/0.69357. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66241/0.69360. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66308/0.69259. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66089/0.69245. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66207/0.69180. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65931/0.69269. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65860/0.69175. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65789/0.69162. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65735/0.69136. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65602/0.68994. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65845/0.68984. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65520/0.68878. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65690/0.68890. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65639/0.68875. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65585/0.68675. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65335/0.68565. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 0.65422/0.68406. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65128/0.68378. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65344/0.68399. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65060/0.68422. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64988/0.68459. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64828/0.68405. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64990/0.68204. Took 0.23 sec\n",
      "Epoch 87, Loss(train/val) 0.64778/0.68046. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 0.64460/0.68087. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64659/0.68173. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64533/0.68246. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64574/0.68172. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64575/0.68238. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64249/0.68052. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64096/0.68076. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63999/0.68054. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64107/0.68158. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63925/0.68155. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63889/0.68217. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63871/0.68227. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69319/0.69212. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69341/0.69224. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69273/0.69230. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69291/0.69237. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69198/0.69238. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69282/0.69232. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69164/0.69226. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69179/0.69221. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69113/0.69212. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69072/0.69204. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69131/0.69191. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.69055/0.69182. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68911/0.69167. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68975/0.69153. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68931/0.69131. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68883/0.69109. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68850/0.69104. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68869/0.69072. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68837/0.69047. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68716/0.69023. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68658/0.69031. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68662/0.68994. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68592/0.68985. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68600/0.68950. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68459/0.68962. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68501/0.68973. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68424/0.68997. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68370/0.68981. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68263/0.68988. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68257/0.69008. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68210/0.69048. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68220/0.69048. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68120/0.69064. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68096/0.69073. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67693/0.69040. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68057/0.69095. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67845/0.69130. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67806/0.69175. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67911/0.69158. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67657/0.69162. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67715/0.69175. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67554/0.69151. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67573/0.69182. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67453/0.69239. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67242/0.69229. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67122/0.69282. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67051/0.69359. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66987/0.69452. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67304/0.69468. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66831/0.69503. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66961/0.69535. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66719/0.69652. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66787/0.69738. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66670/0.69696. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66743/0.69708. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66651/0.69733. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66340/0.69822. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66483/0.69956. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66240/0.69962. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66173/0.69988. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65984/0.70101. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65794/0.70206. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65666/0.70355. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65881/0.70417. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65596/0.70483. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65541/0.70621. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65280/0.70666. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65516/0.70880. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64969/0.70977. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65212/0.70984. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65286/0.71180. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64433/0.71256. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64386/0.71132. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64475/0.71174. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64369/0.71304. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64274/0.71621. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64576/0.71625. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64266/0.71651. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64145/0.71720. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63583/0.71872. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63895/0.71862. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63765/0.71851. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63779/0.72066. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63302/0.71925. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63349/0.72135. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63288/0.72415. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63306/0.72491. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63540/0.72545. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63021/0.72705. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62563/0.72690. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63251/0.72767. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62662/0.72862. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62210/0.73074. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62356/0.73552. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62536/0.73100. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62403/0.73226. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62111/0.73471. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62293/0.73945. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61560/0.73893. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61538/0.73999. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69490/0.69931. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69335/0.69941. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69231/0.69980. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69189/0.69927. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69120/0.69898. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69121/0.69905. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69106/0.69879. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69040/0.69832. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69033/0.69882. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68988/0.69841. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69007/0.69811. Took 0.22 sec\n",
      "Epoch 11, Loss(train/val) 0.68972/0.69780. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68917/0.69786. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68963/0.69760. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68809/0.69750. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68855/0.69820. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68898/0.69767. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68807/0.69708. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68741/0.69641. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68685/0.69712. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68785/0.69625. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68687/0.69601. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68700/0.69706. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68645/0.69595. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68463/0.69642. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68568/0.69574. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68508/0.69570. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68402/0.69625. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68583/0.69583. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68499/0.69551. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68395/0.69595. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68320/0.69573. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68362/0.69459. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68271/0.69445. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68236/0.69498. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68184/0.69490. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67951/0.69600. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68116/0.69606. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68169/0.69552. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67814/0.69532. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67730/0.69474. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67727/0.69459. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67804/0.69428. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67503/0.69409. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67352/0.69384. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67403/0.69425. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67276/0.69357. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67167/0.69273. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67208/0.69199. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.66919/0.69321. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66734/0.69103. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.66784/0.69233. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66575/0.69282. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66320/0.69032. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66343/0.69046. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65870/0.68999. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65904/0.68977. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.65655/0.68915. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65687/0.68989. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65542/0.68949. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65447/0.68899. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65443/0.69103. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65378/0.69302. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65005/0.69430. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65112/0.69307. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65129/0.69427. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65200/0.69484. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64972/0.69713. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64767/0.69695. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.64233/0.69737. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64645/0.69602. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64242/0.69731. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63982/0.69909. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63908/0.69741. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64134/0.69936. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63919/0.70077. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64047/0.70081. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64070/0.70101. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63683/0.70336. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63648/0.70467. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63292/0.70279. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63403/0.70404. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63341/0.70567. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.62936/0.70560. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63170/0.70698. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62742/0.70921. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62575/0.70708. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62707/0.70750. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62355/0.71176. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62579/0.71025. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62590/0.71287. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62510/0.71160. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62091/0.71785. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61817/0.71677. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61940/0.71385. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61960/0.71667. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62245/0.71519. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61661/0.71405. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61787/0.72092. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61797/0.71973. Took 0.19 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69501/0.68857. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69235/0.68875. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69215/0.68868. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69100/0.68879. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69050/0.68888. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69026/0.68909. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68967/0.68900. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68945/0.68917. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68924/0.68913. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68897/0.68925. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68887/0.68918. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68822/0.68922. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68798/0.68922. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68877/0.68924. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68768/0.68913. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68769/0.68894. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68775/0.68891. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68640/0.68875. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68694/0.68858. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68676/0.68846. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68584/0.68840. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68500/0.68830. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 0.68484/0.68802. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68569/0.68781. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68497/0.68796. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68528/0.68811. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68479/0.68826. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68490/0.68822. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68375/0.68802. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68478/0.68810. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68352/0.68792. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68353/0.68782. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68252/0.68789. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68287/0.68759. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68286/0.68742. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68257/0.68749. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68131/0.68771. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68165/0.68779. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68277/0.68780. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68061/0.68791. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68111/0.68847. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68015/0.68889. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67935/0.68903. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67923/0.68869. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67933/0.68907. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67886/0.68917. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67723/0.68935. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67790/0.68898. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67806/0.68892. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67641/0.68871. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67818/0.68844. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67561/0.68897. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67578/0.68914. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67601/0.68905. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67660/0.68886. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67428/0.68862. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67528/0.68835. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67448/0.68856. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67315/0.68863. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67345/0.68878. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67205/0.68841. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67315/0.69004. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67259/0.68921. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67135/0.68959. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67129/0.68872. Took 0.22 sec\n",
      "Epoch 65, Loss(train/val) 0.67257/0.68786. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67111/0.68802. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67040/0.68891. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67161/0.68838. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67055/0.68883. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66849/0.69048. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66649/0.68975. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66685/0.68971. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66821/0.69020. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66863/0.69032. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66673/0.69068. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66895/0.69042. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66646/0.69033. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66662/0.69080. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66699/0.69053. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66646/0.68950. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66445/0.69103. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66534/0.69113. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66604/0.69159. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66355/0.69018. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66347/0.69151. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66408/0.69162. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66549/0.69157. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66244/0.69194. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66116/0.69267. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66107/0.69265. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66112/0.69201. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.66073/0.69090. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66042/0.69178. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65858/0.69311. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65715/0.69268. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65913/0.69317. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65818/0.69447. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65716/0.69403. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.65703/0.69469. Took 0.19 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69567/0.69677. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69214/0.69763. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69173/0.69861. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69129/0.69967. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69067/0.70070. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68886/0.70194. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68911/0.70335. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68834/0.70447. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68744/0.70553. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68702/0.70673. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68708/0.70803. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68664/0.70909. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68635/0.71011. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68579/0.71122. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68632/0.71196. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68546/0.71305. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68448/0.71395. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68535/0.71421. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68443/0.71479. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68481/0.71568. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68391/0.71542. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68416/0.71592. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68344/0.71685. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68340/0.71703. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68300/0.71714. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68289/0.71771. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68296/0.71828. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68292/0.71914. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68143/0.71936. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68141/0.71976. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68262/0.71998. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68170/0.72061. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68221/0.72157. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68237/0.72115. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68111/0.72133. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68112/0.72175. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68037/0.72217. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67999/0.72303. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68120/0.72303. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67943/0.72333. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67897/0.72345. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68008/0.72350. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67874/0.72375. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67977/0.72399. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67989/0.72451. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67787/0.72467. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67878/0.72535. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67789/0.72621. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67777/0.72624. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67873/0.72674. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67772/0.72692. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67663/0.72704. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67707/0.72812. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67687/0.72884. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67660/0.72910. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67594/0.72938. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67651/0.73004. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67415/0.73086. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67602/0.73209. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67451/0.73198. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67529/0.73323. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67551/0.73345. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67403/0.73382. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67425/0.73451. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67364/0.73513. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67310/0.73548. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67197/0.73708. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67292/0.73721. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67058/0.73808. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67231/0.73834. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67326/0.73921. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67079/0.74038. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66903/0.74116. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67085/0.74167. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66854/0.74260. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66985/0.74335. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66814/0.74436. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66827/0.74496. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66650/0.74642. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66767/0.74786. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66883/0.74864. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66646/0.74983. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66623/0.75143. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66650/0.75053. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66394/0.75322. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66419/0.75449. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66430/0.75510. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66350/0.75649. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66048/0.75588. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66423/0.75655. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66145/0.75884. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66100/0.76091. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65970/0.76120. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65882/0.76321. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66027/0.76446. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65775/0.76528. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65795/0.76583. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65871/0.76784. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65797/0.76825. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65800/0.77079. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69491/0.69315. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69180/0.69591. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68963/0.69904. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68903/0.70175. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68825/0.70411. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68694/0.70652. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68633/0.70858. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68636/0.71065. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68656/0.71252. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68597/0.71387. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68635/0.71531. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68501/0.71635. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68551/0.71741. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68467/0.71832. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68478/0.71845. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68486/0.71954. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68437/0.72021. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68596/0.72080. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68481/0.72097. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68474/0.72153. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68341/0.72217. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68344/0.72266. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68341/0.72327. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68366/0.72369. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68393/0.72344. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68367/0.72364. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68367/0.72390. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68362/0.72379. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68237/0.72434. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68295/0.72470. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68239/0.72492. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68206/0.72535. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68272/0.72567. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68189/0.72615. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68219/0.72670. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68236/0.72628. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68146/0.72669. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68113/0.72697. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68241/0.72713. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68100/0.72757. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68160/0.72782. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68087/0.72752. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68188/0.72766. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67998/0.72748. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68012/0.72813. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68131/0.72899. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68130/0.72925. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68051/0.72959. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67941/0.73025. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67922/0.73085. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67923/0.73135. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68042/0.73113. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67960/0.73093. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67887/0.73119. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67794/0.73248. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67787/0.73231. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67776/0.73308. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67771/0.73297. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67907/0.73320. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67707/0.73402. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67743/0.73430. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67632/0.73471. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67752/0.73545. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67667/0.73568. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67631/0.73632. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67729/0.73616. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.67489/0.73728. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67518/0.73751. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67572/0.73815. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67539/0.73704. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67484/0.73804. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67447/0.73760. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.67371/0.73821. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67313/0.73855. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67340/0.73845. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67242/0.73865. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.67199/0.74114. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67020/0.74034. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67138/0.74241. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67188/0.74189. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67004/0.74176. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66845/0.74169. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67024/0.74350. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66804/0.74306. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66933/0.74321. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66775/0.74534. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66624/0.74507. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66625/0.74497. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66647/0.74381. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66547/0.74438. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66492/0.74622. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66499/0.74631. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66302/0.74721. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66363/0.74868. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66406/0.74790. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65952/0.74827. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66429/0.74856. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66087/0.74745. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66243/0.74785. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66093/0.74757. Took 0.18 sec\n",
      "ACC: 0.3958333333333333\n",
      "Epoch 0, Loss(train/val) 0.69439/0.69940. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69213/0.69845. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69140/0.69886. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68996/0.69909. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68992/0.69966. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68906/0.70010. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68892/0.70071. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68813/0.70111. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68815/0.70177. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68834/0.70214. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68787/0.70276. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68759/0.70280. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68713/0.70337. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68649/0.70361. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68671/0.70382. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68697/0.70416. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68588/0.70418. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68612/0.70450. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68635/0.70466. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68636/0.70493. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68523/0.70505. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68485/0.70523. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68441/0.70542. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68544/0.70539. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68533/0.70520. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68403/0.70539. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68423/0.70570. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68402/0.70617. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68305/0.70608. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68348/0.70618. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68297/0.70607. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68310/0.70651. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68299/0.70672. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68198/0.70682. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68229/0.70748. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68256/0.70728. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68261/0.70744. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68262/0.70745. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68178/0.70743. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68163/0.70693. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68129/0.70712. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68193/0.70771. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67982/0.70804. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67964/0.70842. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68000/0.70822. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67995/0.70870. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68002/0.70842. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67941/0.70933. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67943/0.70929. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67986/0.70956. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67909/0.70972. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67817/0.70968. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67772/0.71002. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67695/0.71004. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67765/0.71010. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68001/0.70975. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67717/0.71017. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67562/0.71048. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67688/0.71001. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67683/0.71000. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67605/0.71018. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67635/0.70991. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67547/0.70957. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67722/0.70999. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67446/0.70964. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67559/0.71045. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67509/0.70967. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67474/0.70967. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67442/0.70910. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67307/0.70948. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67442/0.70962. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67219/0.71017. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67285/0.71058. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67118/0.71060. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67273/0.71131. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67042/0.71095. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67045/0.71085. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66808/0.71057. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66923/0.71182. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66910/0.71187. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66795/0.71205. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66817/0.71250. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66635/0.71237. Took 0.22 sec\n",
      "Epoch 83, Loss(train/val) 0.66614/0.71279. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66800/0.71406. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66630/0.71293. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66619/0.71366. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66427/0.71236. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66475/0.71241. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66259/0.71395. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66378/0.71296. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66201/0.71318. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66304/0.71378. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66088/0.71376. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66135/0.71369. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65926/0.71436. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65755/0.71568. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65741/0.71611. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65823/0.71714. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65829/0.71651. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69296/0.69603. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69208/0.69734. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69134/0.69815. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69179/0.69867. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69114/0.69901. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69054/0.69941. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69070/0.69938. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69027/0.69948. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68946/0.69933. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68902/0.69939. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68903/0.69986. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68837/0.70002. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68835/0.70031. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68823/0.70080. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68790/0.70129. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68785/0.70147. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68642/0.70141. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68719/0.70183. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68628/0.70209. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68555/0.70232. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68518/0.70282. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68593/0.70344. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68451/0.70333. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68414/0.70364. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68360/0.70370. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68380/0.70450. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68312/0.70463. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68303/0.70538. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68246/0.70585. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68206/0.70545. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68102/0.70529. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68230/0.70504. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67913/0.70597. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68007/0.70588. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67885/0.70601. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67899/0.70565. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67880/0.70638. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.67697/0.70612. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67770/0.70633. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67652/0.70598. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67561/0.70599. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67485/0.70550. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67298/0.70623. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67610/0.70523. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67219/0.70513. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67417/0.70552. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66962/0.70440. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67304/0.70509. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67138/0.70414. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66967/0.70404. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66908/0.70381. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66742/0.70161. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66746/0.70160. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67078/0.70243. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66670/0.70005. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.66551/0.70008. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66575/0.70068. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66334/0.70084. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66495/0.69934. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66255/0.69963. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66231/0.69975. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66152/0.69967. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66027/0.70011. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66022/0.70000. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65829/0.70014. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65470/0.70156. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65588/0.70114. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65614/0.70023. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65432/0.69940. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65302/0.70021. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64995/0.70180. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64913/0.69975. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64696/0.70154. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65026/0.69735. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64920/0.69799. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64470/0.69808. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64196/0.70123. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64270/0.70063. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64472/0.70158. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64242/0.70293. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64330/0.70149. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63945/0.70348. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63929/0.70481. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63295/0.70364. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63558/0.70328. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63060/0.70557. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63192/0.70646. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63750/0.71076. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62646/0.71138. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62631/0.71088. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62879/0.70993. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62641/0.71058. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62466/0.71162. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62514/0.71056. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62264/0.71487. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62178/0.71519. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62177/0.71676. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61906/0.71422. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61854/0.71892. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61452/0.72004. Took 0.18 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69360/0.69112. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69267/0.68955. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69245/0.68869. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69171/0.68776. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69125/0.68689. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69101/0.68616. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.69047/0.68533. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69045/0.68499. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68930/0.68426. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68996/0.68392. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68939/0.68357. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68901/0.68308. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68835/0.68294. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68822/0.68274. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68743/0.68233. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68808/0.68218. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68786/0.68201. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68717/0.68211. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68600/0.68207. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68776/0.68210. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68665/0.68237. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68628/0.68234. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68676/0.68256. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68456/0.68254. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68612/0.68278. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68479/0.68266. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68595/0.68293. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68426/0.68311. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68383/0.68313. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68339/0.68338. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68283/0.68368. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68123/0.68412. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68216/0.68423. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68201/0.68434. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68213/0.68477. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68130/0.68503. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68084/0.68538. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68178/0.68545. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67881/0.68591. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67826/0.68605. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67780/0.68645. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67872/0.68720. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67866/0.68782. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67631/0.68859. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67694/0.68905. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67467/0.68978. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67658/0.69034. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67379/0.69087. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67415/0.69115. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67485/0.69160. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67449/0.69179. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67262/0.69280. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67116/0.69364. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66957/0.69466. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67111/0.69502. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67146/0.69590. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66759/0.69642. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66916/0.69738. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66728/0.69772. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66825/0.69768. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66599/0.69910. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66380/0.69938. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66384/0.70041. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66156/0.70092. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66217/0.70166. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65855/0.70185. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65859/0.70340. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65910/0.70426. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65379/0.70594. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65774/0.70628. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65483/0.70551. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65406/0.70666. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64826/0.70835. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64866/0.70829. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64617/0.70828. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65030/0.70939. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64573/0.71020. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64641/0.71232. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64151/0.71304. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64013/0.71488. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64196/0.71575. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64094/0.71621. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63956/0.71596. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63628/0.71907. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63776/0.72335. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63309/0.72254. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63488/0.72205. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63432/0.72357. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63135/0.72498. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63219/0.72327. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.62810/0.72640. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63067/0.72689. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62791/0.72831. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62546/0.72699. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62374/0.72836. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62377/0.72940. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62318/0.73210. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62620/0.73154. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62318/0.73396. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62426/0.73480. Took 0.20 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69402/0.69662. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69274/0.69624. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69242/0.69620. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69265/0.69635. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69186/0.69666. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69188/0.69696. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69150/0.69720. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69159/0.69744. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69106/0.69768. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69089/0.69789. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69045/0.69820. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68971/0.69846. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68896/0.69874. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68889/0.69895. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68840/0.69923. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68829/0.69942. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68790/0.69963. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68702/0.69941. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68679/0.69958. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68613/0.70014. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68617/0.70041. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68450/0.70045. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68382/0.70029. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68265/0.70059. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68246/0.70091. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67984/0.70018. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68009/0.70018. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67893/0.70068. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67836/0.69991. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67503/0.69944. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67307/0.69998. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67263/0.69975. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67006/0.70039. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66959/0.70100. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66864/0.70006. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66780/0.70095. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66715/0.70226. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66677/0.70302. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66508/0.70302. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66168/0.70446. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66347/0.70399. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66051/0.70549. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66210/0.70562. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66061/0.70639. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65882/0.70822. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65713/0.70960. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65828/0.71153. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65588/0.71318. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65477/0.71432. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65204/0.71635. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65251/0.71724. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.64908/0.71673. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.64945/0.71742. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64858/0.71993. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64950/0.72038. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64763/0.71872. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64704/0.72027. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64348/0.72078. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64613/0.72332. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64530/0.72135. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64235/0.72014. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64281/0.71988. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64194/0.71817. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.64442/0.71974. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63953/0.72083. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64057/0.72299. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63815/0.72453. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63796/0.72260. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63446/0.72525. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63507/0.72483. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63223/0.72516. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63478/0.72576. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63217/0.72634. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63170/0.72507. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62839/0.72639. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.63041/0.72837. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62734/0.72998. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62458/0.72795. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62613/0.72623. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62475/0.72856. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62263/0.72813. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61878/0.73017. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62175/0.72987. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.61787/0.72996. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61272/0.73231. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61821/0.72998. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61281/0.73049. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.61393/0.73122. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61457/0.73272. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61129/0.73281. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60862/0.73399. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.60401/0.73586. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60915/0.73406. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60489/0.73675. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.60334/0.73498. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60197/0.73559. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60174/0.73740. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60145/0.73676. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.59775/0.73422. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.59926/0.73500. Took 0.19 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69518/0.69404. Took 0.30 sec\n",
      "Epoch 1, Loss(train/val) 0.69123/0.68965. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69127/0.68768. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69150/0.68660. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69069/0.68594. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69096/0.68581. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68973/0.68550. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69012/0.68530. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68913/0.68499. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68994/0.68495. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68962/0.68479. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68936/0.68457. Took 0.22 sec\n",
      "Epoch 12, Loss(train/val) 0.68910/0.68436. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68842/0.68421. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.68911/0.68418. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68881/0.68386. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.68796/0.68371. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68918/0.68356. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.68770/0.68342. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68819/0.68319. Took 0.22 sec\n",
      "Epoch 20, Loss(train/val) 0.68666/0.68285. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68626/0.68272. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 0.68738/0.68257. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68615/0.68241. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.68674/0.68202. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68526/0.68202. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68537/0.68196. Took 0.22 sec\n",
      "Epoch 27, Loss(train/val) 0.68451/0.68190. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.68644/0.68154. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.68509/0.68146. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68482/0.68182. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68387/0.68203. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68358/0.68127. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68295/0.68069. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68229/0.68116. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68236/0.68056. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68141/0.68114. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68094/0.68045. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68020/0.68028. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67920/0.68028. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67794/0.68052. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67835/0.67985. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67765/0.68052. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67831/0.67999. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67774/0.68004. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67547/0.67957. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67701/0.68045. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67430/0.67948. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67301/0.68002. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67392/0.67997. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67207/0.68008. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67283/0.68079. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67249/0.68016. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67244/0.68106. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67073/0.68261. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67016/0.68334. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66759/0.68262. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66720/0.68340. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66799/0.68400. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66655/0.68356. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66743/0.68448. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66454/0.68429. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66462/0.68613. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66352/0.68708. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66157/0.68564. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66329/0.68727. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66255/0.68719. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66097/0.68912. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66014/0.68826. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65520/0.68835. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65740/0.68857. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65824/0.68982. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65615/0.69065. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65384/0.69037. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65159/0.69365. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65521/0.69432. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64846/0.69517. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64905/0.69664. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64953/0.69510. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64960/0.69667. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64503/0.69737. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64750/0.70102. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64523/0.70010. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64314/0.69845. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64369/0.70235. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64367/0.70104. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64154/0.69820. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63891/0.70135. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63750/0.70417. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63773/0.70516. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63675/0.70534. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63438/0.70621. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.63613/0.70574. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63194/0.70783. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63042/0.70916. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63276/0.70670. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63289/0.70805. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62993/0.70847. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62824/0.71067. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62597/0.71253. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69118/0.69300. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69098/0.69301. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69057/0.69293. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69102/0.69276. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69075/0.69259. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68994/0.69237. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68955/0.69212. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68872/0.69199. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68922/0.69177. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68874/0.69154. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68856/0.69135. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68826/0.69112. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68783/0.69095. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68638/0.69052. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68775/0.69046. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68631/0.69038. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68717/0.69030. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68543/0.69005. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68530/0.68965. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68537/0.68954. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68447/0.68934. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68493/0.68920. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68342/0.68901. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68417/0.68881. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68219/0.68897. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68309/0.68883. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68371/0.68898. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68103/0.68865. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68124/0.68831. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68146/0.68801. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67930/0.68775. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67995/0.68767. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67843/0.68739. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67855/0.68737. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67683/0.68716. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67497/0.68758. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67639/0.68747. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67535/0.68691. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67259/0.68733. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67302/0.68749. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67197/0.68766. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67255/0.68731. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67107/0.68768. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66672/0.68780. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66886/0.68795. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66845/0.68813. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66600/0.68871. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66609/0.68899. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66297/0.68930. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66208/0.68999. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66299/0.69102. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.65908/0.69236. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65967/0.69261. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65919/0.69370. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65428/0.69454. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65607/0.69476. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65724/0.69501. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65443/0.69464. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65074/0.69573. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65118/0.69582. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.64785/0.69913. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64743/0.70147. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64890/0.70294. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.64894/0.70236. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64752/0.70536. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64391/0.70890. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64451/0.70713. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63976/0.71319. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63938/0.71286. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63854/0.71079. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63677/0.71470. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63644/0.71574. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63685/0.71403. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63814/0.71802. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.63444/0.71764. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.62949/0.71828. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62707/0.72160. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63273/0.72617. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62661/0.72591. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62357/0.72609. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62390/0.73064. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62369/0.73181. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62694/0.73199. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.61614/0.73609. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62191/0.73511. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61627/0.73931. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61537/0.74275. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.61035/0.74174. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61403/0.73936. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60915/0.74483. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60969/0.74574. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61262/0.74957. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60496/0.75126. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60468/0.74604. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60692/0.75062. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60162/0.74729. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60101/0.74696. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60159/0.75436. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59668/0.74881. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.60406/0.75523. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69383/0.68725. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69220/0.68792. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69110/0.68878. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69150/0.68927. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69102/0.69002. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69092/0.69065. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69056/0.69135. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68992/0.69217. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69001/0.69281. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68952/0.69357. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68972/0.69391. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68898/0.69450. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68870/0.69489. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68923/0.69510. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68866/0.69543. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68808/0.69582. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68800/0.69621. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68747/0.69638. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68711/0.69670. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68742/0.69697. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68711/0.69718. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68710/0.69753. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68627/0.69744. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68567/0.69753. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68600/0.69782. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68485/0.69809. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68542/0.69831. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68507/0.69860. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68508/0.69885. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68453/0.69917. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68331/0.69942. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68363/0.69946. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68342/0.69955. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68301/0.69941. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68247/0.70051. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68193/0.70101. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68303/0.70088. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68148/0.70109. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68176/0.70114. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67991/0.70085. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67945/0.70098. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68065/0.70110. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67910/0.70139. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67979/0.70169. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67694/0.70205. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67803/0.70251. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67734/0.70278. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67725/0.70344. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67567/0.70353. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67682/0.70390. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67594/0.70406. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67491/0.70405. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67485/0.70348. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67435/0.70397. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67260/0.70451. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67435/0.70502. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67259/0.70491. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67243/0.70550. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67220/0.70518. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67248/0.70451. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66934/0.70475. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67059/0.70529. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67047/0.70503. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66874/0.70580. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66826/0.70535. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66826/0.70658. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66807/0.70725. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66477/0.70759. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66781/0.70712. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66870/0.70710. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66412/0.70711. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66486/0.70754. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66552/0.70697. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66536/0.70678. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66451/0.70747. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66195/0.70821. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66298/0.70701. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66178/0.70822. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66172/0.70954. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65859/0.70910. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65837/0.70892. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65879/0.70876. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65886/0.70799. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65751/0.70956. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65810/0.70775. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65460/0.70897. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65819/0.70909. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65312/0.71010. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65543/0.71032. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65175/0.71084. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65321/0.70895. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65226/0.70842. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65201/0.70755. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65139/0.70615. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65102/0.70504. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64693/0.70680. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64837/0.70719. Took 0.22 sec\n",
      "Epoch 97, Loss(train/val) 0.64820/0.70702. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64512/0.70742. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64517/0.70968. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69060/0.69069. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68847/0.69102. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68739/0.69052. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68795/0.69015. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68612/0.68991. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68660/0.68964. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68600/0.68943. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68561/0.68909. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68569/0.68894. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68399/0.68918. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68406/0.68914. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68390/0.68890. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68365/0.68896. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68310/0.68902. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68283/0.68867. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68244/0.68833. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68125/0.68803. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68141/0.68865. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68048/0.68827. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68124/0.68818. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67982/0.68781. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67963/0.68744. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68033/0.68751. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67970/0.68658. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67824/0.68696. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67866/0.68644. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67787/0.68650. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67755/0.68593. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67773/0.68537. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67797/0.68546. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67672/0.68587. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67581/0.68598. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67549/0.68624. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67595/0.68666. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67483/0.68630. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67378/0.68654. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67399/0.68647. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67138/0.68676. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67106/0.68698. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67264/0.68781. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67145/0.68793. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67026/0.68780. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66857/0.68851. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66951/0.68797. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66801/0.68923. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66701/0.68992. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66739/0.68956. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66581/0.69084. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66674/0.69151. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66513/0.69222. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66723/0.69259. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66593/0.69411. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66434/0.69514. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66321/0.69535. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66376/0.69619. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66298/0.69649. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65957/0.69727. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65974/0.69877. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66065/0.69988. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66118/0.69949. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65937/0.69923. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65845/0.70129. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65693/0.70133. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66039/0.70074. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65836/0.70097. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65808/0.70235. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65935/0.70200. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65448/0.70244. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65549/0.70311. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65423/0.70416. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65474/0.70512. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65296/0.70554. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65460/0.70492. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65420/0.70570. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65403/0.70653. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65260/0.70695. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65266/0.70802. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65182/0.70777. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65128/0.70943. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65196/0.70944. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64937/0.70939. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64865/0.70981. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65046/0.71064. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65043/0.71148. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64849/0.71218. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64696/0.71223. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64867/0.71290. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64748/0.71261. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64545/0.71193. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64490/0.71389. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64579/0.71370. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64359/0.71278. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64301/0.71498. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64426/0.71297. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64537/0.71455. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64465/0.71457. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64250/0.71552. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64048/0.71542. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64044/0.71729. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63809/0.71565. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.68958/0.70138. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68796/0.70082. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68694/0.70093. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68633/0.70088. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68562/0.70113. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68519/0.70104. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68522/0.70149. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68412/0.70168. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68347/0.70245. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68336/0.70285. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68219/0.70377. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68209/0.70415. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68158/0.70506. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68113/0.70554. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68050/0.70652. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67986/0.70723. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67967/0.70812. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.67904/0.70934. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67854/0.70969. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67872/0.71087. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67741/0.71115. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67769/0.71268. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67681/0.71391. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67712/0.71507. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67616/0.71570. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67621/0.71644. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67495/0.71773. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67404/0.71947. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67440/0.72055. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67380/0.72192. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67377/0.72346. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67301/0.72376. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67246/0.72535. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67364/0.72702. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67283/0.72755. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67169/0.73001. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67260/0.73037. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67106/0.73128. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67079/0.73309. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66993/0.73462. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67045/0.73517. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.66899/0.73746. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67004/0.73767. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66974/0.73824. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66926/0.73892. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66813/0.73956. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66881/0.73993. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66904/0.74086. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66891/0.74169. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66728/0.74194. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66825/0.74300. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66788/0.74437. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66623/0.74426. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66561/0.74634. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66734/0.74686. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66568/0.74782. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66721/0.74763. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66679/0.74802. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66519/0.74877. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66523/0.74932. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66554/0.74945. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66623/0.74928. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66420/0.75095. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66421/0.75135. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66348/0.75190. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66373/0.75111. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66366/0.75264. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66330/0.75344. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66405/0.75416. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66308/0.75430. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66220/0.75511. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66130/0.75426. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66170/0.75572. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66270/0.75571. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66017/0.75733. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66133/0.75655. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66100/0.75778. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65791/0.75905. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66199/0.75894. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65923/0.75692. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66026/0.75842. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65688/0.75943. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65812/0.75996. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65752/0.75993. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65758/0.76043. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65882/0.76011. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65830/0.76188. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65488/0.76279. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65761/0.76237. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65695/0.76331. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65274/0.76457. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65686/0.76402. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65468/0.76419. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65341/0.76544. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65542/0.76604. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65346/0.76579. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65456/0.76563. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65194/0.76690. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65087/0.76749. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65011/0.76797. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69530/0.69016. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68943/0.68906. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68910/0.68883. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69000/0.68848. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68895/0.68810. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68923/0.68787. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68862/0.68763. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68879/0.68721. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68799/0.68693. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68767/0.68676. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68830/0.68643. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68719/0.68618. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68714/0.68600. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68582/0.68581. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68657/0.68568. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68558/0.68537. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.68437/0.68541. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68512/0.68537. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68327/0.68524. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68388/0.68514. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68344/0.68514. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68416/0.68513. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68264/0.68503. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68281/0.68496. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68296/0.68495. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68104/0.68481. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68159/0.68475. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68174/0.68456. Took 0.22 sec\n",
      "Epoch 28, Loss(train/val) 0.68098/0.68467. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68171/0.68506. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68108/0.68494. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67924/0.68489. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67920/0.68481. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67920/0.68457. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67752/0.68412. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67770/0.68393. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67948/0.68420. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67704/0.68429. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67707/0.68457. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67693/0.68449. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67571/0.68449. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67585/0.68425. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67482/0.68395. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67566/0.68444. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67500/0.68445. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67310/0.68416. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67206/0.68401. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67273/0.68449. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67064/0.68508. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67235/0.68519. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67088/0.68516. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66955/0.68488. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66949/0.68530. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66997/0.68580. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66956/0.68659. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66671/0.68778. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66816/0.68758. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66767/0.68806. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66564/0.68869. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66560/0.68815. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66411/0.68830. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66336/0.68869. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66273/0.68918. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66421/0.69000. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66291/0.69102. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66233/0.69218. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66351/0.69237. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65970/0.69254. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65708/0.69354. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66041/0.69398. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65779/0.69495. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65815/0.69623. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65584/0.69774. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65665/0.69670. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65411/0.69762. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65293/0.69765. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65297/0.70117. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65145/0.70115. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65161/0.70023. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64906/0.69943. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65277/0.70237. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64673/0.70434. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.64942/0.70506. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64496/0.70489. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64624/0.70459. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64216/0.70614. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64669/0.70853. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64466/0.70758. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64436/0.70835. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64146/0.70831. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64193/0.71243. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64290/0.71103. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64280/0.71293. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63714/0.71518. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63938/0.71292. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64096/0.71392. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63471/0.71516. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63794/0.71643. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63519/0.71557. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63662/0.71938. Took 0.19 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69481/0.69010. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68939/0.69074. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68864/0.69079. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68904/0.69051. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68870/0.69022. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68819/0.69022. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68817/0.69010. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68809/0.69000. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68778/0.68990. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68664/0.68988. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68694/0.68978. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68625/0.68941. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68585/0.68920. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68507/0.68901. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68551/0.68888. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68527/0.68882. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68478/0.68861. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68549/0.68857. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68476/0.68853. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68409/0.68834. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68408/0.68829. Took 0.24 sec\n",
      "Epoch 21, Loss(train/val) 0.68378/0.68781. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68336/0.68772. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68199/0.68758. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68175/0.68737. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68106/0.68746. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68165/0.68746. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68099/0.68710. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68102/0.68704. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68105/0.68714. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67938/0.68660. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67901/0.68717. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67967/0.68679. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67801/0.68696. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67750/0.68667. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67825/0.68689. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67732/0.68692. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67784/0.68626. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67701/0.68697. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67678/0.68692. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67503/0.68666. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67570/0.68662. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67597/0.68691. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67649/0.68749. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 0.67380/0.68733. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67407/0.68755. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67370/0.68766. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67390/0.68797. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67144/0.68814. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67386/0.68797. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67182/0.68787. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66991/0.68829. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67082/0.68872. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67152/0.68888. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67005/0.68932. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66886/0.68946. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 0.66781/0.68947. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66575/0.69024. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66824/0.69024. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66629/0.69063. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66554/0.69173. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66435/0.69176. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 0.66380/0.69146. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66382/0.69206. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66433/0.69245. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66226/0.69208. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66088/0.69285. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66037/0.69324. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 0.66022/0.69167. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65862/0.69190. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66001/0.69250. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65686/0.69290. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65510/0.69332. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65560/0.69435. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65473/0.69443. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65468/0.69526. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65519/0.69527. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65515/0.69539. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65463/0.69619. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65374/0.69613. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65176/0.69694. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65110/0.69801. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65196/0.69846. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65069/0.69837. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65012/0.69888. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65144/0.69972. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64669/0.70060. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64768/0.70139. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64288/0.70136. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64561/0.70214. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64500/0.70298. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64447/0.70267. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64363/0.70318. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64282/0.70435. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64245/0.70518. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63753/0.70543. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64085/0.70420. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64115/0.70422. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64073/0.70511. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63734/0.70599. Took 0.20 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69172/0.70172. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68911/0.70198. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68811/0.70127. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68743/0.70102. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68620/0.70141. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68556/0.70099. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68507/0.70198. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68385/0.70254. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68367/0.70311. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68363/0.70350. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68310/0.70453. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68205/0.70526. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68258/0.70573. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68264/0.70645. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68262/0.70713. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68173/0.70770. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68141/0.70889. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68195/0.70945. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68074/0.70889. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68083/0.71015. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68029/0.71080. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68066/0.71111. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68032/0.71196. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67964/0.71208. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68041/0.71252. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67999/0.71259. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67803/0.71270. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67982/0.71312. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67886/0.71374. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67861/0.71349. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67806/0.71513. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67809/0.71534. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67739/0.71594. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67758/0.71598. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67684/0.71676. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67739/0.71641. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67646/0.71712. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67579/0.71713. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67706/0.71719. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67708/0.71792. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67520/0.71734. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67550/0.71879. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67511/0.71893. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67408/0.71941. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67399/0.71950. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67332/0.71979. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67391/0.72017. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67433/0.72018. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67186/0.72032. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67478/0.72016. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67319/0.72078. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67346/0.72105. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67184/0.72069. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67301/0.72200. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67231/0.72202. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67133/0.72243. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67097/0.72281. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67053/0.72307. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67021/0.72331. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67079/0.72383. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66920/0.72367. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66937/0.72427. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66809/0.72510. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66927/0.72439. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66800/0.72466. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66937/0.72372. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66772/0.72472. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66831/0.72440. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66761/0.72651. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66619/0.72569. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66485/0.72597. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66436/0.72651. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66443/0.72714. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66202/0.72903. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66380/0.72711. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66323/0.72920. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66281/0.72917. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66250/0.73075. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66180/0.73180. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66190/0.73195. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65933/0.73170. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66051/0.73166. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66227/0.73439. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65887/0.73236. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65890/0.73460. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65744/0.73346. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65771/0.73407. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65664/0.73615. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65714/0.73641. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65428/0.74013. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65280/0.73938. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65402/0.74069. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65248/0.74121. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65272/0.74255. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65265/0.74498. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65045/0.74399. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64863/0.74506. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64995/0.74680. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65095/0.74941. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64868/0.74697. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69036/0.68389. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68962/0.68466. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68864/0.68506. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68814/0.68555. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68798/0.68618. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68700/0.68647. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68751/0.68692. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68635/0.68746. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68593/0.68782. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68510/0.68806. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68581/0.68827. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68455/0.68847. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68446/0.68888. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68444/0.68911. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68405/0.68951. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68338/0.68983. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68242/0.69022. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68308/0.69066. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68310/0.69147. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68283/0.69201. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68346/0.69263. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68045/0.69340. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68094/0.69400. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68088/0.69438. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68150/0.69458. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68031/0.69550. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68038/0.69629. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67923/0.69705. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67959/0.69762. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67868/0.69829. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67820/0.69867. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67755/0.69925. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67828/0.70006. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67770/0.70043. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67579/0.70116. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67560/0.70192. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67515/0.70292. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67387/0.70475. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67354/0.70616. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67243/0.70798. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67060/0.70838. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67161/0.70830. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67156/0.70943. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66975/0.70974. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67033/0.70989. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66885/0.71163. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66928/0.71309. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66623/0.71529. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66728/0.71540. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66679/0.71557. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66691/0.71543. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66334/0.71693. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66266/0.71837. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66112/0.71815. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66069/0.71800. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65875/0.71900. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65953/0.71923. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65953/0.71996. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65883/0.72224. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65831/0.72159. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65588/0.72388. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65355/0.72389. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65548/0.72566. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65300/0.72546. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65042/0.72493. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64971/0.72633. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64881/0.72612. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64641/0.73027. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64864/0.73008. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64229/0.73038. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64490/0.73273. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64361/0.73326. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64364/0.73511. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.64228/0.73649. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63875/0.73929. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.63622/0.74027. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63666/0.74129. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63371/0.74453. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63179/0.74281. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63369/0.74594. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63428/0.74783. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63097/0.75090. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63060/0.74783. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62652/0.75211. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62438/0.75553. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62600/0.75522. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61989/0.75722. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62361/0.75656. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61943/0.76273. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61850/0.76489. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61612/0.76843. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61356/0.76784. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61200/0.76787. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60881/0.77251. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61219/0.77764. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60843/0.78280. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60361/0.77567. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.60464/0.78492. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60565/0.78941. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60474/0.79172. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69305/0.69203. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69088/0.69116. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69032/0.69031. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69045/0.68963. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69010/0.68901. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68986/0.68844. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68932/0.68787. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68892/0.68722. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68874/0.68660. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68891/0.68606. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68842/0.68580. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68752/0.68537. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68749/0.68498. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68764/0.68463. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68684/0.68425. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68729/0.68403. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68609/0.68385. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68759/0.68365. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68619/0.68351. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68602/0.68338. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68674/0.68354. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68560/0.68340. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68624/0.68347. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68488/0.68343. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68546/0.68339. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68513/0.68368. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68436/0.68369. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68445/0.68371. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68329/0.68393. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68342/0.68428. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68412/0.68428. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68358/0.68464. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68292/0.68465. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68269/0.68460. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68176/0.68484. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68166/0.68511. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68211/0.68540. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68059/0.68583. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68084/0.68650. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68012/0.68691. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67834/0.68708. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67902/0.68794. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67977/0.68826. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67843/0.68896. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67891/0.68926. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67836/0.68965. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67612/0.69040. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67636/0.69077. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67520/0.69106. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67478/0.69199. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67609/0.69326. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.67250/0.69381. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67346/0.69418. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67394/0.69414. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67132/0.69469. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67119/0.69524. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67245/0.69551. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67138/0.69506. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67161/0.69654. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.66976/0.69748. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66995/0.69744. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66740/0.69805. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66579/0.69877. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66804/0.69922. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66718/0.69960. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66579/0.70033. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66548/0.70156. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.66322/0.70224. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66536/0.70212. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66259/0.70330. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66209/0.70264. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66140/0.70369. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66071/0.70320. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65997/0.70377. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65827/0.70535. Took 0.22 sec\n",
      "Epoch 75, Loss(train/val) 0.65746/0.70471. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65858/0.70503. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65893/0.70493. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65740/0.70574. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65576/0.70641. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65586/0.70657. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65557/0.70667. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65179/0.70739. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65064/0.70617. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 0.65309/0.70650. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65130/0.70679. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65056/0.70749. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65090/0.70642. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65114/0.70619. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64800/0.70757. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64986/0.70661. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64477/0.70656. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64479/0.70719. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64612/0.70766. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64501/0.70755. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64402/0.70626. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64283/0.70689. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64380/0.70711. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64351/0.70772. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63855/0.70671. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69408/0.69438. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.69224/0.69206. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69239/0.69163. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69171/0.69144. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69082/0.69148. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.69145/0.69157. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69085/0.69163. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69080/0.69206. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69012/0.69204. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68978/0.69214. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68903/0.69202. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68907/0.69224. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68902/0.69196. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68875/0.69199. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68827/0.69183. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68763/0.69142. Took 0.22 sec\n",
      "Epoch 16, Loss(train/val) 0.68703/0.69160. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68716/0.69165. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68708/0.69132. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68647/0.69098. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.68669/0.69075. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68593/0.69065. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68464/0.69050. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68563/0.69062. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68551/0.69023. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68396/0.69043. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68317/0.69030. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68339/0.69013. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68252/0.69041. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68255/0.69069. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68127/0.69046. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68102/0.69033. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68060/0.68994. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68037/0.69073. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68083/0.69001. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68025/0.69009. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67855/0.68982. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67667/0.68983. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67717/0.69008. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67713/0.68997. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67663/0.69020. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67655/0.69012. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67621/0.69036. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67316/0.69069. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67414/0.69100. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67413/0.69099. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67345/0.69101. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67239/0.69128. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67137/0.69136. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67308/0.69030. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67086/0.69120. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67170/0.69134. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.66992/0.69224. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66972/0.69146. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66743/0.69290. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66472/0.69470. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66622/0.69422. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66501/0.69425. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66619/0.69525. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66566/0.69581. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66243/0.69660. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66510/0.69680. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66422/0.69706. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66197/0.69693. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66009/0.69674. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66166/0.69765. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65962/0.69880. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66058/0.69795. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65845/0.69867. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65691/0.70095. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65755/0.69924. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65732/0.70262. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65514/0.70096. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65284/0.70158. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65706/0.70284. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65572/0.70242. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65402/0.70314. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65132/0.70485. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64843/0.70703. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64814/0.70591. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64874/0.70722. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64881/0.70772. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.64868/0.71043. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64867/0.70996. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64893/0.71066. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64829/0.71089. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64561/0.71387. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64360/0.71523. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64373/0.71497. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64108/0.71236. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64481/0.71359. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64276/0.71500. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64313/0.71457. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64251/0.71619. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64188/0.71582. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63935/0.71665. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63935/0.71768. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63411/0.71922. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63544/0.72132. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63452/0.72373. Took 0.19 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69440/0.70137. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69386/0.70215. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69312/0.70169. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69228/0.70107. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69157/0.70026. Took 0.23 sec\n",
      "Epoch 5, Loss(train/val) 0.69179/0.69951. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69186/0.69884. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69187/0.69822. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69127/0.69794. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69070/0.69739. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69042/0.69696. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69028/0.69630. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68917/0.69571. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68915/0.69508. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68991/0.69430. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68911/0.69394. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68920/0.69317. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68872/0.69294. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68884/0.69223. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68850/0.69161. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68794/0.69149. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68779/0.69078. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68779/0.69006. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68756/0.69006. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68707/0.68904. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68690/0.68904. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68651/0.68848. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68660/0.68885. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68796/0.68817. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.68555/0.68798. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68586/0.68738. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68521/0.68719. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68626/0.68623. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68542/0.68542. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68544/0.68525. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68529/0.68524. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68388/0.68482. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68457/0.68558. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68498/0.68461. Took 0.22 sec\n",
      "Epoch 39, Loss(train/val) 0.68473/0.68525. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68456/0.68433. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68410/0.68353. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68277/0.68337. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68236/0.68389. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68455/0.68230. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68135/0.68323. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68349/0.68140. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68161/0.68139. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67950/0.68116. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.68107/0.68152. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67988/0.68142. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67967/0.68024. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67977/0.68139. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67997/0.68129. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67890/0.68036. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67827/0.68224. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67789/0.68216. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67800/0.68166. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67723/0.68162. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67549/0.68054. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67575/0.68110. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67548/0.68086. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67498/0.68190. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67390/0.68052. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67501/0.68131. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67432/0.68280. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67412/0.68203. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67179/0.68208. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67218/0.68278. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67275/0.68185. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67336/0.68334. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67210/0.68393. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66966/0.68077. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67117/0.68357. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67040/0.68245. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66843/0.68234. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.67035/0.68369. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67079/0.68244. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66840/0.68383. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66785/0.68188. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66823/0.68147. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66514/0.68193. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66379/0.68155. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66729/0.68294. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66346/0.68277. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66464/0.68191. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66635/0.68333. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66102/0.68216. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66132/0.68402. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66350/0.68374. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65963/0.68331. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66200/0.68308. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65985/0.68297. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65596/0.68275. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65845/0.68549. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65611/0.68436. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65521/0.68287. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65638/0.68312. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65426/0.68453. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65413/0.68609. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69342/0.69076. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69307/0.69042. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69298/0.69014. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69222/0.68994. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69267/0.68967. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69264/0.68942. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69119/0.68911. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69168/0.68887. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69204/0.68866. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69076/0.68833. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69080/0.68804. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69013/0.68787. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69016/0.68760. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69008/0.68718. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69022/0.68687. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68906/0.68669. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68827/0.68654. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68829/0.68634. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68732/0.68620. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68749/0.68597. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68651/0.68570. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68619/0.68545. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68640/0.68538. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68615/0.68525. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68541/0.68515. Took 0.22 sec\n",
      "Epoch 25, Loss(train/val) 0.68575/0.68484. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68514/0.68463. Took 0.22 sec\n",
      "Epoch 27, Loss(train/val) 0.68401/0.68456. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68274/0.68458. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68226/0.68442. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68170/0.68418. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68049/0.68444. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67923/0.68427. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67958/0.68398. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67841/0.68390. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67726/0.68375. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67539/0.68366. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67445/0.68344. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67421/0.68313. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67251/0.68316. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67005/0.68377. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66870/0.68347. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66963/0.68393. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66560/0.68317. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66548/0.68271. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66479/0.68366. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66233/0.68228. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66129/0.68230. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65998/0.68173. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.65667/0.68329. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.65766/0.68360. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65476/0.68218. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65472/0.68137. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.65067/0.68329. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64985/0.68185. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64808/0.68602. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64627/0.68629. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64327/0.68634. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64397/0.68873. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64298/0.68946. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.64083/0.69116. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.64122/0.68974. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.63942/0.68960. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64099/0.69108. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63593/0.69351. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.63764/0.69200. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63688/0.69691. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.63444/0.69793. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63103/0.69851. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.62961/0.69795. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.62889/0.70075. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62748/0.70130. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62839/0.70242. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.62584/0.70304. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62271/0.70575. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62394/0.70330. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62235/0.70646. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.61930/0.70748. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62230/0.70904. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61752/0.70999. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62255/0.71173. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.61909/0.71525. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61719/0.71483. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.61441/0.71697. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61385/0.71796. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61214/0.71808. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61243/0.71896. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61344/0.72180. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61197/0.72088. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61030/0.72686. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61000/0.72215. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.61073/0.72392. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60864/0.72609. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60274/0.72759. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60844/0.72643. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.60249/0.72891. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60293/0.73468. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60446/0.73217. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60487/0.73386. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60212/0.73733. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69487/0.69337. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69405/0.69263. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69257/0.69199. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69185/0.69156. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69183/0.69113. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69151/0.69069. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69106/0.69032. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69088/0.69004. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69030/0.68978. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69027/0.68962. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68997/0.68947. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68942/0.68939. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68907/0.68929. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68845/0.68927. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68854/0.68937. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68812/0.68960. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68715/0.68990. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68650/0.69030. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68681/0.69075. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68658/0.69143. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68570/0.69198. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68597/0.69245. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68476/0.69308. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68347/0.69394. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68361/0.69480. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68364/0.69568. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68320/0.69667. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68275/0.69753. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68170/0.69833. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68162/0.69940. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68144/0.70053. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68094/0.70151. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68094/0.70236. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67944/0.70345. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67897/0.70487. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67893/0.70632. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67840/0.70773. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67791/0.70890. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67692/0.71015. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67638/0.71112. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67775/0.71171. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67606/0.71272. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67591/0.71337. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67660/0.71428. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67432/0.71539. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67336/0.71668. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67246/0.71791. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67362/0.71859. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67179/0.71956. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67221/0.72041. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67105/0.72113. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67148/0.72195. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67129/0.72269. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67005/0.72349. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67128/0.72387. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66958/0.72421. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66646/0.72633. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66833/0.72709. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66596/0.72770. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66478/0.72914. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66559/0.72974. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66556/0.72987. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66351/0.73078. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66219/0.73098. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66308/0.73201. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66174/0.73349. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66372/0.73381. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66128/0.73411. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66060/0.73441. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65939/0.73418. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65712/0.73541. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65633/0.73623. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65678/0.73746. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65462/0.73823. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65228/0.73933. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65311/0.73922. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65100/0.74134. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65059/0.74074. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64961/0.74180. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65163/0.74151. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65038/0.74222. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65004/0.74128. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64689/0.74105. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64221/0.74270. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64368/0.74394. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64275/0.74397. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64362/0.74357. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64151/0.74475. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64151/0.74410. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63815/0.74593. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63911/0.74566. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63521/0.74782. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63839/0.75005. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63142/0.74913. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63066/0.75192. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63048/0.75182. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62996/0.75229. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62818/0.75146. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63346/0.75329. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62792/0.75571. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69450/0.69342. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69210/0.69219. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69153/0.69131. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69056/0.69098. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69070/0.69073. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69034/0.69074. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68989/0.69076. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68939/0.69086. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68839/0.69128. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68840/0.69153. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68851/0.69175. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68811/0.69217. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68769/0.69269. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68713/0.69310. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68715/0.69356. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68734/0.69411. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68665/0.69469. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68581/0.69505. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68595/0.69550. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68538/0.69598. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68553/0.69626. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68458/0.69646. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68346/0.69709. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68454/0.69749. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68489/0.69804. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68439/0.69835. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68375/0.69898. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68280/0.69942. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68334/0.69967. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68323/0.70013. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68120/0.70017. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68235/0.70061. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68106/0.70086. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68056/0.70118. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68009/0.70208. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68030/0.70213. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67851/0.70239. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67830/0.70252. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67758/0.70296. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67831/0.70330. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67631/0.70354. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67658/0.70357. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67692/0.70433. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67500/0.70494. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67605/0.70490. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67529/0.70558. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67580/0.70625. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67459/0.70649. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67444/0.70617. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67383/0.70604. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67310/0.70556. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67275/0.70556. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67002/0.70604. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67147/0.70698. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66990/0.70678. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66786/0.70724. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66935/0.70733. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66930/0.70868. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66726/0.70860. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66763/0.70886. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66704/0.70841. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66670/0.70808. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66674/0.70908. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66544/0.70878. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66515/0.70946. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66345/0.70963. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66583/0.70988. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66506/0.71059. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66272/0.70989. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66192/0.71106. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66211/0.71066. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66383/0.71096. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66370/0.71089. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65880/0.71174. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66180/0.71222. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65930/0.71240. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65765/0.71308. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65956/0.71246. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65734/0.71342. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65816/0.71386. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65799/0.71256. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65797/0.71355. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65773/0.71382. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65741/0.71463. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65475/0.71485. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65554/0.71403. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65280/0.71438. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65452/0.71386. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65224/0.71401. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65234/0.71527. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65223/0.71773. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65151/0.71722. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65083/0.71687. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65227/0.71735. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65142/0.71700. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65062/0.71773. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65029/0.71850. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64841/0.72003. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64708/0.72117. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64328/0.72254. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69607/0.69659. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69349/0.69584. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69230/0.69559. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69171/0.69543. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69102/0.69539. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69066/0.69550. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69070/0.69548. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68993/0.69568. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68994/0.69592. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68956/0.69599. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68929/0.69645. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68894/0.69671. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68849/0.69695. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68841/0.69729. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68826/0.69759. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68786/0.69763. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68758/0.69791. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68714/0.69822. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68728/0.69846. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68697/0.69884. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68637/0.69923. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68612/0.69947. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68568/0.69970. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68541/0.69982. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68491/0.69986. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68521/0.69998. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68490/0.70007. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68374/0.70035. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68388/0.70050. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68300/0.70059. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68402/0.70045. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68271/0.70065. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68225/0.70124. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68201/0.70151. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68166/0.70165. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68008/0.70205. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68088/0.70220. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67928/0.70263. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67930/0.70297. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67983/0.70297. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67714/0.70342. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.67862/0.70376. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67767/0.70432. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67720/0.70464. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67763/0.70511. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67593/0.70590. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67507/0.70627. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67562/0.70659. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67428/0.70706. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67444/0.70780. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67527/0.70801. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67501/0.70871. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67445/0.70878. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67363/0.70920. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67172/0.70929. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67209/0.70980. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67209/0.71041. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67077/0.71132. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66973/0.71232. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67047/0.71263. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66810/0.71316. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66763/0.71446. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66726/0.71563. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66727/0.71621. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66660/0.71681. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66609/0.71682. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66357/0.71733. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66413/0.71885. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66335/0.71941. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66011/0.71915. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 0.66201/0.71999. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66121/0.72051. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65938/0.72075. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65479/0.72183. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65747/0.72312. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65541/0.72378. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65530/0.72360. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65447/0.72447. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65279/0.72400. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65111/0.72592. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64989/0.72635. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64796/0.72422. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 0.64743/0.72526. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64602/0.72749. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64651/0.72721. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64575/0.72764. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64667/0.72677. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64331/0.72877. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64607/0.72870. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64132/0.72742. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64196/0.72802. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64091/0.72985. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.63829/0.72888. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63701/0.73086. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.64049/0.72988. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63855/0.73202. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64198/0.73118. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63618/0.73052. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 0.63134/0.73133. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63801/0.72989. Took 0.19 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69220/0.69435. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69155/0.69477. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69136/0.69519. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69168/0.69534. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69047/0.69570. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69060/0.69595. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69057/0.69612. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69040/0.69638. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68984/0.69661. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68935/0.69683. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68970/0.69722. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68853/0.69727. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68900/0.69757. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68856/0.69784. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68796/0.69811. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68808/0.69816. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68820/0.69831. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68859/0.69837. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68800/0.69863. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68688/0.69903. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68681/0.69940. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68592/0.69933. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68683/0.69978. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68552/0.70011. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68606/0.70017. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68537/0.70013. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68384/0.70041. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68584/0.70098. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68453/0.70122. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68405/0.70161. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68395/0.70182. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68287/0.70211. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68255/0.70252. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68257/0.70280. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68260/0.70344. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68102/0.70382. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68077/0.70407. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68074/0.70411. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68046/0.70455. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68184/0.70498. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67945/0.70532. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67957/0.70525. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67889/0.70546. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67995/0.70609. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67921/0.70587. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67898/0.70567. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68051/0.70632. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67795/0.70646. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67919/0.70714. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67640/0.70686. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67751/0.70765. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67745/0.70818. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67583/0.70767. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67581/0.70812. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67673/0.70757. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67458/0.70886. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67536/0.70818. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67332/0.70903. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67242/0.70911. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67316/0.70953. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67159/0.70975. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67333/0.71074. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67103/0.71106. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67293/0.70992. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67253/0.71173. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66901/0.71162. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66915/0.71174. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67237/0.71186. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66676/0.71390. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66864/0.71469. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66980/0.71474. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66641/0.71579. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66680/0.71662. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.66633/0.71744. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66645/0.71878. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66313/0.71915. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66564/0.72036. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66584/0.72098. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66114/0.72306. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66448/0.72194. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66143/0.72356. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66259/0.72171. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66176/0.72218. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65863/0.72315. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65923/0.72594. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66114/0.72511. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65926/0.72475. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66195/0.72558. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65572/0.72734. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65846/0.72872. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65758/0.72798. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65745/0.72998. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65414/0.73275. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65308/0.73285. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65175/0.73365. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65193/0.73447. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65418/0.73561. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65362/0.73605. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65212/0.73613. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64910/0.73921. Took 0.18 sec\n",
      "ACC: 0.6354166666666666\n",
      "Epoch 0, Loss(train/val) 0.69461/0.67807. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69243/0.68150. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69211/0.68370. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69108/0.68441. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69211/0.68539. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69012/0.68594. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69033/0.68748. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69010/0.68808. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69002/0.68872. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68870/0.69001. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68880/0.69075. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68896/0.69095. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68841/0.69145. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68817/0.69227. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68748/0.69312. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68784/0.69287. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68730/0.69338. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68667/0.69365. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68596/0.69494. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68603/0.69532. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68670/0.69567. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68504/0.69614. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68485/0.69577. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68543/0.69620. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68402/0.69620. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68521/0.69663. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68425/0.69614. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68347/0.69544. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68293/0.69740. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68246/0.69705. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68200/0.69613. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68211/0.69794. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68088/0.69698. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68274/0.69756. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68054/0.69621. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67989/0.69769. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67823/0.69756. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67924/0.69666. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67957/0.69611. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67911/0.69716. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67738/0.69637. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67827/0.69666. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67804/0.69763. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67687/0.69593. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67470/0.69833. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67554/0.69790. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67495/0.69797. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67527/0.69909. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67416/0.69916. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67208/0.69989. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67545/0.69918. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67280/0.70051. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67244/0.70039. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67123/0.69997. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67223/0.69977. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67053/0.69961. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67055/0.70188. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67020/0.70170. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67210/0.70115. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66958/0.70147. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66952/0.70286. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66952/0.70238. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66936/0.70276. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66752/0.70336. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66699/0.70400. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66836/0.70113. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66728/0.70429. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66510/0.70311. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66475/0.70384. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66392/0.70535. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66564/0.70438. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66320/0.70492. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66238/0.70636. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66389/0.70757. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66198/0.70685. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66188/0.70848. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65980/0.70502. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66155/0.70690. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66093/0.70908. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65930/0.70913. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65640/0.70901. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65782/0.70905. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65588/0.71113. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65430/0.70607. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65339/0.71196. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65476/0.70977. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65161/0.70922. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65247/0.70895. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65279/0.71085. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65153/0.70941. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64879/0.71116. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65042/0.71249. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64753/0.71371. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64746/0.71657. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64458/0.71299. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64538/0.71181. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64203/0.71356. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64393/0.71412. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64393/0.71940. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64340/0.71457. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69407/0.69491. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69344/0.69431. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69224/0.69401. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69192/0.69360. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69155/0.69327. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69099/0.69285. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69061/0.69239. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69046/0.69199. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68965/0.69155. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68952/0.69119. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68917/0.69070. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68857/0.69020. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68848/0.68980. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68836/0.68918. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68829/0.68866. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68751/0.68810. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68788/0.68762. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68740/0.68701. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68758/0.68647. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68621/0.68596. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68664/0.68525. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68607/0.68453. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68648/0.68400. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68604/0.68354. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68595/0.68279. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68572/0.68215. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68493/0.68139. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68525/0.68054. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68416/0.68010. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68394/0.67951. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68421/0.67907. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 0.68376/0.67838. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68317/0.67757. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68266/0.67698. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68162/0.67641. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.68171/0.67622. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68167/0.67585. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68177/0.67540. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67983/0.67549. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67994/0.67532. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67883/0.67499. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.67951/0.67443. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67789/0.67423. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67864/0.67374. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67723/0.67337. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67822/0.67357. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67591/0.67354. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67626/0.67274. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67614/0.67284. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67372/0.67349. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67362/0.67359. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67215/0.67287. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67129/0.67359. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67220/0.67397. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67216/0.67428. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66996/0.67408. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 0.66950/0.67410. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66786/0.67476. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66856/0.67493. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66693/0.67477. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66562/0.67529. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66585/0.67546. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66475/0.67542. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66238/0.67669. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66207/0.67651. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66391/0.67676. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66144/0.67730. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66320/0.67677. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65946/0.67815. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65724/0.67824. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65916/0.67853. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65688/0.67779. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65388/0.67688. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65426/0.67802. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65247/0.67926. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65045/0.67882. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65426/0.67730. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65284/0.67642. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65182/0.67643. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64723/0.67722. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64575/0.67845. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64966/0.67686. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.64979/0.67619. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64397/0.67635. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64448/0.67691. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64424/0.67823. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 0.64440/0.67608. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64058/0.67791. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64081/0.67742. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63956/0.67689. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63570/0.67740. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63464/0.67638. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.63540/0.67496. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63545/0.67614. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.63103/0.67785. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63688/0.67656. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63060/0.67843. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63143/0.67890. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 0.63286/0.67663. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62388/0.67995. Took 0.20 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69494/0.69340. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69407/0.69113. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69281/0.69053. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69205/0.69010. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69235/0.69008. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69179/0.68992. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69146/0.68954. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68995/0.68931. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68980/0.68934. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68908/0.68957. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68932/0.68969. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68935/0.68992. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68804/0.69006. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68699/0.69036. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68653/0.69081. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68536/0.69119. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68480/0.69185. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68331/0.69242. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68498/0.69338. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68375/0.69407. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68292/0.69457. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68270/0.69511. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68074/0.69557. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68188/0.69609. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68247/0.69696. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68202/0.69748. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68037/0.69791. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68060/0.69833. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67894/0.69900. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67985/0.69955. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68154/0.69985. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67936/0.69986. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67791/0.70035. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67723/0.70079. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67820/0.70157. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67736/0.70212. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67539/0.70257. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67552/0.70229. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67645/0.70264. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67509/0.70335. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67503/0.70365. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67295/0.70353. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67052/0.70383. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67107/0.70437. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67255/0.70412. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67281/0.70460. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67027/0.70502. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67208/0.70593. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66946/0.70609. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66901/0.70615. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67113/0.70655. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66876/0.70654. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66780/0.70675. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66866/0.70695. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66560/0.70829. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66436/0.70824. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66589/0.70863. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66827/0.70852. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66203/0.70948. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66237/0.71045. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66141/0.71028. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66144/0.71074. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66279/0.71087. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66237/0.71075. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66166/0.71105. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66226/0.71097. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65900/0.71138. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65977/0.71204. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65807/0.71266. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65715/0.71263. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65792/0.71221. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65571/0.71338. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65592/0.71358. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65351/0.71338. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65170/0.71401. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65337/0.71447. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65063/0.71529. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64882/0.71643. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64698/0.71708. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64719/0.71686. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64888/0.71671. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64644/0.71703. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64262/0.71746. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64608/0.71935. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64559/0.71928. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64297/0.71889. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64234/0.71901. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64196/0.71931. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63557/0.72093. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63452/0.72118. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63909/0.72069. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63691/0.72273. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63250/0.72191. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63821/0.72273. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63190/0.72508. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63442/0.72266. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63290/0.72076. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63285/0.72058. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63223/0.72356. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62895/0.72423. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69455/0.69009. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.69400/0.68824. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69347/0.68800. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69315/0.68787. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69261/0.68757. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69225/0.68736. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69138/0.68692. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69211/0.68671. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69161/0.68680. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69113/0.68636. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69142/0.68636. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69099/0.68632. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69017/0.68599. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69112/0.68600. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69093/0.68614. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68923/0.68601. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69047/0.68630. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68984/0.68568. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68954/0.68566. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68793/0.68561. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68811/0.68527. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68818/0.68529. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68888/0.68510. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68745/0.68525. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68697/0.68530. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68719/0.68460. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68757/0.68500. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68678/0.68450. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68618/0.68470. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68523/0.68405. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68454/0.68332. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68337/0.68280. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68287/0.68205. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68214/0.68184. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68119/0.68187. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68100/0.68060. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67955/0.68063. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67976/0.67977. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67980/0.67837. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67862/0.67847. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67697/0.67755. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67651/0.67576. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67505/0.67538. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67486/0.67551. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67136/0.67366. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67117/0.67302. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.67213/0.67256. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66981/0.66936. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66574/0.67009. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66478/0.66742. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.66424/0.66498. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66064/0.66414. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.65942/0.66434. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.65862/0.66439. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65772/0.66202. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65790/0.66068. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 0.65418/0.66218. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65177/0.65973. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.65387/0.65849. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64961/0.66072. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.64520/0.65742. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.64562/0.65970. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64564/0.65836. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.64355/0.65699. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.64023/0.65506. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64142/0.65666. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64011/0.65274. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63902/0.65647. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.63533/0.65298. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.63125/0.65353. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63727/0.65385. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.63281/0.65407. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63034/0.65532. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62809/0.65523. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62828/0.65395. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63006/0.65575. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62598/0.65335. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62072/0.64970. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.62447/0.65197. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.61951/0.65253. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.62110/0.65336. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61503/0.65590. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61342/0.65403. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61397/0.65454. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.61198/0.65170. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61471/0.65212. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61389/0.65531. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61265/0.65336. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61147/0.65539. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.60988/0.65590. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60971/0.65104. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.60651/0.65477. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.60438/0.65318. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.59928/0.65599. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.59640/0.65764. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60048/0.65765. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59340/0.66043. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60299/0.65699. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.59417/0.65911. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59320/0.65853. Took 0.20 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69420/0.69529. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69268/0.69466. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69214/0.69448. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69199/0.69448. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69169/0.69440. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69119/0.69434. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69055/0.69434. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69034/0.69436. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69086/0.69449. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69002/0.69458. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69074/0.69474. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68881/0.69495. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68871/0.69505. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68801/0.69538. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68806/0.69554. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68777/0.69594. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68678/0.69643. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68522/0.69700. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68601/0.69756. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68441/0.69809. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68481/0.69873. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68418/0.69949. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68348/0.70018. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68181/0.70104. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68114/0.70195. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68048/0.70253. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67822/0.70345. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68020/0.70427. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67695/0.70562. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67582/0.70649. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67600/0.70799. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67500/0.70826. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67495/0.70947. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67106/0.71098. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67101/0.71141. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67094/0.71179. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67068/0.71259. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66958/0.71301. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66624/0.71470. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66600/0.71575. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66295/0.71729. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66379/0.71559. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.65983/0.71763. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66110/0.71833. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66154/0.71914. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65660/0.71800. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65535/0.71936. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65746/0.71861. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65641/0.71773. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65310/0.72260. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.64704/0.71955. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.64953/0.72152. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.64838/0.72153. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.64744/0.72078. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64468/0.72271. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.64555/0.72468. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64429/0.72793. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64045/0.72683. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.63939/0.72525. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.63647/0.72851. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.63779/0.72684. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.63626/0.72737. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63342/0.72696. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63517/0.72762. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63048/0.73013. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63177/0.72823. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63065/0.72972. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.62601/0.73411. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.62467/0.73372. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.62179/0.73403. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.62421/0.73454. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62344/0.73249. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62483/0.73388. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.61904/0.73853. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62105/0.73550. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.61793/0.73795. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.61559/0.73638. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.61361/0.73916. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.61301/0.73774. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61121/0.73735. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.60890/0.73736. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.60691/0.73894. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.61146/0.73940. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.60587/0.73968. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.60940/0.74206. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.60758/0.74094. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 0.60013/0.73959. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.60312/0.73872. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.60203/0.73944. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60010/0.74113. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.60064/0.74362. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.59811/0.73989. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.59494/0.74269. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.59355/0.74667. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.59530/0.74736. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.59083/0.74508. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.58695/0.74463. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.59178/0.74987. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 0.58433/0.74803. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.58826/0.74922. Took 0.20 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69463/0.68900. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69149/0.68707. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69152/0.68666. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69045/0.68651. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69086/0.68645. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69042/0.68640. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.69042/0.68622. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68975/0.68614. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68961/0.68618. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68894/0.68619. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69015/0.68604. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68886/0.68615. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68805/0.68600. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68880/0.68591. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68806/0.68585. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68789/0.68582. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68713/0.68598. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68709/0.68604. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68764/0.68629. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68598/0.68626. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68635/0.68649. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68654/0.68661. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68545/0.68666. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68684/0.68665. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68501/0.68682. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68584/0.68699. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68345/0.68683. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68410/0.68689. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68361/0.68704. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68292/0.68691. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68212/0.68701. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68291/0.68719. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68223/0.68751. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68109/0.68754. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68228/0.68791. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67996/0.68833. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68103/0.68842. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68083/0.68903. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68004/0.68911. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67940/0.68906. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67847/0.68920. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67831/0.68950. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67623/0.69021. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67592/0.69084. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67605/0.69143. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67666/0.69242. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67570/0.69243. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67685/0.69280. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67343/0.69397. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67504/0.69406. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67470/0.69471. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67247/0.69559. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67337/0.69648. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67213/0.69703. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67162/0.69779. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67055/0.69867. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67068/0.69928. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67009/0.69988. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66932/0.70076. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67002/0.70202. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66992/0.70206. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66733/0.70264. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66585/0.70428. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66527/0.70433. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66512/0.70516. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66267/0.70675. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66508/0.70777. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66373/0.70955. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66332/0.70966. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66523/0.71017. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66047/0.71080. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65968/0.71237. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66204/0.71222. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66044/0.71336. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65747/0.71456. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65478/0.71630. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65902/0.71662. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65505/0.71704. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65836/0.71814. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65484/0.71909. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65417/0.71982. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65184/0.72124. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65148/0.72205. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65351/0.72246. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65389/0.72303. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64850/0.72397. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65252/0.72381. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64967/0.72565. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65129/0.72706. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64896/0.72655. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64648/0.72682. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64505/0.72873. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64512/0.72717. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64759/0.72875. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64347/0.73070. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64375/0.73141. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64387/0.73317. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64396/0.73383. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64151/0.73313. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64157/0.73504. Took 0.20 sec\n",
      "ACC: 0.6458333333333334\n",
      "Epoch 0, Loss(train/val) 0.69740/0.68423. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68961/0.67263. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68875/0.67039. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68940/0.66967. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68890/0.66942. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68811/0.66897. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68962/0.66921. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68813/0.66883. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68852/0.66869. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68730/0.66768. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68777/0.66753. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68669/0.66756. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68771/0.66763. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68727/0.66764. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68641/0.66734. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68683/0.66757. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68698/0.66764. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68647/0.66735. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68805/0.66716. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68542/0.66652. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68643/0.66632. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68610/0.66640. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68588/0.66750. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68539/0.66733. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68457/0.66757. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68409/0.66739. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68445/0.66670. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68386/0.66686. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68338/0.66662. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68391/0.66636. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68446/0.66616. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68232/0.66652. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68306/0.66694. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68339/0.66707. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68194/0.66751. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68249/0.66720. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68215/0.66786. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68091/0.66772. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68150/0.66818. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68144/0.66872. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68049/0.66873. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67983/0.66818. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68096/0.66832. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67761/0.66975. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67949/0.66948. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67900/0.66930. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67728/0.66953. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67846/0.66989. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67709/0.66902. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67662/0.66972. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67752/0.67013. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67565/0.66993. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67706/0.67084. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67604/0.66999. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67525/0.67008. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67348/0.67106. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67475/0.67106. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67280/0.67217. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67469/0.67217. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67377/0.67395. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67288/0.67228. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67299/0.67140. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66967/0.67166. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67112/0.67204. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66987/0.67287. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66901/0.67276. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66897/0.67395. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67032/0.67175. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66745/0.67298. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66548/0.67417. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66861/0.67264. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66672/0.67440. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66746/0.67367. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66752/0.67446. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66527/0.67454. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66408/0.67843. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66535/0.67621. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66084/0.67603. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66185/0.67726. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66131/0.67827. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66023/0.67868. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66099/0.68042. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66045/0.68063. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65773/0.68201. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66266/0.68104. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65688/0.68429. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65887/0.68444. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65650/0.68512. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65727/0.68445. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65323/0.68533. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65156/0.68699. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65252/0.68862. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65396/0.68937. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65259/0.68730. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65317/0.68908. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64959/0.68664. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65234/0.69143. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65278/0.68793. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65239/0.69519. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64938/0.69014. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.68704/0.69413. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68702/0.69577. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68682/0.69639. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68582/0.69701. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68595/0.69753. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68536/0.69790. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68497/0.69834. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68535/0.69874. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68498/0.69923. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68390/0.69957. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68395/0.70012. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68421/0.70086. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68343/0.70136. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68358/0.70225. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68234/0.70270. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68208/0.70355. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68284/0.70445. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68144/0.70509. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68159/0.70604. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68067/0.70663. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68043/0.70720. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68045/0.70829. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68012/0.70910. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67957/0.70938. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67906/0.71054. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67924/0.71105. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67827/0.71161. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67849/0.71231. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67728/0.71296. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67587/0.71394. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67565/0.71457. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67430/0.71533. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67445/0.71607. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67477/0.71682. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67471/0.71686. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67250/0.71838. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67356/0.71852. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67188/0.71950. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67092/0.72040. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66956/0.72193. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66814/0.72230. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66787/0.72298. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66789/0.72398. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66727/0.72530. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66610/0.72661. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66609/0.72744. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66370/0.72641. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66563/0.72653. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66386/0.72754. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66422/0.72737. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66283/0.72870. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66192/0.73071. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66254/0.73133. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65901/0.73106. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66108/0.73157. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66039/0.73057. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65905/0.73193. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65738/0.73412. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65552/0.73314. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65624/0.73450. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65647/0.73477. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65485/0.73399. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65419/0.73449. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65299/0.73430. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65338/0.73649. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65255/0.73653. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64917/0.73728. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65431/0.73633. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64671/0.73603. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64953/0.73674. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64809/0.74080. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64483/0.74084. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64638/0.74152. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64817/0.74245. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64503/0.74235. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64691/0.74504. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64332/0.74484. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64567/0.74473. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64278/0.74536. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64319/0.74509. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64006/0.74456. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64043/0.74759. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64011/0.74802. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64047/0.75000. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63531/0.75038. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63598/0.75152. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63759/0.75305. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63679/0.75351. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63747/0.75374. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63440/0.75594. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63360/0.75719. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63284/0.75872. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63195/0.76337. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63136/0.76252. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63176/0.76258. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63067/0.76428. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63079/0.76395. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62980/0.76829. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.62861/0.76791. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62196/0.76822. Took 0.19 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.68966/0.67740. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68887/0.67704. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.68902/0.67734. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68852/0.67767. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68780/0.67810. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68733/0.67854. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68735/0.67906. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68751/0.67947. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68733/0.67964. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68647/0.67991. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68619/0.68035. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68624/0.68078. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68607/0.68103. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68555/0.68129. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68538/0.68140. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68470/0.68149. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.68511/0.68179. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68487/0.68197. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68485/0.68227. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68400/0.68237. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68365/0.68217. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68351/0.68216. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68305/0.68207. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68249/0.68190. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68290/0.68210. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68291/0.68204. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68250/0.68158. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68166/0.68200. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68156/0.68177. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68092/0.68219. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68231/0.68180. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68082/0.68138. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68079/0.68144. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67988/0.68113. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68083/0.68060. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67957/0.68058. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67911/0.67985. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67942/0.67925. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68066/0.67921. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67833/0.67990. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67828/0.67965. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67720/0.67985. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67788/0.67914. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67720/0.67944. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67789/0.67912. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67607/0.67895. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67806/0.67874. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67644/0.67876. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67627/0.67831. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67531/0.67806. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67556/0.67790. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67506/0.67826. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67403/0.67823. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67368/0.67834. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67464/0.67845. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67345/0.67827. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67362/0.67782. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67218/0.67801. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67252/0.67802. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67023/0.67764. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67187/0.67809. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67034/0.67808. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66833/0.67851. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66796/0.67828. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66843/0.67793. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66633/0.67855. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66704/0.67761. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66809/0.67653. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66532/0.67751. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66680/0.67828. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66613/0.67711. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66295/0.67710. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66373/0.67789. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66204/0.67901. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66121/0.67915. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66119/0.67885. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66105/0.67927. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65854/0.67928. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66043/0.67892. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65735/0.67993. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65643/0.67958. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65582/0.67929. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65638/0.67989. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65189/0.68030. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65142/0.68008. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65046/0.68108. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65219/0.68073. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64754/0.68323. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64814/0.68358. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64732/0.68395. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64683/0.68283. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.64482/0.68534. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64196/0.68502. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64113/0.68555. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64267/0.68553. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63606/0.68922. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63873/0.68833. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63214/0.69198. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63635/0.69373. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63899/0.69109. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69109/0.68671. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68847/0.68374. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68883/0.68259. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68785/0.68214. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68664/0.68187. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68750/0.68167. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68732/0.68149. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68655/0.68142. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68602/0.68127. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68646/0.68106. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68453/0.68099. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68588/0.68088. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68526/0.68065. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68478/0.68033. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68363/0.68016. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68408/0.68000. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68405/0.67996. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68375/0.67978. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68331/0.67960. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68351/0.67964. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68369/0.67971. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68379/0.67946. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68181/0.67911. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68196/0.67894. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68178/0.67891. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68130/0.67885. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68178/0.67901. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68098/0.67892. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68165/0.67861. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67943/0.67883. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68089/0.67901. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67838/0.67863. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67896/0.67876. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67770/0.67911. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67694/0.67885. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67859/0.67874. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68033/0.67873. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67737/0.67875. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67788/0.67906. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67682/0.67918. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67848/0.67974. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67746/0.67976. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67596/0.67966. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67544/0.67983. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67613/0.67975. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67574/0.67954. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67433/0.67958. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67404/0.67992. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67501/0.67997. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67150/0.68008. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67312/0.67978. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67500/0.67989. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67361/0.67943. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67176/0.67986. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67192/0.67989. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66898/0.67952. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66994/0.67959. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67146/0.67981. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67011/0.68057. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66989/0.68040. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66959/0.68018. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67036/0.68124. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67184/0.68013. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66821/0.68150. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66876/0.67988. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66987/0.67990. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66692/0.68157. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66811/0.68101. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66853/0.68158. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66603/0.68149. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66665/0.68186. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66804/0.68161. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66582/0.68162. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66260/0.68165. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66239/0.68092. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66525/0.68231. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66325/0.68222. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66150/0.68165. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66150/0.68052. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66055/0.68091. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66196/0.68114. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65883/0.68119. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66145/0.68375. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65956/0.68220. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66097/0.68239. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65810/0.68360. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65891/0.68250. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65882/0.68485. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65590/0.68537. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65689/0.68391. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65584/0.68409. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65593/0.68313. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65611/0.68328. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65693/0.68243. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65257/0.68161. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65447/0.68360. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65318/0.68371. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65056/0.68295. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65388/0.68210. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64984/0.68270. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.68986/0.68896. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68653/0.68793. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68644/0.68778. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68554/0.68787. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68601/0.68791. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68531/0.68811. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68511/0.68826. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68437/0.68840. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68496/0.68855. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68376/0.68885. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68378/0.68931. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68357/0.68952. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68305/0.68987. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68285/0.69014. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68278/0.69050. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68216/0.69095. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68203/0.69130. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68128/0.69167. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68128/0.69197. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68069/0.69229. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68059/0.69273. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68045/0.69319. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67948/0.69358. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68036/0.69381. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68058/0.69408. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67908/0.69447. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67906/0.69450. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67854/0.69489. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67840/0.69523. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67730/0.69548. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67767/0.69593. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67801/0.69629. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67714/0.69653. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67719/0.69678. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67725/0.69669. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67611/0.69690. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67653/0.69718. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67532/0.69757. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67474/0.69811. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67469/0.69834. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67421/0.69840. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67385/0.69879. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67540/0.69947. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67429/0.69965. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67429/0.69989. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67312/0.69988. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67228/0.70010. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67233/0.70021. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67216/0.70030. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67131/0.70028. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67137/0.70048. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67163/0.70092. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67119/0.70201. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67066/0.70275. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67238/0.70174. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66940/0.70199. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67124/0.70283. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66765/0.70312. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66996/0.70382. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66767/0.70301. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66627/0.70365. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66854/0.70332. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66708/0.70334. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66488/0.70387. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66656/0.70546. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66616/0.70512. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66577/0.70509. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66455/0.70594. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66143/0.70568. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66307/0.70572. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66356/0.70746. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66123/0.70780. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66309/0.70845. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66251/0.70897. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66199/0.70945. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66137/0.70969. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66101/0.70988. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65954/0.71031. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65838/0.71091. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65836/0.71215. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66280/0.71246. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65850/0.71416. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65578/0.71459. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65691/0.71520. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65650/0.71502. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65611/0.71574. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65380/0.71707. Took 0.23 sec\n",
      "Epoch 87, Loss(train/val) 0.65129/0.71579. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65676/0.71581. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65262/0.71663. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65023/0.71827. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65388/0.71804. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65248/0.71948. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65021/0.72003. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64874/0.72118. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64741/0.72121. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64879/0.72329. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65001/0.72253. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64583/0.72308. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64506/0.72349. Took 0.19 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.68948/0.71490. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68650/0.71531. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68492/0.71319. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68471/0.71322. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68487/0.71302. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68434/0.71308. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68364/0.71266. Took 0.22 sec\n",
      "Epoch 7, Loss(train/val) 0.68387/0.71168. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68233/0.71124. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68327/0.71169. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68334/0.71152. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68267/0.71168. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68243/0.71147. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68206/0.71140. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68175/0.71175. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68208/0.71073. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.68095/0.71197. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68161/0.71210. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68149/0.71104. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68020/0.71146. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.67933/0.71224. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.67963/0.71183. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.67982/0.71108. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67959/0.71116. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.67957/0.71168. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67923/0.71207. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67868/0.71191. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67877/0.71233. Took 0.22 sec\n",
      "Epoch 28, Loss(train/val) 0.67793/0.71328. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.67821/0.71307. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67795/0.71346. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67696/0.71425. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67724/0.71367. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67763/0.71448. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67705/0.71492. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.67573/0.71545. Took 0.22 sec\n",
      "Epoch 36, Loss(train/val) 0.67626/0.71626. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67634/0.71502. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67614/0.71639. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67646/0.71587. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67555/0.71558. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67660/0.71699. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67577/0.71656. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67547/0.71592. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67528/0.71745. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67487/0.71665. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67536/0.71884. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67394/0.71705. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67388/0.71757. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67377/0.71806. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67175/0.71840. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67342/0.71868. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67374/0.71807. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67245/0.72009. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67126/0.71800. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67262/0.71924. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67278/0.71711. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67280/0.71956. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67084/0.72057. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67085/0.72061. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67142/0.72039. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67028/0.71919. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66926/0.72271. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66895/0.72218. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.67121/0.72255. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66759/0.72288. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66817/0.72396. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66707/0.72159. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66884/0.72357. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67191/0.72208. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66645/0.72381. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66832/0.72615. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66796/0.72289. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66856/0.72677. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66513/0.72280. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66556/0.72370. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 0.66790/0.72502. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66777/0.72587. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66675/0.72509. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66312/0.72617. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66390/0.72525. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66494/0.72443. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66410/0.72914. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66208/0.72560. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 0.66367/0.72659. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66311/0.72706. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66019/0.72831. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66377/0.72939. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66288/0.73028. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66431/0.72821. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65846/0.72617. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66120/0.72963. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66074/0.72920. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65698/0.73070. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.66056/0.72947. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.66005/0.72679. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65788/0.72990. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66065/0.73016. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65818/0.73199. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65687/0.73100. Took 0.19 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69908/0.69156. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69194/0.69508. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69020/0.69715. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69029/0.69808. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68963/0.69868. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68941/0.69926. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68873/0.69994. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68888/0.70074. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68873/0.70127. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68860/0.70145. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68803/0.70183. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68767/0.70208. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68800/0.70254. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68786/0.70291. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68748/0.70325. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68716/0.70388. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68705/0.70407. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68783/0.70407. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68729/0.70427. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68590/0.70475. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68599/0.70499. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68542/0.70564. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68582/0.70603. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68552/0.70658. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68522/0.70694. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68520/0.70733. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68580/0.70720. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68464/0.70781. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68462/0.70828. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68396/0.70854. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68368/0.70836. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68472/0.70842. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68447/0.70837. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68379/0.70892. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68238/0.70929. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68229/0.70994. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68332/0.70938. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68311/0.70993. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68246/0.71016. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68120/0.70966. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68066/0.71107. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68163/0.70959. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68057/0.71083. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68014/0.70998. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68080/0.71001. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68055/0.71016. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68005/0.71011. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67806/0.71061. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67792/0.71062. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67749/0.71190. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67828/0.71103. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67729/0.71130. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67695/0.71107. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67643/0.71287. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67617/0.71188. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67672/0.71357. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67522/0.71335. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67588/0.71480. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67609/0.71425. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67361/0.71412. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67349/0.71427. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67357/0.71324. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67187/0.71362. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67313/0.71305. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67229/0.71438. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67199/0.71449. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67121/0.71435. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67009/0.71565. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67048/0.71516. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66906/0.71664. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66821/0.71522. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66765/0.71636. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66571/0.71688. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66547/0.71624. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66598/0.71815. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66849/0.71671. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66259/0.71864. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66436/0.71849. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66186/0.71810. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66279/0.71732. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66125/0.71920. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65958/0.71952. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65816/0.72099. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65718/0.71933. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65881/0.71961. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66013/0.72056. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65487/0.72226. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65551/0.72249. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65559/0.72189. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65680/0.72574. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65211/0.72699. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65231/0.72631. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65171/0.72793. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65023/0.72715. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65115/0.72631. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65037/0.72680. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64961/0.72829. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64823/0.72848. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64758/0.72825. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64684/0.73194. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69468/0.69449. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69300/0.69419. Took 0.22 sec\n",
      "Epoch 2, Loss(train/val) 0.69213/0.69409. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69115/0.69407. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69085/0.69412. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68948/0.69428. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68816/0.69457. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68824/0.69482. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68721/0.69505. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68621/0.69540. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68654/0.69563. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68487/0.69618. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68437/0.69642. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68518/0.69668. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68371/0.69700. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68404/0.69720. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68350/0.69740. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68294/0.69743. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68371/0.69725. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68128/0.69741. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68208/0.69735. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68182/0.69723. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68194/0.69745. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68002/0.69757. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68110/0.69769. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68085/0.69770. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68021/0.69738. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68037/0.69704. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67943/0.69678. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68017/0.69658. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67929/0.69642. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67899/0.69662. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67900/0.69665. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67618/0.69681. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67738/0.69668. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67606/0.69670. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67712/0.69681. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67627/0.69662. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67635/0.69633. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67493/0.69635. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67531/0.69603. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67283/0.69599. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67390/0.69608. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67406/0.69607. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67280/0.69597. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67279/0.69529. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67108/0.69534. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67180/0.69488. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67103/0.69484. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66995/0.69493. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67014/0.69473. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66933/0.69461. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66610/0.69471. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66636/0.69452. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66612/0.69378. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66651/0.69314. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66422/0.69290. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66389/0.69360. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66430/0.69477. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66243/0.69397. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66393/0.69376. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66147/0.69260. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66311/0.69361. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66122/0.69322. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66053/0.69346. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65738/0.69419. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65806/0.69719. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65581/0.69760. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65439/0.69536. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65567/0.69550. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65353/0.69646. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65549/0.69446. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65371/0.69565. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65232/0.69661. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65071/0.69624. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64804/0.69469. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.64758/0.69536. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65048/0.69637. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64633/0.69747. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64447/0.69751. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64564/0.69829. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64744/0.69752. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64495/0.69928. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64160/0.69793. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64277/0.69792. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64076/0.69918. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 0.63710/0.70094. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.63785/0.70259. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63855/0.70422. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63982/0.70391. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63491/0.70375. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63361/0.70358. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63266/0.70365. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63291/0.70279. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63285/0.70484. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62858/0.70760. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62924/0.70829. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62575/0.70691. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62800/0.71575. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62390/0.71365. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69461/0.69336. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69214/0.69368. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69155/0.69419. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69102/0.69459. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69046/0.69502. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69010/0.69548. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68927/0.69606. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68908/0.69672. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68824/0.69746. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68853/0.69794. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68802/0.69853. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68728/0.69926. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68668/0.69981. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68726/0.70036. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68620/0.70089. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68520/0.70155. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68546/0.70238. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68477/0.70294. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68378/0.70373. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68370/0.70410. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68412/0.70457. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68184/0.70536. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68253/0.70620. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68197/0.70721. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67941/0.70770. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68061/0.70764. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67924/0.70800. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67882/0.70891. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67802/0.70982. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67806/0.70996. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67762/0.71114. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67681/0.71122. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67612/0.71142. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67564/0.71182. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67411/0.71210. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67326/0.71244. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67442/0.71238. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.67284/0.71262. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67328/0.71280. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67299/0.71258. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67080/0.71363. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67020/0.71332. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66761/0.71392. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66881/0.71468. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66849/0.71479. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66762/0.71515. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66910/0.71563. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66767/0.71594. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66463/0.71512. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66552/0.71493. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66722/0.71568. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66404/0.71576. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66026/0.71576. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66198/0.71538. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66290/0.71692. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66393/0.71628. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65771/0.71670. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66145/0.71716. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65888/0.71712. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65882/0.71681. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65883/0.71749. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65734/0.71662. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65609/0.71696. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65575/0.71698. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65586/0.71746. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65405/0.71667. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65126/0.71857. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65421/0.71896. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65455/0.71889. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65073/0.71834. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65182/0.71912. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64960/0.72028. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64804/0.72098. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64724/0.72083. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64847/0.71973. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64264/0.72226. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64484/0.72322. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64328/0.72391. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63897/0.72421. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63757/0.72340. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64327/0.72452. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63940/0.72574. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63794/0.72737. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63663/0.72738. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63621/0.72659. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63716/0.72812. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63235/0.72896. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63198/0.72929. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63259/0.72843. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63097/0.73152. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62946/0.72956. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62494/0.73075. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62838/0.73227. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62759/0.73307. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62426/0.73436. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62982/0.73340. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62474/0.73287. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61900/0.73506. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61828/0.73790. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.61791/0.74070. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69784/0.68955. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69383/0.69049. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69266/0.69144. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69205/0.69209. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69097/0.69260. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69018/0.69296. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68997/0.69321. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68973/0.69341. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68921/0.69351. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68886/0.69354. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68746/0.69357. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68841/0.69351. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68740/0.69333. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68746/0.69321. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68714/0.69306. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68619/0.69313. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68607/0.69310. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68498/0.69312. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68396/0.69305. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68358/0.69253. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68385/0.69249. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68358/0.69207. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68289/0.69213. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68141/0.69228. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68153/0.69232. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68218/0.69248. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68057/0.69264. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68050/0.69227. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68043/0.69225. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67954/0.69266. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67982/0.69304. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67965/0.69334. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67859/0.69366. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67862/0.69412. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67615/0.69503. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67902/0.69538. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67706/0.69577. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67715/0.69604. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67587/0.69628. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67515/0.69716. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67462/0.69725. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67461/0.69825. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67399/0.69868. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67330/0.69894. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67367/0.69920. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67271/0.69960. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67293/0.70005. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67241/0.70017. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67258/0.70080. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67173/0.70122. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67081/0.70211. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67050/0.70221. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67076/0.70339. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66883/0.70372. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66981/0.70395. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66971/0.70469. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66773/0.70555. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66640/0.70646. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66629/0.70698. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66625/0.70711. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66746/0.70749. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66458/0.70802. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66638/0.70761. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66283/0.70863. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66488/0.70863. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66499/0.70889. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66424/0.71004. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66525/0.71077. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66224/0.71142. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66024/0.71334. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66308/0.71394. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66026/0.71465. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65950/0.71450. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66023/0.71440. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65743/0.71491. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65867/0.71557. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65594/0.71626. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65713/0.71670. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65579/0.71793. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65597/0.71959. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65417/0.72040. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65378/0.72190. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65739/0.72263. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65451/0.72267. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65291/0.72216. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65076/0.72221. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65199/0.72377. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65298/0.72499. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64975/0.72668. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65328/0.72638. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64780/0.72808. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65279/0.72779. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65070/0.72917. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64818/0.72920. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64689/0.73034. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64861/0.73196. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64535/0.73398. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64211/0.73546. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64549/0.73716. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64213/0.73628. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69331/0.68813. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69226/0.68813. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69194/0.68871. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69193/0.68852. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69127/0.68892. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69165/0.68930. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69094/0.68947. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69051/0.68967. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68981/0.68986. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68923/0.68999. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68882/0.69031. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68771/0.69045. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68813/0.69091. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68739/0.69114. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68767/0.69131. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68611/0.69163. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68622/0.69121. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68544/0.69148. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68533/0.69090. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68498/0.69152. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68463/0.69163. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68314/0.69095. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68294/0.69143. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68189/0.68999. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68174/0.69016. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68188/0.68910. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68100/0.68882. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68083/0.68975. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67904/0.68969. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68026/0.69025. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67928/0.68973. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67983/0.68919. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67841/0.68936. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67828/0.68915. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67834/0.68957. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67859/0.68887. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67751/0.68992. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.67651/0.68874. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67668/0.69013. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67637/0.69004. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67503/0.68962. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67493/0.69104. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67532/0.69053. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67592/0.68981. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67535/0.69015. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67587/0.69049. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67358/0.69105. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67360/0.69165. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67396/0.69055. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67071/0.69153. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67295/0.69060. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66968/0.69140. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67038/0.69172. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67050/0.69026. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67003/0.69288. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66940/0.69201. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66926/0.69278. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66848/0.69326. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66967/0.69291. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66918/0.69289. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66941/0.69385. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66801/0.69361. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66544/0.69315. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66637/0.69315. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66527/0.69448. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66346/0.69658. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66435/0.69604. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66489/0.69399. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66401/0.69724. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66284/0.69747. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66111/0.69748. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66136/0.69587. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66089/0.69638. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65950/0.69746. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65818/0.69956. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65820/0.69886. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65717/0.69822. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65638/0.70131. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65579/0.69971. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65487/0.69831. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65457/0.70176. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65350/0.70198. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65166/0.70088. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65328/0.70350. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65336/0.70076. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65004/0.70271. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65002/0.70437. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64777/0.70327. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64691/0.70498. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64832/0.70787. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64205/0.70709. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64367/0.70617. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64093/0.71067. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64199/0.71198. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64437/0.71237. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64126/0.70839. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63861/0.70964. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63427/0.71143. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63771/0.71443. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63545/0.71634. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69374/0.69685. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69260/0.69683. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69092/0.69689. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69105/0.69726. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69055/0.69767. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68994/0.69808. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68890/0.69869. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.68967/0.69907. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68838/0.69987. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68815/0.70052. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68760/0.70110. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68657/0.70193. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68637/0.70267. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68534/0.70359. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68531/0.70423. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68576/0.70469. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68461/0.70519. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68370/0.70557. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68372/0.70619. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68379/0.70631. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68108/0.70739. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68283/0.70764. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68151/0.70818. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68051/0.70851. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68152/0.70941. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67930/0.70980. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67958/0.71060. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68085/0.71046. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.67863/0.71077. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67847/0.71131. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67975/0.71197. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67854/0.71245. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67784/0.71276. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67687/0.71309. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 0.67829/0.71373. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67612/0.71460. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67510/0.71512. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67623/0.71583. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67636/0.71595. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67417/0.71629. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67330/0.71650. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67542/0.71703. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67453/0.71758. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67286/0.71796. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 0.67376/0.71829. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67368/0.71894. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67243/0.71998. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67282/0.71915. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67152/0.71997. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67089/0.72035. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67079/0.72003. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66951/0.72037. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.67032/0.72050. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.67249/0.71979. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66776/0.71967. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66904/0.72027. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66916/0.72092. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66928/0.72221. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66684/0.72246. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66880/0.72233. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66652/0.72283. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66670/0.72201. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66815/0.72288. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66337/0.72376. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66536/0.72386. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66511/0.72381. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66364/0.72309. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66570/0.72300. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66313/0.72360. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66240/0.72523. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66140/0.72436. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65922/0.72405. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66052/0.72394. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.65880/0.72445. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65787/0.72529. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66027/0.72587. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65907/0.72476. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65525/0.72559. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65530/0.72654. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65542/0.72872. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65374/0.72874. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65378/0.72784. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65648/0.72828. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65402/0.72713. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65284/0.72594. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65161/0.72663. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65506/0.72607. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65041/0.72811. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64844/0.72794. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64703/0.72872. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65084/0.72736. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64563/0.72615. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64968/0.72585. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64459/0.72544. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64547/0.72664. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64450/0.72992. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64386/0.72937. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64687/0.72770. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64313/0.72985. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64333/0.72933. Took 0.18 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69944/0.68480. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69414/0.68790. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69263/0.69175. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69008/0.69539. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68941/0.69896. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68839/0.70211. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68770/0.70527. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68627/0.70794. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68690/0.70996. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68626/0.71169. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68579/0.71311. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68534/0.71435. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68413/0.71590. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68459/0.71702. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68387/0.71799. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68436/0.71867. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68317/0.71958. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68249/0.72025. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68269/0.72066. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68209/0.72110. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68139/0.72148. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68016/0.72188. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68177/0.72079. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68103/0.72076. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67991/0.72181. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68067/0.72209. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68008/0.72214. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67994/0.72232. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67770/0.72267. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67721/0.72276. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67774/0.72412. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67824/0.72485. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67634/0.72569. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67624/0.72666. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67634/0.72590. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67670/0.72619. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67654/0.72609. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67529/0.72631. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67492/0.72770. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67510/0.72883. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67311/0.72891. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67297/0.72981. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67523/0.73020. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67392/0.73187. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67317/0.73172. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67300/0.73246. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67270/0.73223. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67133/0.73173. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67268/0.73258. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67208/0.73327. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67081/0.73460. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67277/0.73502. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67185/0.73491. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67083/0.73573. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67020/0.73466. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67072/0.73705. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67075/0.73472. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67101/0.73447. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67035/0.73566. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66807/0.73673. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66749/0.73699. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66760/0.73976. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66781/0.74000. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66682/0.74123. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66732/0.74144. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66604/0.74103. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66554/0.73990. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66500/0.74242. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66520/0.74306. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66602/0.74331. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66407/0.74174. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66446/0.74294. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66288/0.74510. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66365/0.74459. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66400/0.74169. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66337/0.74473. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66180/0.74798. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66158/0.74497. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66282/0.74596. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66356/0.74599. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66043/0.74735. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66165/0.74646. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65980/0.74777. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66058/0.74707. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65879/0.75079. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65528/0.75341. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65855/0.74734. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65796/0.74857. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65758/0.74674. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65718/0.75022. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65735/0.75130. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65600/0.75002. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65668/0.75258. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65584/0.74870. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65487/0.75041. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65403/0.74909. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65376/0.75292. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65286/0.75419. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65374/0.75199. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65192/0.75250. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69315/0.69285. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69034/0.69418. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69005/0.69513. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68786/0.69649. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68649/0.69783. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68618/0.69949. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68576/0.70116. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68533/0.70259. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68534/0.70427. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68426/0.70584. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68426/0.70740. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68392/0.70875. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68342/0.71025. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68291/0.71166. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68181/0.71301. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68327/0.71432. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68134/0.71568. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68169/0.71694. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68166/0.71803. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68031/0.71903. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68035/0.71997. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68132/0.72105. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67898/0.72185. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67985/0.72225. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67901/0.72314. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67881/0.72472. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67792/0.72590. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67806/0.72582. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67686/0.72585. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67775/0.72679. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67768/0.72708. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67846/0.72754. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67540/0.72878. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67524/0.72951. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67425/0.72976. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67524/0.73025. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67459/0.73101. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67339/0.73121. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67342/0.73212. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67328/0.73267. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67321/0.73291. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67307/0.73329. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67172/0.73274. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67172/0.73322. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67153/0.73413. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66918/0.73516. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67022/0.73590. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66860/0.73674. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66642/0.73822. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66761/0.73722. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66768/0.73763. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66294/0.73828. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66463/0.73924. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66424/0.73953. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66390/0.73959. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66329/0.73999. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66005/0.74062. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65980/0.74142. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65807/0.74455. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65609/0.74492. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65702/0.74493. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65609/0.74587. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65413/0.74832. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65237/0.75053. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65357/0.75135. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.64877/0.75312. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65059/0.75347. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65141/0.75592. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64937/0.75569. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65035/0.75696. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64688/0.75853. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64567/0.75843. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64465/0.76375. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64243/0.76328. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64348/0.76406. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.63841/0.76720. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63941/0.76803. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64153/0.77318. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63576/0.76808. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63825/0.77035. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63637/0.77154. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63648/0.77324. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63391/0.77437. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63258/0.77808. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63222/0.77675. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62856/0.78169. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62680/0.78139. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62648/0.78563. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62644/0.78309. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62408/0.79346. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62428/0.78979. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62258/0.79372. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62204/0.79367. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62221/0.79643. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62106/0.79997. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62074/0.79831. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61801/0.80303. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61873/0.79858. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61296/0.80309. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61294/0.80981. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69230/0.69657. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68982/0.69750. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68904/0.69816. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68884/0.69872. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68793/0.69965. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68703/0.70062. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68714/0.70152. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68621/0.70272. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68548/0.70375. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68557/0.70481. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68568/0.70576. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68484/0.70680. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68403/0.70759. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68365/0.70858. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68411/0.70916. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68301/0.70963. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68348/0.71011. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68171/0.71073. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68188/0.71141. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68203/0.71168. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68169/0.71166. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68069/0.71239. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68176/0.71227. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68065/0.71224. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68081/0.71234. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68002/0.71228. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67946/0.71233. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68009/0.71231. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68033/0.71240. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67906/0.71249. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67918/0.71305. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67854/0.71328. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67809/0.71350. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67857/0.71364. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67832/0.71355. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.67759/0.71362. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67842/0.71371. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67750/0.71376. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67698/0.71425. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67516/0.71407. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67397/0.71383. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67428/0.71460. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67634/0.71546. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67520/0.71532. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67777/0.71531. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67591/0.71541. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67125/0.71588. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67409/0.71609. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67499/0.71637. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67271/0.71641. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67302/0.71625. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.67044/0.71720. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67216/0.71718. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67015/0.71756. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66892/0.71801. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66894/0.71970. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67069/0.71939. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66854/0.72039. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66749/0.72163. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66858/0.72191. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66796/0.72294. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66518/0.72403. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66774/0.72422. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 0.66596/0.72389. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66522/0.72551. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66459/0.72587. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66528/0.72733. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66337/0.72668. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66432/0.72894. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66370/0.72953. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66228/0.73006. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66212/0.72962. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65882/0.73119. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66269/0.73178. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66116/0.73150. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66143/0.73198. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65892/0.73304. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65655/0.73357. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65693/0.73403. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65867/0.73385. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65612/0.73733. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65387/0.73663. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65547/0.73698. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65450/0.74063. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65465/0.73977. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65506/0.73944. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65502/0.74031. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65458/0.74053. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65412/0.73911. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65113/0.74121. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65264/0.74155. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64710/0.74328. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65021/0.74281. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64690/0.74379. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65045/0.74267. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64935/0.74415. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64941/0.74459. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65009/0.74678. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64754/0.74359. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64444/0.74645. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69140/0.69690. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68851/0.69678. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68741/0.69636. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68624/0.69651. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68526/0.69634. Took 0.22 sec\n",
      "Epoch 5, Loss(train/val) 0.68545/0.69638. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68524/0.69607. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68423/0.69693. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68300/0.69696. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68346/0.69701. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68372/0.69672. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68330/0.69711. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68267/0.69773. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68218/0.69781. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68219/0.69823. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68152/0.69863. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68203/0.69926. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68125/0.69922. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68116/0.69977. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68020/0.69972. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68113/0.70133. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68030/0.70018. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67920/0.70168. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67889/0.70167. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67956/0.70220. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67836/0.70222. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67954/0.70324. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67794/0.70336. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67755/0.70402. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67658/0.70466. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67723/0.70557. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67635/0.70514. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67688/0.70547. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67687/0.70818. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67694/0.70788. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67627/0.70909. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67609/0.71072. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67523/0.71020. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67191/0.71162. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67510/0.71260. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67379/0.71316. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67369/0.71361. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67151/0.71500. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67150/0.71633. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67269/0.71834. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.67378/0.71846. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66930/0.71762. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67151/0.71967. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66931/0.72042. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66944/0.72179. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66945/0.72263. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66919/0.72402. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66812/0.72551. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66524/0.72675. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66600/0.72705. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66689/0.72891. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66543/0.72932. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66642/0.73139. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66534/0.73247. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66625/0.73348. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66483/0.73474. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66323/0.73632. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66383/0.73729. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66342/0.73836. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66091/0.73883. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66079/0.74120. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66089/0.74168. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65807/0.74317. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65880/0.74639. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65731/0.74865. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65756/0.74758. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65516/0.74975. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65594/0.75164. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65605/0.75363. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65507/0.75270. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65298/0.75378. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65143/0.75744. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65362/0.75740. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65099/0.75858. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65162/0.76142. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64803/0.76126. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64793/0.76115. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64734/0.76598. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64812/0.76601. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64918/0.76613. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64862/0.76617. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64426/0.76793. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64337/0.77210. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64398/0.77171. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64246/0.77275. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64471/0.77618. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63974/0.77679. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63910/0.77878. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64330/0.77895. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63963/0.77884. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63929/0.78062. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63943/0.78158. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64048/0.78394. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63477/0.78674. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63427/0.79068. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69514/0.68931. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69215/0.68937. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69073/0.68938. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68913/0.68983. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68933/0.69015. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68862/0.69070. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68697/0.69137. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68722/0.69199. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68589/0.69270. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68578/0.69371. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68611/0.69417. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68595/0.69495. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68592/0.69533. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68552/0.69599. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68443/0.69683. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68414/0.69763. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68506/0.69790. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68457/0.69847. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68341/0.69928. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68360/0.69961. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68353/0.70005. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68379/0.70072. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68306/0.70075. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68255/0.70169. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68321/0.70165. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68134/0.70215. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68115/0.70269. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68079/0.70358. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68103/0.70410. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68105/0.70467. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68095/0.70520. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67910/0.70570. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67975/0.70636. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67822/0.70725. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67898/0.70755. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67864/0.70811. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67830/0.70857. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67775/0.70919. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67656/0.70980. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67781/0.70979. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67723/0.71009. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67589/0.71118. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67700/0.71168. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67738/0.71242. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67584/0.71271. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67534/0.71344. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67698/0.71312. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67417/0.71389. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67470/0.71440. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67390/0.71406. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67693/0.71382. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67257/0.71434. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67281/0.71515. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67297/0.71504. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67134/0.71493. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67062/0.71503. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67054/0.71521. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66995/0.71521. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66752/0.71608. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66701/0.71633. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67061/0.71656. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66561/0.71761. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66695/0.71739. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66732/0.71830. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66549/0.71868. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66561/0.71838. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66407/0.71862. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66399/0.71827. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66469/0.71791. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66250/0.71864. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66381/0.71854. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66395/0.71796. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66219/0.71755. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66123/0.71780. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66013/0.71747. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65777/0.71830. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65850/0.71929. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65605/0.71813. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65845/0.71854. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65758/0.71840. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65501/0.71803. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65525/0.71897. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65471/0.72018. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65358/0.71973. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65307/0.71884. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65338/0.71798. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64994/0.71895. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64820/0.71936. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65181/0.72008. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64728/0.71902. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64401/0.72000. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64423/0.72020. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64322/0.71992. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64562/0.72026. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64163/0.72092. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64138/0.72047. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64112/0.72105. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64108/0.72244. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63659/0.72313. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63915/0.72487. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69447/0.69500. Took 0.34 sec\n",
      "Epoch 1, Loss(train/val) 0.69364/0.69507. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69253/0.69537. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69308/0.69556. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69319/0.69584. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69205/0.69612. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69202/0.69632. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69205/0.69646. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69225/0.69639. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69222/0.69637. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69242/0.69643. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69198/0.69649. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69174/0.69660. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69199/0.69650. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69150/0.69660. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69208/0.69685. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69168/0.69680. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69259/0.69677. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69237/0.69686. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.69149/0.69694. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.69139/0.69664. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69095/0.69645. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.69129/0.69649. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.69083/0.69653. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.69089/0.69648. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.69096/0.69628. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.69096/0.69619. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.69051/0.69628. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.69054/0.69610. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68994/0.69626. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.69023/0.69642. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.69010/0.69602. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68994/0.69594. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68952/0.69604. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68959/0.69604. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68895/0.69614. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68915/0.69641. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68865/0.69669. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68900/0.69684. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68905/0.69638. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68933/0.69616. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68838/0.69621. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68872/0.69651. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68737/0.69659. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68727/0.69595. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68719/0.69637. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68580/0.69630. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68652/0.69666. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68714/0.69661. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.68499/0.69711. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68470/0.69726. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.68512/0.69664. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68494/0.69659. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.68284/0.69630. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.68338/0.69594. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.68338/0.69613. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.68238/0.69593. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.68005/0.69660. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.68011/0.69726. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67947/0.69701. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67999/0.69717. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.67965/0.69670. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.68129/0.69725. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67721/0.69654. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67759/0.69691. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67734/0.69726. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67801/0.69848. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67607/0.69776. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67673/0.69788. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 0.67364/0.69825. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.67391/0.69865. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67308/0.69781. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67133/0.69712. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67473/0.69803. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67101/0.69950. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67076/0.69901. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67126/0.69770. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66830/0.70011. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67057/0.69843. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66651/0.69899. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66511/0.69918. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66585/0.70073. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66599/0.70063. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66326/0.69993. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66342/0.70106. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66271/0.70057. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65929/0.70227. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65854/0.70035. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65875/0.70132. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65928/0.70176. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65512/0.70371. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65555/0.70085. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65646/0.70207. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65532/0.70302. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65903/0.70254. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65295/0.70405. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65378/0.70304. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65106/0.70355. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64932/0.70744. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65017/0.70478. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69373/0.69121. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69318/0.69140. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69304/0.69128. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69242/0.69094. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69282/0.69080. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69250/0.69077. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69207/0.69048. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69235/0.69024. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69173/0.69042. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69175/0.69036. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69193/0.69038. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69104/0.69017. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69156/0.68998. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69138/0.69004. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69102/0.69001. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69048/0.68975. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.69115/0.68963. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68999/0.68953. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.69052/0.68957. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.69030/0.68970. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.69034/0.68935. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68949/0.68929. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68998/0.68931. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68923/0.68977. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68826/0.68941. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68851/0.68947. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68915/0.68949. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68921/0.68952. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68881/0.68952. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68878/0.68954. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68784/0.68985. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68761/0.68947. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68731/0.68934. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68674/0.68974. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68705/0.68964. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68694/0.68965. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68604/0.68943. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68475/0.68991. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68563/0.68958. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68492/0.68950. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68448/0.68959. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68411/0.68972. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68314/0.68895. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68116/0.68866. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68252/0.68884. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68260/0.68798. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68056/0.68807. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67981/0.68730. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67907/0.68770. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67852/0.68673. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67772/0.68628. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67623/0.68637. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67405/0.68518. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67480/0.68607. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67528/0.68552. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67341/0.68475. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66994/0.68304. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67063/0.68250. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67042/0.68178. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.66731/0.68163. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66635/0.68100. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66975/0.68024. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66774/0.67985. Took 0.22 sec\n",
      "Epoch 63, Loss(train/val) 0.66280/0.67847. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66449/0.67909. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66287/0.67985. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66229/0.67974. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65958/0.67922. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65758/0.67899. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65463/0.67784. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 0.65683/0.67808. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65695/0.67772. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 0.65708/0.67950. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65708/0.67987. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65161/0.67924. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65429/0.67848. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65449/0.68025. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65334/0.68098. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64808/0.68068. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64627/0.68032. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64755/0.68245. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64527/0.68171. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64092/0.68219. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64209/0.68180. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64391/0.68160. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63786/0.68208. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64156/0.68216. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63998/0.68184. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63546/0.68244. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63278/0.68238. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63590/0.68255. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63394/0.68267. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62963/0.68557. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63535/0.68617. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63294/0.68588. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62598/0.68494. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62740/0.68747. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62750/0.68860. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62204/0.68880. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62104/0.68797. Took 0.19 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69527/0.69672. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69346/0.69644. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69314/0.69584. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69337/0.69544. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69229/0.69497. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69252/0.69432. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69270/0.69405. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69185/0.69358. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69175/0.69352. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69191/0.69317. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69158/0.69259. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.69125/0.69199. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69101/0.69152. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69108/0.69111. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69077/0.69052. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68998/0.68981. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68967/0.68917. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68984/0.68867. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68965/0.68835. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68928/0.68787. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.68999/0.68726. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68861/0.68642. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68938/0.68613. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68840/0.68599. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.68827/0.68597. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68935/0.68557. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68672/0.68486. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68718/0.68459. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68651/0.68389. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68650/0.68353. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68651/0.68314. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68635/0.68283. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68506/0.68275. Took 0.30 sec\n",
      "Epoch 33, Loss(train/val) 0.68499/0.68146. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68498/0.68156. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68409/0.68063. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68420/0.68042. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68323/0.68016. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68260/0.67988. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68293/0.68005. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68273/0.67996. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68020/0.67850. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67832/0.67732. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67988/0.67857. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68069/0.67729. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68089/0.67848. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67779/0.67833. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67702/0.67741. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67785/0.67693. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67765/0.67696. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67457/0.67758. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67481/0.67639. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67467/0.67770. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67358/0.67750. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67096/0.67843. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66896/0.67644. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66990/0.67912. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66672/0.67632. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67050/0.67768. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66775/0.67533. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66574/0.67482. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66273/0.67643. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66519/0.67817. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66359/0.67750. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66362/0.67749. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65947/0.67830. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65676/0.67725. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65848/0.67757. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65686/0.67669. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65569/0.68008. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65475/0.67713. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65548/0.67904. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65130/0.67868. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65078/0.67936. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65057/0.68270. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64845/0.68321. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64902/0.68323. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64356/0.68631. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64553/0.68561. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64380/0.68283. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64406/0.68354. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63891/0.68653. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64440/0.68820. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63609/0.69179. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63665/0.69098. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63772/0.68964. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63583/0.69082. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63091/0.69707. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63006/0.69621. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63297/0.69472. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62643/0.69583. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62702/0.69685. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62565/0.69818. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62216/0.69687. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62594/0.70255. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62274/0.69927. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62009/0.70401. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62284/0.70787. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.61880/0.70558. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62179/0.70764. Took 0.18 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69383/0.69788. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69206/0.69666. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69213/0.69710. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69112/0.69744. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69089/0.69770. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69088/0.69847. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69000/0.69890. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68994/0.69951. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68928/0.70011. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68936/0.70075. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68765/0.70125. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68851/0.70168. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68792/0.70207. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68786/0.70231. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68759/0.70292. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68770/0.70325. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68792/0.70338. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68650/0.70413. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68666/0.70428. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68568/0.70439. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68586/0.70443. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68422/0.70470. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68516/0.70580. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68432/0.70582. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68398/0.70668. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68238/0.70684. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68392/0.70690. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68253/0.70699. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68216/0.70765. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68089/0.70792. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68132/0.70875. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68132/0.70891. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67983/0.70849. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67879/0.70899. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67897/0.70908. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67841/0.70889. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67661/0.70900. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67470/0.70985. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67295/0.70959. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67268/0.71058. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67121/0.71117. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.67142/0.71079. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66901/0.71130. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66975/0.71257. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66731/0.71241. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66723/0.71481. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66547/0.71574. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66808/0.71616. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66224/0.71852. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66279/0.71846. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66029/0.71924. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66093/0.71842. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65998/0.72069. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65940/0.72236. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65484/0.72432. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65679/0.72653. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65605/0.72590. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65727/0.72373. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65485/0.72869. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65501/0.72506. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65355/0.72609. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65159/0.72835. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64970/0.73322. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65173/0.73298. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64810/0.73651. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.64750/0.73841. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64705/0.73664. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.64664/0.73613. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64757/0.73813. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 0.64640/0.74036. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64215/0.74430. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64707/0.74288. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64303/0.74207. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64346/0.74147. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64212/0.74055. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64047/0.73822. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64099/0.74038. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64075/0.73956. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63770/0.74475. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63498/0.74586. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.63753/0.74522. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63283/0.74486. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63445/0.74755. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63100/0.74830. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63327/0.74611. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63029/0.75181. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63288/0.75132. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62811/0.75274. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62865/0.74800. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.62254/0.74677. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62862/0.75122. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62492/0.74990. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62171/0.75377. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62687/0.75301. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62624/0.75238. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62071/0.75621. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61808/0.75544. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61903/0.75633. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61744/0.75779. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62139/0.75624. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69386/0.69155. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69245/0.69130. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69184/0.69158. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69169/0.69198. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69162/0.69236. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69094/0.69276. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69047/0.69307. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69103/0.69342. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69080/0.69380. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69000/0.69419. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69060/0.69439. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69028/0.69473. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68971/0.69504. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68964/0.69516. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68950/0.69544. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69001/0.69564. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68836/0.69596. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68791/0.69618. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68932/0.69629. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68896/0.69648. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68834/0.69660. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68798/0.69681. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68825/0.69697. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68774/0.69716. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68725/0.69750. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68686/0.69765. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68794/0.69772. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68684/0.69802. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68732/0.69820. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68688/0.69848. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68693/0.69869. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68716/0.69888. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68602/0.69926. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68541/0.69933. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68537/0.69953. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68530/0.69960. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68491/0.69992. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68439/0.70052. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68322/0.70059. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68280/0.70097. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68321/0.70117. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68139/0.70163. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68203/0.70246. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68267/0.70276. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68098/0.70330. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68063/0.70359. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68047/0.70414. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67974/0.70439. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68053/0.70464. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67924/0.70528. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67828/0.70525. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.67602/0.70620. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67796/0.70670. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67714/0.70724. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67816/0.70806. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67504/0.70870. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67701/0.70924. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67452/0.71020. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67516/0.71130. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67398/0.71212. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67322/0.71289. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67291/0.71394. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67109/0.71549. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67242/0.71560. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67114/0.71689. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67076/0.71776. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66870/0.72002. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66745/0.72209. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66900/0.72266. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66987/0.72424. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66638/0.72499. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66527/0.72542. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66589/0.72662. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66608/0.72873. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66732/0.72970. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66522/0.72969. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66733/0.72983. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66493/0.73230. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66184/0.73263. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66270/0.73456. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66234/0.73609. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66239/0.73772. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66322/0.73830. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65937/0.73892. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65887/0.74132. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65929/0.74306. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65963/0.74310. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65846/0.74466. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65269/0.74760. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65781/0.74874. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65904/0.74878. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65816/0.75040. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65656/0.75124. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65387/0.75329. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64984/0.75495. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65357/0.75640. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65441/0.75712. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65205/0.75807. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65005/0.75945. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64974/0.76064. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69301/0.68723. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69218/0.68674. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69115/0.68616. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69087/0.68562. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69103/0.68499. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68974/0.68463. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68992/0.68428. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68898/0.68400. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68966/0.68391. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68941/0.68352. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68840/0.68312. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68831/0.68280. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68857/0.68253. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68763/0.68237. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68654/0.68210. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68657/0.68194. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68599/0.68198. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68593/0.68219. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68559/0.68173. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68643/0.68221. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68502/0.68241. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68507/0.68218. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68335/0.68246. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68357/0.68286. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68357/0.68322. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68348/0.68318. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68202/0.68335. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68118/0.68334. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68126/0.68375. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68066/0.68392. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68048/0.68453. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67954/0.68514. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67984/0.68572. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67913/0.68616. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67959/0.68684. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67798/0.68762. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67936/0.68827. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67695/0.68829. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67611/0.68862. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67819/0.68968. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67753/0.69005. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67374/0.69015. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67480/0.69103. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67366/0.69167. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67325/0.69300. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67359/0.69414. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67400/0.69414. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67185/0.69530. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67036/0.69540. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66952/0.69635. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66915/0.69914. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66708/0.69961. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66872/0.70093. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66826/0.70259. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66595/0.70434. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66655/0.70456. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66363/0.70525. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66497/0.70722. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66355/0.70841. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66374/0.70801. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66113/0.70941. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66062/0.71083. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65884/0.71345. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66077/0.71368. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65920/0.71470. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65701/0.71609. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65863/0.71708. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65812/0.71854. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65430/0.72146. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65433/0.72217. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65442/0.72387. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65463/0.72455. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65154/0.72471. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65231/0.72631. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65421/0.72520. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65218/0.72676. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64888/0.72768. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64854/0.72931. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64756/0.73054. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64753/0.73049. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64458/0.73281. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64974/0.73229. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64225/0.73184. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64341/0.73346. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64203/0.73426. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64103/0.73656. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64059/0.73710. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63955/0.73762. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63899/0.73734. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63694/0.73664. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63629/0.73666. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63492/0.73931. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63361/0.74070. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63083/0.73969. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63427/0.74021. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62828/0.74070. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63049/0.74208. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62748/0.74323. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63274/0.74454. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62480/0.74603. Took 0.18 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69382/0.69303. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69072/0.69464. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68912/0.69611. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68776/0.69786. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68672/0.69961. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68643/0.70131. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68504/0.70290. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68494/0.70409. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68409/0.70555. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68343/0.70720. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68437/0.70852. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68314/0.70970. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68175/0.71113. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68148/0.71265. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68195/0.71392. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68126/0.71497. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68081/0.71630. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68066/0.71748. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67953/0.71861. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67937/0.71988. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67917/0.72117. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67789/0.72237. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67728/0.72355. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67665/0.72469. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67744/0.72584. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67632/0.72643. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67658/0.72736. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67527/0.72846. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67456/0.73003. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67571/0.73058. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67369/0.73187. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67380/0.73299. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67396/0.73375. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67354/0.73409. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67344/0.73518. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67345/0.73515. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67234/0.73554. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67136/0.73638. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67181/0.73700. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67088/0.73797. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67018/0.73944. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.67002/0.74016. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66996/0.74120. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66887/0.74158. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66970/0.74241. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66874/0.74207. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67025/0.74242. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66897/0.74310. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66605/0.74267. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66787/0.74321. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66799/0.74405. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66554/0.74430. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66598/0.74467. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66522/0.74519. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66507/0.74601. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66591/0.74641. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66367/0.74703. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66402/0.74648. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66377/0.74678. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66326/0.74832. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66524/0.74780. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66237/0.74775. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66290/0.74827. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66263/0.74902. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66076/0.74960. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66119/0.74794. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66000/0.74850. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66233/0.74914. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66225/0.74826. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65870/0.74926. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66148/0.74843. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65944/0.74793. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65953/0.74702. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66017/0.74795. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65800/0.74747. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66001/0.74717. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65765/0.74727. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65896/0.74738. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65872/0.74672. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65628/0.74709. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65434/0.74751. Took 0.22 sec\n",
      "Epoch 81, Loss(train/val) 0.65411/0.74721. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65473/0.74690. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.65272/0.74671. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65553/0.74520. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.65463/0.74718. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65302/0.74788. Took 0.22 sec\n",
      "Epoch 87, Loss(train/val) 0.65509/0.74795. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65320/0.74816. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65314/0.74938. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.65130/0.74942. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65211/0.74763. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.65120/0.74714. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65091/0.74755. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65266/0.74720. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 0.64993/0.74751. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64783/0.74876. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64688/0.74975. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64761/0.74814. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.64778/0.74922. Took 0.20 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69386/0.69291. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69166/0.69474. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69058/0.69607. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68994/0.69699. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68948/0.69743. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68846/0.69827. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68854/0.69909. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68832/0.70003. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68777/0.70121. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68689/0.70258. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68549/0.70392. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68654/0.70523. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68439/0.70689. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68458/0.70898. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68385/0.71058. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68352/0.71214. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68275/0.71437. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68264/0.71639. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68211/0.71847. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68135/0.71944. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68105/0.72135. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68100/0.72361. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67960/0.72517. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68016/0.72653. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67858/0.72804. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67867/0.72996. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67813/0.73045. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67745/0.73162. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67818/0.73216. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67629/0.73339. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67794/0.73348. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67659/0.73515. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67594/0.73685. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67545/0.73746. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67506/0.73851. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67411/0.73852. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67539/0.73915. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67582/0.74173. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67349/0.74128. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67331/0.74129. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67233/0.74192. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67474/0.74266. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67311/0.74405. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67356/0.74302. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67249/0.74396. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67137/0.74471. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67188/0.74570. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67025/0.74586. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66900/0.74670. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66982/0.74843. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66908/0.74998. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66791/0.74997. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67008/0.74982. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66823/0.74996. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66677/0.74921. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66628/0.75121. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66687/0.75343. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66747/0.75214. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66770/0.75100. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66612/0.75285. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66488/0.75322. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66619/0.75437. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66460/0.75593. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66276/0.75746. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66574/0.75828. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66416/0.75845. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66096/0.76107. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66246/0.75956. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66102/0.76052. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66127/0.76040. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66212/0.76082. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66289/0.76185. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65999/0.76494. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65731/0.76598. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65948/0.76636. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65773/0.76848. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65775/0.76871. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65740/0.76993. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65876/0.76978. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65683/0.77175. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65481/0.77114. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65324/0.77284. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65567/0.77516. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65246/0.77940. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65509/0.77764. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65140/0.77934. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65334/0.77929. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65223/0.78118. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65148/0.78134. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64851/0.78446. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65173/0.78581. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64779/0.78648. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64847/0.78715. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64549/0.79047. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64881/0.79201. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64921/0.79264. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64402/0.79502. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64691/0.79679. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64544/0.79887. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64431/0.79924. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69397/0.69051. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69184/0.69053. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69057/0.69066. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69024/0.69074. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68927/0.69094. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68922/0.69114. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68857/0.69131. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68885/0.69144. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68743/0.69162. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68662/0.69184. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68633/0.69200. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68730/0.69228. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68607/0.69226. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68594/0.69237. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68546/0.69254. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68410/0.69270. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68466/0.69287. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68408/0.69280. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68310/0.69256. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68262/0.69237. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68246/0.69225. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68130/0.69199. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68155/0.69185. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68027/0.69135. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67957/0.69094. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67849/0.69091. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67984/0.69037. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67742/0.68994. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.67853/0.68956. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67600/0.68923. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67410/0.68836. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67331/0.68755. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.67403/0.68655. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67054/0.68654. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67035/0.68597. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67064/0.68500. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.66889/0.68489. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.66629/0.68438. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.66774/0.68354. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66464/0.68130. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.66337/0.67992. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66299/0.67942. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.66032/0.67930. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66158/0.67838. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 0.65868/0.67681. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65625/0.67630. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.65606/0.67550. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65580/0.67523. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.65376/0.67455. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65489/0.67257. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.65191/0.67201. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.65393/0.67165. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.64900/0.67162. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.64788/0.67270. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64675/0.67312. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64631/0.67224. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.63987/0.66972. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64365/0.67078. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64373/0.67217. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64535/0.67240. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.63655/0.67147. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64186/0.67169. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63933/0.67232. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.63674/0.67117. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63583/0.67204. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63745/0.67232. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63473/0.67289. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63339/0.67161. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63454/0.67382. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63319/0.67445. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63158/0.67394. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63283/0.67463. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62982/0.67221. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.62666/0.67456. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63097/0.67602. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.62656/0.67778. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62448/0.67875. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62533/0.67810. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62692/0.68128. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62493/0.68103. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62469/0.68160. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62099/0.68324. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61988/0.68471. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.61991/0.68529. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61693/0.68886. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62161/0.68783. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62039/0.68305. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62026/0.68409. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61557/0.68340. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61943/0.68284. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61439/0.68423. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61274/0.68617. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60892/0.68484. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.60708/0.68769. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61100/0.69102. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60859/0.68928. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60958/0.68687. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60974/0.69001. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60767/0.68979. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61138/0.69047. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69260/0.68971. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69137/0.68927. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69111/0.68918. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69062/0.68914. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69029/0.68923. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68951/0.68929. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68879/0.68923. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68886/0.68930. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68772/0.68911. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68702/0.68921. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68619/0.68940. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68555/0.68990. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68515/0.69008. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68458/0.69033. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68318/0.69080. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68212/0.69192. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68159/0.69221. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68104/0.69231. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68108/0.69319. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67909/0.69383. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67834/0.69366. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67754/0.69403. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67582/0.69415. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67607/0.69440. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67530/0.69509. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67409/0.69592. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67394/0.69690. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67183/0.69812. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67322/0.69791. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67009/0.69825. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67050/0.69932. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.66825/0.69937. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66496/0.70055. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.66704/0.70120. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66535/0.70154. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66234/0.70345. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.65886/0.70641. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.65762/0.70792. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.65794/0.70839. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.65593/0.70977. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.65433/0.71113. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.65561/0.71332. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.65580/0.71365. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.64877/0.71585. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.64981/0.71911. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.64668/0.71993. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65164/0.72051. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.64319/0.72228. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.64207/0.72452. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.64128/0.72613. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.64008/0.72700. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.64237/0.72710. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.63513/0.72839. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.63675/0.73084. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.63394/0.73225. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.63645/0.73363. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.63293/0.73534. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.63424/0.73737. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.63334/0.73689. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.62725/0.73677. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.62632/0.73819. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.62703/0.73793. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.62561/0.74112. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.62559/0.74035. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.62055/0.74305. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.62170/0.74328. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.61834/0.74488. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.61551/0.74568. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.62045/0.74549. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.61377/0.74895. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.61683/0.74909. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.61254/0.75052. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.60866/0.75402. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.60895/0.75158. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.60839/0.75303. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.60652/0.75349. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.60717/0.75468. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.60797/0.75367. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.60381/0.75395. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.59797/0.76065. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.60022/0.76151. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.59331/0.76178. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.60265/0.76204. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.59500/0.76008. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.58800/0.76221. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.58708/0.76542. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.58462/0.76885. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.58640/0.76894. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.59069/0.77035. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.58723/0.77521. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.57941/0.77540. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.58424/0.76904. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.57634/0.77426. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.58175/0.76864. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.57859/0.77441. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.58095/0.77652. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.57261/0.77359. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.58021/0.78025. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.57657/0.77597. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.57456/0.77904. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69396/0.69319. Took 0.36 sec\n",
      "Epoch 1, Loss(train/val) 0.69277/0.69348. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69251/0.69325. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69226/0.69327. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69167/0.69318. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69179/0.69324. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69163/0.69341. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69127/0.69361. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69110/0.69348. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69101/0.69379. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69062/0.69401. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69072/0.69443. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69023/0.69459. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68939/0.69498. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68878/0.69538. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68808/0.69537. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68815/0.69592. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68810/0.69683. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68873/0.69743. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68773/0.69790. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68708/0.69855. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68707/0.70004. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68595/0.70025. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68605/0.70092. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68571/0.70154. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68534/0.70263. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68488/0.70327. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68391/0.70398. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68388/0.70503. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68462/0.70492. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68320/0.70631. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68365/0.70663. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68177/0.70684. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68238/0.70764. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68180/0.70784. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68151/0.70826. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67933/0.70891. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68169/0.71075. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67907/0.71139. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67813/0.71214. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67721/0.71166. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67734/0.71214. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67644/0.71281. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67553/0.71352. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67412/0.71510. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67476/0.71486. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67526/0.71703. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67648/0.71683. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67455/0.71731. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67263/0.71865. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67162/0.71868. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67252/0.72082. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67114/0.72137. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66872/0.72263. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66978/0.72314. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66911/0.72479. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66778/0.72496. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66921/0.72524. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66940/0.72718. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66642/0.72753. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66582/0.72879. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66665/0.72884. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66420/0.73010. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66765/0.73217. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66403/0.73157. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66324/0.73423. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66212/0.73390. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66443/0.73429. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66197/0.73361. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66148/0.73631. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65849/0.73682. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65946/0.73758. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65877/0.73721. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65613/0.73919. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65658/0.73947. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66015/0.73984. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65521/0.74019. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65402/0.73968. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65327/0.74065. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65457/0.74296. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65406/0.74375. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65076/0.74465. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65113/0.74690. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65016/0.74670. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64853/0.74841. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64636/0.75008. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64668/0.74782. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64893/0.74999. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64766/0.74929. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64505/0.75067. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64407/0.75228. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64531/0.75201. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64410/0.75265. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64151/0.75381. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64213/0.75369. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63903/0.75237. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63881/0.75487. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63774/0.75427. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63343/0.75681. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63569/0.75810. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69333/0.69457. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69310/0.69466. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69273/0.69479. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69244/0.69502. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69297/0.69513. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69180/0.69511. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69181/0.69504. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69239/0.69505. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69173/0.69504. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69184/0.69502. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69133/0.69513. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69170/0.69529. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69158/0.69555. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69136/0.69589. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69136/0.69598. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69085/0.69613. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69052/0.69621. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69061/0.69616. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68994/0.69636. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.69074/0.69661. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.69044/0.69661. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.69029/0.69667. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68946/0.69648. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68977/0.69672. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68920/0.69685. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68903/0.69721. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68952/0.69724. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68858/0.69720. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68799/0.69746. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68748/0.69731. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68673/0.69736. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68719/0.69753. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68623/0.69741. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68488/0.69730. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68567/0.69769. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68432/0.69801. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68330/0.69787. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68490/0.69778. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68284/0.69756. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68251/0.69745. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68335/0.69785. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68266/0.69730. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67967/0.69720. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68108/0.69688. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67965/0.69689. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67765/0.69630. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67800/0.69619. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67770/0.69573. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67764/0.69580. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67446/0.69561. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67419/0.69675. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67194/0.69474. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67175/0.69510. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67131/0.69457. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67126/0.69380. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.66861/0.69303. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66858/0.69305. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66909/0.69405. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66927/0.69526. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66463/0.69456. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66582/0.69574. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66441/0.69356. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66216/0.69433. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66199/0.69249. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 0.66075/0.69429. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66152/0.69561. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66062/0.69351. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65654/0.69431. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65727/0.69362. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65422/0.69376. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65593/0.69382. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65469/0.69396. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65467/0.69329. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65341/0.69136. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65209/0.69295. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65092/0.69343. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64957/0.69284. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64709/0.69184. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64955/0.69235. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64483/0.69069. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64400/0.69078. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64537/0.69063. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63998/0.69057. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.64004/0.69245. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63837/0.69254. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64056/0.69080. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63440/0.69072. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63320/0.69100. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63455/0.69243. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63219/0.69139. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62995/0.69193. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63293/0.69235. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62901/0.69313. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62848/0.69106. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62527/0.69308. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62626/0.69320. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62597/0.69529. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61951/0.69626. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61562/0.69785. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61981/0.69895. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69389/0.69995. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69317/0.69820. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69183/0.69676. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69213/0.69575. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69213/0.69468. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69118/0.69394. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69058/0.69311. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69119/0.69242. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69014/0.69182. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69026/0.69123. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69001/0.69069. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69008/0.69036. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68932/0.68993. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68886/0.68971. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68886/0.68929. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68881/0.68905. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68883/0.68890. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68826/0.68844. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68782/0.68818. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68677/0.68781. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68759/0.68764. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68764/0.68724. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68654/0.68714. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68605/0.68694. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68566/0.68651. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68534/0.68618. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68502/0.68614. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68470/0.68590. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68506/0.68555. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68296/0.68501. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68306/0.68474. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68210/0.68438. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.68137/0.68403. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68053/0.68400. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68021/0.68368. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67966/0.68326. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67605/0.68197. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67747/0.68179. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67678/0.68135. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67638/0.68052. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67424/0.68030. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67120/0.67978. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67272/0.67863. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67174/0.67853. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66902/0.67807. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66928/0.67756. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66685/0.67807. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66757/0.67575. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66451/0.67592. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66636/0.67536. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66261/0.67460. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.66289/0.67492. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66381/0.67331. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66086/0.67266. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65992/0.67188. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65791/0.67332. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65766/0.67193. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65304/0.67225. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65244/0.67051. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65760/0.67020. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65284/0.66852. Took 0.22 sec\n",
      "Epoch 61, Loss(train/val) 0.65517/0.66971. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65174/0.66931. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.64677/0.67009. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64952/0.66922. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64795/0.66704. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64649/0.66865. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64418/0.66710. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64746/0.66858. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64544/0.66947. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64567/0.67088. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64766/0.66881. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64205/0.66720. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64308/0.66786. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63890/0.66850. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64055/0.66813. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63890/0.66885. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63554/0.66829. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63626/0.66753. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63402/0.66791. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63379/0.66570. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63086/0.66617. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.62984/0.66749. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63269/0.66754. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63325/0.66672. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63376/0.66718. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62766/0.66652. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.62751/0.66784. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62433/0.66637. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62628/0.66612. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62474/0.66862. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62229/0.66683. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.62118/0.66927. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62396/0.66608. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62351/0.66502. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.61818/0.66756. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.62227/0.66668. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61580/0.66817. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.62195/0.66782. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61937/0.66833. Took 0.20 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69572/0.69441. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69404/0.69328. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69376/0.69323. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.69298/0.69316. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69252/0.69321. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69245/0.69324. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69195/0.69358. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69175/0.69376. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69119/0.69407. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69171/0.69436. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69199/0.69508. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.69040/0.69498. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69043/0.69548. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69032/0.69631. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69068/0.69655. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.69044/0.69693. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68974/0.69718. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68886/0.69773. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68898/0.69825. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68865/0.69892. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68783/0.69969. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68819/0.70049. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68897/0.70140. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68807/0.70117. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68732/0.70192. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68690/0.70235. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68690/0.70346. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68628/0.70375. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68622/0.70437. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68501/0.70545. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68569/0.70593. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68526/0.70597. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68426/0.70711. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68487/0.70675. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68416/0.70736. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68501/0.70820. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68415/0.70908. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68340/0.70924. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68364/0.70980. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68275/0.71032. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68323/0.71087. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68284/0.71035. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68249/0.71098. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68192/0.71142. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68027/0.71223. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68105/0.71273. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68076/0.71282. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67929/0.71358. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68077/0.71422. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67976/0.71376. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67989/0.71349. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67952/0.71408. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67929/0.71420. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67863/0.71480. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67831/0.71503. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67728/0.71621. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67625/0.71591. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67708/0.71539. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67779/0.71508. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67692/0.71495. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67827/0.71586. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67657/0.71561. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67578/0.71563. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67603/0.71658. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67521/0.71703. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67466/0.71696. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67379/0.71758. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67396/0.71708. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67485/0.71711. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67455/0.71796. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67354/0.71832. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67235/0.71796. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67248/0.71733. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67279/0.71856. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67293/0.71860. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67115/0.71939. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67070/0.72041. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66981/0.71998. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67013/0.71947. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66965/0.71943. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66904/0.71896. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66930/0.71924. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66830/0.71891. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66675/0.71832. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66664/0.72029. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66643/0.72120. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66663/0.72082. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66686/0.72154. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66365/0.72189. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66382/0.72243. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66602/0.72191. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66324/0.72150. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66542/0.72121. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66221/0.72267. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66083/0.72317. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66189/0.72325. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66020/0.72348. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.66223/0.72245. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66092/0.72405. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66035/0.72520. Took 0.18 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69576/0.69763. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69305/0.69784. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69313/0.69802. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69197/0.69846. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69238/0.69883. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69239/0.69901. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69173/0.69933. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69135/0.69969. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69004/0.69998. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69174/0.70013. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69017/0.70049. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69039/0.70082. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68980/0.70085. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68975/0.70097. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68952/0.70146. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68947/0.70177. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68905/0.70186. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68827/0.70196. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68813/0.70198. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68786/0.70199. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68728/0.70196. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68830/0.70212. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68684/0.70226. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68745/0.70230. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68659/0.70235. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68668/0.70241. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68484/0.70220. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68554/0.70236. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68557/0.70288. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68452/0.70232. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68454/0.70215. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68325/0.70203. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68481/0.70221. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68378/0.70195. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68290/0.70237. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68357/0.70253. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68073/0.70239. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68126/0.70292. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68067/0.70275. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67956/0.70285. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68049/0.70343. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68087/0.70407. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67832/0.70375. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67968/0.70412. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67590/0.70421. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67703/0.70440. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67580/0.70538. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67715/0.70517. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67710/0.70597. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67584/0.70679. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67459/0.70707. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67439/0.70701. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67536/0.70772. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67360/0.70807. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67165/0.70846. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67274/0.70808. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67128/0.70838. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66989/0.70937. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66836/0.71025. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66932/0.70994. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66796/0.70962. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66837/0.70883. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66956/0.70908. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66842/0.70994. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66860/0.70997. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66664/0.71118. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66407/0.71243. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66344/0.71220. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66428/0.71188. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66186/0.71039. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66204/0.71044. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66199/0.71405. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66057/0.71220. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66031/0.71506. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65883/0.71316. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65822/0.71151. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65866/0.71514. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65550/0.71515. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65477/0.71395. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65430/0.71395. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65339/0.71152. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65063/0.71475. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65096/0.71438. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64850/0.71657. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64970/0.71677. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64785/0.71423. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64872/0.71362. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64617/0.71574. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64926/0.71682. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64503/0.71412. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64150/0.71434. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64410/0.71493. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64145/0.71721. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64397/0.71912. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64079/0.71782. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64103/0.71918. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64226/0.72136. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64137/0.71933. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63792/0.72099. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63820/0.71608. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69353/0.69696. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69270/0.69520. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69203/0.69459. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69267/0.69419. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69258/0.69405. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69082/0.69388. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69039/0.69404. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69174/0.69404. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69070/0.69402. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69024/0.69406. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68931/0.69425. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69073/0.69458. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68964/0.69459. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69000/0.69457. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68882/0.69485. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68750/0.69507. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68890/0.69503. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68703/0.69521. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68769/0.69570. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68648/0.69602. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68798/0.69658. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68769/0.69674. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68654/0.69699. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68616/0.69733. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68605/0.69774. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68492/0.69808. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68431/0.69874. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68470/0.69930. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68319/0.70029. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68255/0.70119. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68259/0.70178. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68136/0.70264. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68212/0.70342. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67986/0.70437. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67846/0.70527. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67772/0.70671. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67886/0.70717. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67865/0.70781. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67626/0.70869. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67781/0.70931. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67575/0.71030. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67330/0.71135. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67403/0.71318. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67533/0.71444. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67099/0.71587. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67344/0.71673. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67006/0.71774. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67090/0.71984. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67115/0.71962. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66866/0.72092. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66770/0.72156. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66593/0.72402. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66715/0.72292. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66311/0.72482. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66428/0.72722. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66238/0.72813. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66082/0.72943. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66089/0.73148. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65653/0.73156. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65477/0.73443. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65603/0.73539. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65885/0.73511. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65342/0.73848. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65550/0.73762. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65205/0.73769. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65168/0.74101. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65151/0.74231. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64682/0.74529. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64673/0.74579. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64436/0.74680. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64899/0.74728. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64359/0.74517. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63866/0.74771. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64019/0.75130. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.63979/0.75211. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63882/0.75419. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63745/0.75365. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.63705/0.75718. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63436/0.76144. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63527/0.75884. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63291/0.76055. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62763/0.76904. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62880/0.76795. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62493/0.77118. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62835/0.76946. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62465/0.76677. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63043/0.76969. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62210/0.76997. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61973/0.77183. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61868/0.77502. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62143/0.77317. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61558/0.77463. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.61594/0.77664. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61431/0.78400. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61589/0.77993. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61137/0.78393. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60892/0.78953. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61216/0.78672. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.61565/0.78802. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.60286/0.79582. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69714/0.69040. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69102/0.69365. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69194/0.69334. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69105/0.69363. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69074/0.69384. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68998/0.69432. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69019/0.69441. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68957/0.69458. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68972/0.69497. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68937/0.69524. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68844/0.69550. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68991/0.69580. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68858/0.69591. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68833/0.69593. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68809/0.69569. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68781/0.69614. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68777/0.69638. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68725/0.69676. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68703/0.69735. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68756/0.69743. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68571/0.69817. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68566/0.69842. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68594/0.69858. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68498/0.69915. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68562/0.69979. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68482/0.70018. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68423/0.70117. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68316/0.70161. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68364/0.70228. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68380/0.70241. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68310/0.70259. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68228/0.70314. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68248/0.70446. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68121/0.70452. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68234/0.70578. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68041/0.70597. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67965/0.70676. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68095/0.70733. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67921/0.70748. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68009/0.70825. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67837/0.70949. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67949/0.71015. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67778/0.71079. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67579/0.71246. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67511/0.71248. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67575/0.71353. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67619/0.71305. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67437/0.71495. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67361/0.71631. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67382/0.71656. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67245/0.71687. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67311/0.71818. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67266/0.71862. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67283/0.71909. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67020/0.72005. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67026/0.72086. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67068/0.72223. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66838/0.72405. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66942/0.72478. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66782/0.72661. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66415/0.72855. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66592/0.72879. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66507/0.73129. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66465/0.73270. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66300/0.73487. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66629/0.73649. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66218/0.73827. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66338/0.73643. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65964/0.73688. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65974/0.73736. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65825/0.74281. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65970/0.74390. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65824/0.74548. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65605/0.74483. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65457/0.74610. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65233/0.74828. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65651/0.75074. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65105/0.75275. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64705/0.75516. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65051/0.75475. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64867/0.75633. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64594/0.75930. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64918/0.75772. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64519/0.76044. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64533/0.76417. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64512/0.76215. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64092/0.76810. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64257/0.76714. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64196/0.76959. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64294/0.77399. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64095/0.77469. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63825/0.77837. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63669/0.77671. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64073/0.77518. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63549/0.78167. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63626/0.78569. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63399/0.78377. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63036/0.78350. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62976/0.78565. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62683/0.79271. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69472/0.68926. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69294/0.69130. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69135/0.69336. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69040/0.69497. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69013/0.69654. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68923/0.69812. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68898/0.69972. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68874/0.70113. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68817/0.70238. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68716/0.70318. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68729/0.70432. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68714/0.70555. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68707/0.70637. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68630/0.70740. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68568/0.70802. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68570/0.70895. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68521/0.71003. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68476/0.71100. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68488/0.71193. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68367/0.71215. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68374/0.71285. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68382/0.71339. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68296/0.71422. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68270/0.71514. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68214/0.71551. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68084/0.71643. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68116/0.71693. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67970/0.71821. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67972/0.71886. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67888/0.71988. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67834/0.72050. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67868/0.72107. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67711/0.72167. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67799/0.72224. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67708/0.72213. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67529/0.72250. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67635/0.72392. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67405/0.72404. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67331/0.72418. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67311/0.72548. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67231/0.72602. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67195/0.72566. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67043/0.72601. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66948/0.72755. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67009/0.72796. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66712/0.72710. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66780/0.72591. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66702/0.72507. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66435/0.72587. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66398/0.72467. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66151/0.72495. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66327/0.72474. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65993/0.72411. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66017/0.72556. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65505/0.72471. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65771/0.72487. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65785/0.72549. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65131/0.72860. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65121/0.72705. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65113/0.72689. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65005/0.72874. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64634/0.72790. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64700/0.73214. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64318/0.72987. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64272/0.72918. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63960/0.73238. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64078/0.72919. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.63988/0.73385. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63737/0.73053. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63674/0.73460. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63248/0.73610. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63286/0.73494. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63484/0.73641. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63052/0.73628. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62998/0.74176. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62774/0.74270. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62261/0.74321. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.62640/0.74075. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62130/0.74387. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62110/0.74198. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62201/0.74577. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61737/0.74169. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61911/0.74500. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.61742/0.74870. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61418/0.74712. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.61647/0.74945. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61172/0.75002. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.61047/0.74991. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60895/0.75044. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60726/0.75539. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61174/0.75282. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.60965/0.75011. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60492/0.75330. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60271/0.75504. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.59953/0.75526. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.59776/0.75519. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.59686/0.75779. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.59973/0.76139. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59409/0.76301. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.59594/0.76421. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69251/0.69580. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68978/0.69419. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68847/0.69296. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68836/0.69292. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68719/0.69202. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68644/0.69220. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68627/0.69194. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68554/0.69198. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68517/0.69252. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68578/0.69284. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68477/0.69308. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68552/0.69327. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68480/0.69321. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68398/0.69339. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68356/0.69365. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68401/0.69351. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68329/0.69409. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68290/0.69472. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68323/0.69472. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68287/0.69509. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68215/0.69528. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68174/0.69527. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68153/0.69570. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68106/0.69600. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68048/0.69591. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68087/0.69575. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68044/0.69588. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67947/0.69610. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67863/0.69692. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67864/0.69695. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67899/0.69687. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67810/0.69771. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67875/0.69821. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67684/0.69894. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67768/0.69790. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67667/0.69852. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67626/0.69849. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67492/0.69918. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67484/0.69936. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67495/0.70047. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67662/0.70029. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67542/0.70023. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67333/0.70050. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67404/0.69999. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67308/0.70042. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67300/0.70158. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67305/0.70140. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67210/0.70183. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67277/0.70172. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67123/0.70179. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67222/0.70257. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67074/0.70372. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66803/0.70290. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66847/0.70359. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66831/0.70490. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66884/0.70557. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66729/0.70426. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66809/0.70429. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66654/0.70444. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66744/0.70512. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66713/0.70544. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66354/0.70689. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66372/0.70768. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66563/0.70756. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66524/0.70824. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66289/0.70817. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66285/0.70939. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65839/0.70992. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66037/0.70914. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66131/0.71034. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66165/0.71122. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65853/0.70992. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66220/0.71027. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65873/0.71047. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65977/0.71138. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65949/0.71093. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65462/0.71113. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65944/0.71188. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65510/0.71111. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65483/0.71203. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65635/0.71181. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65293/0.71256. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65433/0.71217. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65116/0.71253. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65464/0.71247. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65026/0.71198. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65188/0.71264. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65318/0.71349. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64662/0.71318. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64654/0.71296. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64913/0.71524. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64686/0.71673. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64747/0.71653. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64430/0.71835. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.64307/0.71839. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64266/0.71737. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64375/0.71825. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63992/0.71651. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64385/0.71846. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64166/0.71912. Took 0.19 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69383/0.70072. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69092/0.69856. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69038/0.69892. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68905/0.69956. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68875/0.69996. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68783/0.70079. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68807/0.70116. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68724/0.70188. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68711/0.70279. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68679/0.70322. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68606/0.70404. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68577/0.70485. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68548/0.70542. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68559/0.70581. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68463/0.70633. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68467/0.70750. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68432/0.70773. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68320/0.70846. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68395/0.70899. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68290/0.70972. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68189/0.71066. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68238/0.71160. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68252/0.71220. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68075/0.71339. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68001/0.71422. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68044/0.71569. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67961/0.71657. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67883/0.71758. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67981/0.71820. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.67908/0.71891. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67882/0.72018. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67879/0.72102. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67857/0.72132. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67810/0.72284. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67849/0.72422. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67687/0.72541. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67554/0.72664. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67646/0.72827. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67296/0.72857. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67523/0.72982. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67460/0.73231. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67440/0.73307. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67574/0.73378. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67363/0.73381. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67228/0.73641. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67199/0.73844. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67231/0.74031. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 0.67150/0.74125. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67204/0.74224. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67113/0.74273. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67092/0.74346. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67317/0.74553. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67097/0.74651. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66725/0.74783. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66768/0.74954. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66885/0.75022. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66798/0.75174. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66753/0.75425. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66607/0.75608. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66617/0.75798. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66690/0.75893. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66673/0.76194. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66532/0.76119. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66245/0.76429. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66439/0.76517. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66378/0.76818. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66496/0.76827. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66099/0.77079. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66170/0.77300. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66323/0.77273. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65866/0.77582. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65908/0.77960. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65620/0.78284. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66089/0.78395. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65770/0.78501. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65828/0.78898. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65623/0.78954. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65387/0.79060. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65607/0.79142. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65552/0.79373. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65479/0.79591. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65297/0.79985. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65197/0.79990. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65271/0.79977. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65359/0.80292. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65143/0.80492. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64841/0.80827. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65238/0.80830. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64977/0.81189. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64724/0.81426. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64965/0.81485. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64720/0.81683. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64964/0.81892. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64497/0.82046. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64431/0.82376. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64518/0.82571. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64468/0.82741. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64257/0.83101. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63874/0.83338. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64270/0.83687. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69308/0.69606. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 0.69200/0.69590. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69259/0.69561. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69305/0.69554. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69172/0.69547. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69243/0.69522. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69244/0.69508. Took 0.22 sec\n",
      "Epoch 7, Loss(train/val) 0.69185/0.69503. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.69161/0.69498. Took 0.22 sec\n",
      "Epoch 9, Loss(train/val) 0.69096/0.69486. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69056/0.69485. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69065/0.69479. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69072/0.69480. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68977/0.69487. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69034/0.69487. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68943/0.69526. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68937/0.69536. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68885/0.69572. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68778/0.69589. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68879/0.69615. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68820/0.69641. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68768/0.69686. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68691/0.69726. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68659/0.69792. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68608/0.69833. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68571/0.69898. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68593/0.69914. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68530/0.69940. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68360/0.69911. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68475/0.69861. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68273/0.69842. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68363/0.69925. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68139/0.69915. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68087/0.69817. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68148/0.69807. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67808/0.69819. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68113/0.69932. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67808/0.70001. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67794/0.69974. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67682/0.69979. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67453/0.70024. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67367/0.70019. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67525/0.70099. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67466/0.70328. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67105/0.70280. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67172/0.70461. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67039/0.70699. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66914/0.70617. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66745/0.70977. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66559/0.71034. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66635/0.70947. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66533/0.71217. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66294/0.71300. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66495/0.71397. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65946/0.71610. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66170/0.71704. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66092/0.72020. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65757/0.72002. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66029/0.72123. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65825/0.72208. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65647/0.72416. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65330/0.72455. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65012/0.72701. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65063/0.72804. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65233/0.72831. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65087/0.73213. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65088/0.73164. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64893/0.73401. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64682/0.73414. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64550/0.73709. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64666/0.74039. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64422/0.74321. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64453/0.74629. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64467/0.74644. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64343/0.74711. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64187/0.74878. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64071/0.74951. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64338/0.75087. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63625/0.75197. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63415/0.75137. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63605/0.75435. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63679/0.75857. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63606/0.75199. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63499/0.75541. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62976/0.76070. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62932/0.76407. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63374/0.76363. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63216/0.76444. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63125/0.76817. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62622/0.76957. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62640/0.77301. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62509/0.77056. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62565/0.77183. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61692/0.77839. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61929/0.78546. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62431/0.78053. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61450/0.77981. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61823/0.78042. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61773/0.77869. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61647/0.78651. Took 0.18 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69207/0.68657. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69314/0.68673. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69227/0.68692. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69216/0.68716. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69083/0.68724. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69201/0.68725. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69166/0.68740. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69026/0.68716. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69037/0.68684. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69002/0.68699. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68965/0.68680. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68909/0.68682. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68994/0.68657. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68914/0.68643. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68896/0.68651. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68846/0.68672. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68801/0.68703. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68774/0.68676. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68810/0.68646. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68697/0.68676. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68776/0.68727. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68765/0.68704. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68633/0.68698. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68718/0.68720. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68688/0.68724. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68675/0.68717. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68609/0.68718. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68533/0.68752. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68619/0.68701. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68575/0.68709. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68516/0.68757. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68408/0.68776. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68464/0.68812. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68410/0.68793. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68317/0.68809. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68221/0.68770. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68361/0.68746. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68164/0.68768. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68099/0.68789. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68119/0.68794. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68051/0.68873. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67932/0.68839. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68044/0.68789. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67890/0.68792. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67810/0.68838. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67714/0.68904. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67826/0.68835. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67717/0.68930. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67603/0.68946. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67529/0.69001. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67441/0.68992. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67426/0.69050. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67292/0.68997. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67124/0.69054. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67242/0.69151. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67067/0.69183. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67191/0.69160. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66706/0.69285. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66601/0.69337. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66940/0.69424. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66329/0.69496. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66582/0.69587. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66124/0.69656. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66363/0.69621. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66438/0.69708. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66368/0.69635. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66254/0.69744. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65790/0.70003. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65804/0.70204. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65905/0.70098. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65698/0.70155. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65658/0.70297. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65411/0.70507. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65575/0.70394. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65306/0.70706. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65121/0.70848. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65415/0.70755. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64948/0.70940. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64907/0.71127. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64517/0.71291. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64524/0.71407. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64519/0.71229. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64456/0.71552. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64180/0.71740. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64396/0.71487. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64097/0.71768. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63898/0.71863. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63837/0.72083. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63948/0.72431. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63600/0.72321. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63741/0.72338. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63426/0.72556. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63758/0.72590. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.63273/0.72569. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.63423/0.72580. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63107/0.72952. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63382/0.73141. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63263/0.73082. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62816/0.73121. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62852/0.73240. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69615/0.69339. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69305/0.69376. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69213/0.69412. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69173/0.69416. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69141/0.69387. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69103/0.69356. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69111/0.69328. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69015/0.69301. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68980/0.69281. Took 0.22 sec\n",
      "Epoch 9, Loss(train/val) 0.69072/0.69242. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68902/0.69174. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68872/0.69116. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68816/0.69094. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68886/0.69046. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68694/0.68972. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68734/0.68939. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.68703/0.68911. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68644/0.68866. Took 0.22 sec\n",
      "Epoch 18, Loss(train/val) 0.68552/0.68813. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68563/0.68779. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.68484/0.68710. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68541/0.68695. Took 0.22 sec\n",
      "Epoch 22, Loss(train/val) 0.68367/0.68674. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68543/0.68617. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.68379/0.68596. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68411/0.68545. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 0.68243/0.68519. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68335/0.68488. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.68252/0.68445. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68171/0.68442. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68085/0.68384. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68128/0.68352. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.68180/0.68290. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68044/0.68265. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 0.68039/0.68276. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68006/0.68250. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67863/0.68227. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68054/0.68222. Took 0.22 sec\n",
      "Epoch 38, Loss(train/val) 0.67764/0.68128. Took 0.22 sec\n",
      "Epoch 39, Loss(train/val) 0.67926/0.68118. Took 0.23 sec\n",
      "Epoch 40, Loss(train/val) 0.67856/0.68108. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67689/0.68054. Took 0.22 sec\n",
      "Epoch 42, Loss(train/val) 0.67623/0.68065. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67606/0.68050. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67607/0.67983. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67546/0.67934. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67434/0.67978. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67385/0.67969. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67414/0.67963. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67361/0.67964. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67224/0.67915. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66986/0.67901. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67256/0.67964. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67225/0.67877. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66967/0.67983. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67064/0.68006. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67098/0.67987. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66877/0.67978. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66884/0.68038. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66573/0.67908. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66511/0.67923. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66720/0.67876. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66534/0.67845. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66372/0.67816. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 0.66516/0.67869. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66355/0.67917. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66335/0.67872. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66150/0.67845. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 0.66331/0.67979. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66238/0.67840. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66261/0.67864. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65896/0.67944. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65938/0.68012. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65531/0.67917. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65539/0.67969. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65538/0.67943. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65467/0.67978. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65332/0.67905. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 0.65206/0.68036. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65144/0.67951. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65178/0.68067. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65123/0.68054. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64835/0.68124. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65009/0.68174. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64807/0.68169. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64887/0.68224. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64893/0.68126. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64383/0.68247. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64018/0.68190. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64463/0.68218. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64027/0.68211. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63967/0.68323. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63814/0.68324. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63872/0.68359. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63695/0.68268. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63638/0.68380. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63627/0.68520. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63479/0.68458. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63317/0.68289. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63281/0.68489. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69445/0.68837. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69048/0.68755. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69030/0.68845. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68938/0.68920. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68949/0.69020. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68884/0.69110. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68842/0.69194. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68840/0.69243. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68767/0.69335. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68711/0.69434. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68653/0.69511. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68679/0.69603. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68685/0.69666. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68710/0.69746. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68676/0.69813. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68634/0.69808. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68651/0.69839. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68571/0.69902. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68570/0.69937. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68587/0.69953. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68586/0.69974. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68555/0.70009. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68434/0.70080. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68521/0.70076. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68514/0.70111. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68445/0.70122. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68439/0.70163. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68437/0.70169. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68296/0.70235. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68297/0.70271. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68271/0.70232. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68218/0.70287. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68178/0.70273. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68282/0.70298. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68102/0.70242. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68125/0.70212. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68076/0.70280. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68067/0.70302. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67919/0.70346. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68040/0.70355. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67905/0.70355. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67956/0.70369. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67976/0.70338. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67773/0.70334. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67696/0.70309. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67737/0.70329. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67678/0.70288. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67559/0.70337. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67560/0.70315. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67513/0.70327. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67468/0.70273. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67380/0.70334. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67262/0.70333. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67370/0.70349. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67253/0.70366. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67127/0.70345. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67308/0.70374. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66983/0.70380. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66892/0.70366. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67006/0.70350. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66864/0.70387. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66921/0.70319. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66802/0.70320. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66676/0.70239. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66459/0.70409. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66587/0.70417. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66574/0.70400. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66520/0.70373. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66665/0.70392. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66331/0.70546. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66439/0.70536. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66191/0.70568. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66306/0.70510. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65850/0.70584. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66048/0.70573. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66104/0.70481. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66137/0.70554. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65903/0.70696. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65477/0.70786. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65950/0.70652. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65926/0.70926. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65431/0.70582. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65438/0.70534. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65644/0.70697. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65171/0.70857. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65494/0.70663. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65175/0.70848. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65207/0.70744. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64974/0.70827. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65086/0.70929. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65121/0.70724. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65146/0.70839. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64913/0.71116. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64805/0.71017. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65133/0.71199. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64715/0.70693. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64702/0.70892. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64259/0.70936. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64311/0.70742. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64130/0.70904. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69771/0.69673. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69142/0.70020. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69021/0.70152. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68990/0.70191. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68926/0.70211. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68915/0.70208. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68868/0.70210. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68912/0.70188. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68840/0.70175. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68844/0.70154. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68747/0.70148. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68793/0.70148. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68698/0.70165. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68817/0.70175. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68678/0.70173. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68671/0.70159. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68688/0.70177. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68776/0.70167. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68649/0.70196. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68634/0.70183. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68631/0.70188. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68478/0.70221. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68628/0.70231. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68570/0.70235. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68424/0.70237. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68526/0.70278. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68707/0.70252. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68524/0.70304. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68587/0.70285. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68522/0.70341. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68473/0.70326. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68226/0.70362. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68444/0.70357. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68219/0.70383. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68470/0.70387. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68292/0.70365. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68289/0.70374. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68376/0.70366. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68260/0.70302. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68237/0.70363. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68169/0.70354. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68063/0.70358. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68059/0.70370. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68185/0.70409. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68054/0.70404. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67939/0.70383. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68098/0.70405. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68002/0.70503. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67888/0.70496. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68036/0.70427. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67680/0.70491. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67740/0.70508. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67776/0.70504. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67545/0.70567. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67501/0.70563. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67639/0.70488. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67660/0.70486. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67318/0.70478. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67346/0.70579. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67420/0.70444. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67325/0.70563. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67346/0.70522. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67255/0.70578. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67303/0.70587. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67206/0.70681. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67241/0.70593. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67009/0.70617. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67136/0.70605. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67174/0.70669. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66980/0.70571. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66897/0.70588. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66953/0.70606. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66800/0.70639. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66639/0.70552. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66707/0.70600. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66351/0.70639. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66549/0.70678. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66262/0.70742. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66108/0.70768. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66291/0.70906. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66327/0.70930. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66066/0.71052. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65945/0.71011. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65723/0.70882. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65706/0.71181. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65962/0.71135. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65475/0.71069. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65411/0.71078. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65523/0.70955. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65515/0.71052. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65397/0.71149. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65312/0.71431. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65064/0.71231. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64954/0.71505. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65446/0.71529. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65213/0.71596. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65231/0.71697. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65069/0.71757. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64979/0.71976. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64793/0.72105. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69229/0.69543. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69133/0.69575. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69086/0.69616. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69036/0.69657. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68984/0.69703. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68933/0.69758. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68932/0.69811. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68909/0.69864. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68858/0.69890. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68806/0.69906. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68706/0.69932. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68720/0.69936. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68700/0.69943. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68581/0.69943. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68464/0.69961. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68447/0.69986. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68405/0.70028. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68364/0.70023. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68296/0.70016. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68107/0.70035. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67971/0.70043. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67945/0.70037. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67949/0.70043. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67621/0.70118. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67712/0.70139. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67565/0.70133. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67293/0.70162. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67360/0.70254. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.66869/0.70380. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.66978/0.70462. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.66929/0.70450. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.66897/0.70562. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.66732/0.70728. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.66295/0.70646. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.66458/0.70849. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66234/0.70908. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.66278/0.71027. Took 0.22 sec\n",
      "Epoch 37, Loss(train/val) 0.65942/0.71177. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.65738/0.71270. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.65838/0.71280. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.65472/0.71412. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.65474/0.71635. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.65494/0.71706. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.65409/0.71883. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.65325/0.72002. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.65255/0.71968. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.65122/0.72054. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 0.65393/0.72052. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65168/0.72194. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.64732/0.72264. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.64910/0.72414. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.64892/0.72393. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.64796/0.72425. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.64543/0.72445. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.64522/0.72688. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.64616/0.72677. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 0.64493/0.72631. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64201/0.73044. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.64290/0.72922. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.64510/0.72962. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.63984/0.73362. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64105/0.73215. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.63919/0.73417. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.63376/0.73772. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63795/0.73619. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.63381/0.73994. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.63514/0.73775. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63381/0.73938. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63444/0.73909. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63556/0.74000. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63214/0.73978. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.63150/0.73948. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63133/0.74211. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63208/0.74300. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62572/0.74428. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.62637/0.74473. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.62689/0.74554. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62282/0.74659. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62504/0.74753. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62263/0.74698. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62365/0.74813. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.62215/0.75115. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.62417/0.75132. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62139/0.75160. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.61975/0.75242. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61751/0.75778. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.61771/0.76299. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61938/0.76560. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61759/0.76361. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61330/0.76498. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.61148/0.76643. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61599/0.76594. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.61134/0.76733. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.60872/0.77181. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.61000/0.77278. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60952/0.77815. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.60780/0.77556. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60303/0.77664. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60506/0.77336. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60685/0.77794. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69480/0.70732. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69154/0.70295. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69085/0.70072. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68957/0.69914. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68933/0.69811. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68894/0.69741. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68852/0.69691. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68815/0.69673. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68813/0.69658. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68740/0.69683. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68737/0.69726. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68714/0.69789. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68640/0.69806. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68657/0.69858. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68600/0.69900. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68665/0.69921. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68650/0.69985. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68605/0.70025. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68549/0.70071. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68571/0.70087. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68474/0.70148. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68505/0.70222. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68487/0.70223. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68457/0.70317. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68456/0.70366. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68428/0.70434. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68402/0.70457. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68373/0.70496. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68499/0.70490. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68255/0.70599. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68347/0.70584. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68232/0.70629. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68323/0.70651. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68258/0.70697. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68317/0.70742. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68186/0.70800. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68252/0.70869. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68148/0.70830. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68151/0.70915. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68116/0.70940. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68104/0.70933. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68050/0.70912. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68039/0.70938. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68035/0.71005. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67963/0.71012. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67942/0.70964. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67986/0.70994. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67814/0.71038. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67872/0.71095. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67931/0.71086. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67857/0.71136. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67886/0.71193. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67839/0.71175. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67722/0.71202. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67614/0.71162. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67711/0.71281. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67707/0.71248. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67562/0.71248. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67492/0.71256. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67459/0.71275. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67557/0.71170. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67490/0.71135. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67518/0.71058. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67340/0.71046. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67411/0.71152. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67391/0.71224. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67330/0.71141. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67144/0.71069. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67255/0.71225. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67124/0.71251. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66922/0.71271. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67150/0.71217. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67077/0.71340. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66894/0.71390. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67004/0.71457. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66934/0.71306. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66880/0.71390. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66952/0.71465. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66846/0.71433. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66735/0.71392. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66654/0.71566. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66652/0.71437. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66627/0.71485. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66544/0.71511. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66668/0.71411. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66509/0.71560. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66460/0.71686. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66584/0.71656. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66375/0.71731. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66131/0.71723. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66078/0.71599. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66131/0.71789. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65911/0.71752. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65948/0.71850. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66054/0.71874. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65776/0.72044. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65879/0.71885. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65989/0.71948. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65506/0.72012. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65590/0.71909. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69249/0.69505. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69141/0.69576. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69093/0.69585. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69044/0.69585. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69045/0.69558. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68932/0.69556. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69018/0.69537. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68954/0.69526. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68947/0.69514. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68924/0.69522. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68907/0.69537. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68918/0.69527. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68877/0.69514. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68844/0.69508. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68789/0.69512. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68820/0.69506. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68821/0.69537. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68750/0.69558. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68705/0.69547. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68666/0.69551. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68605/0.69564. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68602/0.69601. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68558/0.69627. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68483/0.69643. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68498/0.69655. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68412/0.69692. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68500/0.69715. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68362/0.69746. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68358/0.69761. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68354/0.69790. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68367/0.69832. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68288/0.69874. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68175/0.69885. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68198/0.69939. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68066/0.69998. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68137/0.69999. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67998/0.70035. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67857/0.70078. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67932/0.70094. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68016/0.70108. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67668/0.70207. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67728/0.70273. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67712/0.70349. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67556/0.70397. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67511/0.70443. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67564/0.70537. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67207/0.70607. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67204/0.70709. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67246/0.70782. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67065/0.70904. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67082/0.70916. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66798/0.71047. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67054/0.71232. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66684/0.71283. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66709/0.71382. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66631/0.71518. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66609/0.71575. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66413/0.71688. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66424/0.71824. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66402/0.71867. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66246/0.71922. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66180/0.72015. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66054/0.72152. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66127/0.72510. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66261/0.72325. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65953/0.72549. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65774/0.72565. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65445/0.72744. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65502/0.73041. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65481/0.72911. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65248/0.72812. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65577/0.72965. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65166/0.72960. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65178/0.73008. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65078/0.73170. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64778/0.73401. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64915/0.73545. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64694/0.73543. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64567/0.73673. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64300/0.73635. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64447/0.73737. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64146/0.74006. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63910/0.73995. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64001/0.73976. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63892/0.73974. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63532/0.73937. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63605/0.74132. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63594/0.74376. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63678/0.74632. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63536/0.74344. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63379/0.74368. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62901/0.74379. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63062/0.74293. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62914/0.74259. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63047/0.74356. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62610/0.74513. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62564/0.74727. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62638/0.74428. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62316/0.74668. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62179/0.74851. Took 0.18 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69443/0.69882. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69183/0.69825. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69166/0.69772. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69154/0.69732. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69086/0.69775. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69032/0.69771. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69032/0.69743. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68977/0.69788. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69034/0.69768. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68955/0.69777. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68871/0.69787. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68851/0.69818. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68849/0.69848. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68765/0.69861. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68754/0.69838. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68772/0.69873. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68736/0.69888. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68754/0.69897. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68678/0.69929. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68613/0.69948. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68648/0.69991. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68583/0.70004. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68565/0.70033. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68546/0.70001. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68471/0.70054. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68484/0.70059. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68562/0.70043. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68472/0.70103. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68350/0.70100. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68471/0.70141. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68357/0.70138. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 0.68423/0.70090. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68370/0.70116. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68316/0.70105. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68303/0.70196. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68380/0.70185. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68278/0.70160. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68290/0.70234. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68169/0.70176. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68168/0.70176. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68148/0.70154. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68193/0.70121. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68187/0.70176. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68173/0.70157. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68080/0.70212. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68058/0.70179. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68008/0.70128. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68106/0.70111. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67953/0.70133. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.68056/0.70132. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67940/0.70125. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67861/0.70150. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.68033/0.70179. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68100/0.70074. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67704/0.70125. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67891/0.70090. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67781/0.70073. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67939/0.70005. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67786/0.70010. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67739/0.70002. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67756/0.69980. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67716/0.69923. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67559/0.69908. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67457/0.69939. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67647/0.69843. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67549/0.69797. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67435/0.69983. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.67439/0.69775. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67436/0.69840. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67351/0.69869. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67584/0.69739. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67309/0.69708. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.67369/0.69699. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67293/0.69727. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67166/0.69841. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67157/0.69678. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 0.67232/0.69599. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.67036/0.69567. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.67076/0.69584. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.67013/0.69522. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 0.67208/0.69719. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66828/0.69750. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66982/0.69731. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67007/0.69783. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66953/0.69707. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66604/0.69622. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66790/0.69657. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66606/0.69501. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.66828/0.69696. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66510/0.69618. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66638/0.69577. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66612/0.69449. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66559/0.69555. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66386/0.69462. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66535/0.69350. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 0.66184/0.69338. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.66368/0.69234. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66130/0.69274. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65955/0.69267. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66078/0.69187. Took 0.19 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69473/0.69429. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.69363. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69168/0.69351. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69116/0.69349. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69083/0.69376. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69057/0.69400. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69032/0.69430. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68955/0.69469. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68851/0.69494. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68891/0.69529. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68868/0.69586. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68803/0.69637. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68822/0.69678. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68786/0.69697. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68672/0.69741. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68624/0.69774. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68654/0.69805. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68540/0.69843. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68518/0.69882. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68532/0.69906. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68528/0.69936. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68454/0.69974. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68472/0.70032. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68304/0.70073. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68391/0.70119. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68270/0.70141. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68346/0.70156. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68363/0.70211. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68287/0.70256. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68238/0.70280. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68271/0.70271. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68141/0.70270. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68030/0.70290. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68084/0.70345. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68005/0.70372. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67822/0.70373. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67817/0.70383. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67964/0.70402. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67783/0.70390. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67903/0.70403. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67519/0.70396. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67788/0.70415. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67420/0.70456. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67479/0.70475. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67495/0.70560. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67432/0.70592. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67372/0.70579. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67194/0.70575. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67184/0.70645. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67272/0.70647. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67224/0.70667. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67280/0.70678. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66982/0.70703. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66727/0.70742. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66736/0.70798. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66592/0.70806. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66682/0.70804. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66501/0.70819. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66484/0.70879. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66336/0.70925. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66255/0.70981. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66026/0.70920. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66140/0.70918. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66048/0.70980. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65909/0.71052. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66055/0.71104. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65663/0.71247. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65781/0.71231. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65844/0.71320. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65386/0.71185. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65354/0.71463. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65645/0.71280. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65549/0.71401. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65391/0.71381. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65126/0.71595. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65264/0.71693. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65029/0.71862. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64904/0.71734. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64863/0.71729. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65095/0.71825. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64794/0.71741. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64516/0.71632. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64413/0.71645. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64391/0.71785. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64375/0.71841. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63931/0.71908. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63986/0.72062. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63938/0.72132. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63502/0.72281. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63792/0.72060. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63557/0.72406. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63551/0.72256. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63114/0.72158. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63508/0.71988. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63149/0.71923. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63051/0.72135. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62948/0.72576. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63541/0.72165. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62616/0.72361. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62642/0.72365. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69334/0.69325. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.69214/0.69213. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69187/0.69201. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69164/0.69168. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69162/0.69149. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69134/0.69150. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69163/0.69139. Took 0.22 sec\n",
      "Epoch 7, Loss(train/val) 0.69089/0.69126. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69028/0.69124. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69049/0.69121. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69078/0.69111. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69021/0.69092. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69014/0.69094. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69000/0.69100. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68983/0.69081. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68943/0.69047. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68856/0.69050. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68945/0.69052. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68694/0.69085. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68752/0.69075. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68846/0.69069. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68719/0.69026. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68806/0.69047. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68700/0.69096. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68670/0.69095. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68696/0.69102. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68576/0.69104. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68583/0.69089. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68653/0.69098. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68529/0.69094. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68578/0.69118. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68440/0.69130. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68430/0.69147. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68357/0.69121. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68408/0.69167. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68261/0.69134. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68310/0.69154. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68170/0.69138. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68325/0.69191. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68269/0.69194. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68111/0.69251. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68095/0.69330. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67795/0.69374. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68027/0.69415. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67839/0.69500. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67755/0.69536. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67560/0.69570. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67828/0.69682. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67733/0.69705. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67631/0.69680. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67615/0.69731. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67396/0.69751. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67493/0.69952. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67276/0.69984. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67364/0.70151. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67271/0.70212. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67298/0.70048. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67229/0.70201. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67124/0.70362. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67226/0.70502. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66976/0.70484. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66835/0.70528. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66742/0.70730. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66966/0.70829. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66900/0.70869. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66778/0.70928. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66849/0.70703. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66681/0.70976. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66485/0.71051. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66497/0.71199. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66477/0.71200. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65940/0.71448. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66316/0.71339. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66153/0.71497. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65906/0.71635. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65778/0.71828. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66136/0.71986. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66143/0.72018. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66079/0.72299. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65612/0.72331. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65530/0.72496. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65316/0.72701. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65903/0.72225. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65585/0.72542. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65520/0.72450. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65448/0.72823. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65383/0.72984. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65044/0.73405. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65511/0.73318. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64848/0.72956. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65194/0.73436. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64806/0.73339. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65131/0.73646. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64697/0.73595. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64487/0.74149. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64587/0.74626. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65029/0.74013. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64561/0.74170. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64203/0.74417. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64624/0.75032. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69296/0.69111. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69306/0.69124. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69203/0.69130. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 0.69258/0.69150. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69092/0.69163. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69130/0.69183. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69041/0.69202. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69009/0.69217. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69072/0.69239. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68935/0.69257. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68940/0.69284. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68872/0.69326. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68747/0.69370. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68812/0.69400. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68698/0.69437. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68646/0.69505. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68558/0.69571. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68481/0.69637. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68430/0.69743. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68382/0.69812. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68303/0.69903. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68236/0.70005. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68121/0.70107. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68043/0.70206. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68008/0.70315. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67919/0.70415. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67766/0.70519. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68054/0.70588. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67858/0.70735. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67766/0.70761. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67506/0.70910. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67483/0.70996. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67413/0.71079. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67156/0.71261. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67058/0.71296. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66937/0.71505. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66794/0.71671. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66927/0.71745. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66758/0.71856. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66368/0.72179. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66218/0.72345. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66458/0.72540. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66033/0.72697. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.65992/0.72929. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.65793/0.73286. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.65653/0.73301. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66069/0.73284. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65538/0.73622. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65176/0.73850. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.65438/0.73796. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.64845/0.74041. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.64910/0.74134. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.64820/0.74813. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.64882/0.74640. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64617/0.74744. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.64516/0.74465. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64486/0.75242. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64514/0.74754. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64158/0.75250. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.64177/0.75287. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.63929/0.75483. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.63977/0.75489. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64093/0.75763. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.63751/0.75664. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64014/0.75609. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.63318/0.76111. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63503/0.76543. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.63176/0.76526. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63360/0.77054. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.62927/0.77146. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.63241/0.77446. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.62752/0.77502. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62854/0.77728. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.62431/0.77545. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.62644/0.78025. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62535/0.78177. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62181/0.78711. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62461/0.78685. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62288/0.78731. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62135/0.78778. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.61865/0.78734. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61469/0.79004. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61731/0.79128. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61744/0.79424. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62075/0.79254. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61541/0.79318. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61232/0.79542. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61313/0.79764. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.61418/0.79640. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61238/0.79765. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.61400/0.80328. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.60417/0.80556. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.60705/0.80414. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61164/0.80342. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.60513/0.80582. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60344/0.80804. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.59999/0.81465. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60066/0.81865. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.60422/0.81759. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59912/0.82017. Took 0.19 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69235/0.69103. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69111/0.69132. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69072/0.69113. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69052/0.69077. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68908/0.69055. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68846/0.69061. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68836/0.69062. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68727/0.69057. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68750/0.69048. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68752/0.69039. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68698/0.69007. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68621/0.69019. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68670/0.69010. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68645/0.69016. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68548/0.69035. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68590/0.69038. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68525/0.69075. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68484/0.69103. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68463/0.69121. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68509/0.69114. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68476/0.69146. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68377/0.69160. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68348/0.69232. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68355/0.69247. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68382/0.69316. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68307/0.69297. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68406/0.69284. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68264/0.69308. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68245/0.69375. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68251/0.69398. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68077/0.69465. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68263/0.69537. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68224/0.69539. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68112/0.69583. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68140/0.69623. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68221/0.69604. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68094/0.69611. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68075/0.69632. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68030/0.69678. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67907/0.69744. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68102/0.69756. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67962/0.69775. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67935/0.69802. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67915/0.69867. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67850/0.69971. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67827/0.69991. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67911/0.70083. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67760/0.70106. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67814/0.70146. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67736/0.70169. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67739/0.70162. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67716/0.70232. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67710/0.70274. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67772/0.70265. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67742/0.70266. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67572/0.70331. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67533/0.70366. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67517/0.70431. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67371/0.70464. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67556/0.70528. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67219/0.70503. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67531/0.70510. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67396/0.70592. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67407/0.70516. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67300/0.70557. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67236/0.70601. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67148/0.70745. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67282/0.70742. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67045/0.70711. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67006/0.70780. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67137/0.70737. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67180/0.70750. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66982/0.70819. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66963/0.70882. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66925/0.70904. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66818/0.70938. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66683/0.70949. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66874/0.71002. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66616/0.70990. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66749/0.70986. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66763/0.70947. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66645/0.71016. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66641/0.71010. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66633/0.71002. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66522/0.71146. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66481/0.71248. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66508/0.71261. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66363/0.71251. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66122/0.71262. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66263/0.71394. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66176/0.71318. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66327/0.71333. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66001/0.71301. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66120/0.71269. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65890/0.71391. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66016/0.71466. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65779/0.71583. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65822/0.71609. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65664/0.71715. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65639/0.71852. Took 0.19 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69481/0.69729. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69170/0.69609. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69075/0.69547. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68975/0.69511. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68896/0.69522. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68794/0.69555. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68830/0.69591. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68790/0.69617. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68682/0.69632. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68737/0.69658. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68631/0.69685. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68646/0.69701. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68628/0.69735. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68609/0.69747. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68625/0.69749. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68594/0.69768. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68579/0.69792. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68509/0.69807. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68482/0.69799. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68428/0.69795. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68422/0.69791. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68440/0.69832. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68413/0.69856. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68358/0.69853. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68357/0.69872. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68348/0.69868. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68296/0.69890. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68268/0.69881. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68257/0.69909. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68190/0.69892. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68169/0.69885. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68203/0.69866. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68247/0.69880. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68171/0.69898. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68220/0.69883. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68129/0.69900. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68095/0.69902. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68126/0.69944. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68134/0.69942. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68067/0.69930. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68018/0.69935. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68050/0.69917. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68014/0.69934. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68008/0.69991. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67929/0.70004. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68101/0.69993. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67950/0.69990. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67985/0.70034. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67909/0.70057. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67869/0.70067. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67893/0.70078. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67966/0.70085. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67746/0.70127. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67787/0.70160. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67772/0.70157. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67778/0.70167. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67808/0.70193. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67801/0.70196. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67704/0.70244. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67685/0.70246. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67563/0.70294. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67542/0.70363. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67620/0.70332. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67556/0.70367. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67801/0.70337. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67487/0.70342. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67318/0.70412. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67611/0.70447. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67540/0.70413. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67373/0.70510. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67376/0.70584. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67382/0.70538. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67384/0.70627. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67354/0.70579. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67064/0.70650. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67180/0.70746. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67246/0.70785. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67077/0.70769. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66969/0.70855. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67174/0.70943. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67127/0.70932. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66939/0.70975. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67073/0.70934. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66952/0.70883. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66679/0.71009. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66807/0.71092. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66745/0.71095. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66994/0.71201. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66885/0.71186. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66677/0.71170. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66740/0.71181. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66667/0.71315. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66540/0.71316. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66588/0.71538. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66351/0.71578. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66423/0.71557. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66722/0.71421. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66266/0.71462. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66367/0.71612. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.66554/0.71590. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69424/0.69267. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69304/0.69290. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69287/0.69306. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69253/0.69329. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69182/0.69352. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69158/0.69376. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69210/0.69394. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69129/0.69425. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69072/0.69456. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69012/0.69491. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69013/0.69538. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69037/0.69588. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69028/0.69635. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68901/0.69682. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69000/0.69727. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68865/0.69779. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68905/0.69818. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68901/0.69874. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68792/0.69938. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68783/0.70007. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68674/0.70094. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68754/0.70162. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68751/0.70213. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68622/0.70273. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68644/0.70338. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68583/0.70390. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68603/0.70463. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68517/0.70524. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68675/0.70569. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68497/0.70631. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68465/0.70681. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68456/0.70736. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68406/0.70805. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68271/0.70863. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68342/0.70899. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68189/0.70978. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68119/0.71041. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68008/0.71101. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68129/0.71151. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68122/0.71149. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67987/0.71182. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68041/0.71215. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67983/0.71300. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67944/0.71358. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67813/0.71423. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67737/0.71444. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67720/0.71529. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67684/0.71558. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67641/0.71604. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67560/0.71602. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67612/0.71639. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67401/0.71688. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67281/0.71645. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67170/0.71712. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67381/0.71750. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66906/0.71824. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67070/0.71884. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67011/0.71973. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66954/0.71951. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66779/0.71931. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66737/0.72054. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.66684/0.72075. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66449/0.72095. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66573/0.72121. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66085/0.72094. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66188/0.72194. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65834/0.72166. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65745/0.72226. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 0.65956/0.72404. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65964/0.72389. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65704/0.72462. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65537/0.72555. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65521/0.72577. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65233/0.72607. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65250/0.72557. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65292/0.72678. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65095/0.72699. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64935/0.72886. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 0.64945/0.73002. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64727/0.73139. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64707/0.73083. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64470/0.73262. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.64417/0.73103. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64093/0.73417. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 0.63802/0.73663. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64047/0.73357. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.63778/0.73610. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63831/0.73738. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63477/0.73972. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63488/0.73731. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63467/0.73907. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63293/0.73888. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63370/0.74089. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63099/0.74270. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62694/0.74384. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62506/0.74563. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62612/0.74648. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62419/0.75002. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62547/0.74709. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62294/0.74943. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69358/0.68749. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69189/0.68766. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69216/0.68780. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69183/0.68766. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69118/0.68777. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69065/0.68797. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69029/0.68781. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69041/0.68763. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69021/0.68756. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68962/0.68748. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68915/0.68730. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68899/0.68724. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68819/0.68721. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68876/0.68707. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68868/0.68710. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68852/0.68704. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68725/0.68697. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68758/0.68675. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68684/0.68676. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68692/0.68704. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68691/0.68727. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68680/0.68709. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68646/0.68705. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68609/0.68727. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68576/0.68715. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68680/0.68713. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68474/0.68762. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68522/0.68731. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68468/0.68747. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68486/0.68732. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68395/0.68733. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68246/0.68751. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68324/0.68824. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68268/0.68843. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68223/0.68809. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68152/0.68810. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68161/0.68826. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68086/0.68805. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67974/0.68905. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68119/0.68894. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67786/0.68974. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67835/0.68935. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67683/0.68899. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67745/0.68904. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67543/0.68922. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67541/0.68940. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67539/0.68832. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67464/0.68878. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67298/0.68990. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67431/0.68808. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67244/0.68832. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67099/0.68757. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67189/0.68735. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66754/0.68599. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66639/0.68636. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66697/0.68514. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66269/0.68376. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66367/0.68323. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66259/0.68083. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66025/0.68157. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65914/0.68163. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65886/0.67940. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65768/0.67904. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65947/0.68006. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65559/0.67841. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.65341/0.67794. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65108/0.67708. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65193/0.67683. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64982/0.67531. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.64965/0.67598. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64743/0.67343. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64563/0.67298. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64516/0.67548. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64452/0.67117. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.64344/0.67369. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64198/0.67449. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64271/0.67395. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64194/0.67390. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63931/0.67289. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63902/0.67331. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63500/0.67078. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63462/0.67219. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63404/0.67144. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63182/0.67219. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63606/0.67047. Took 0.22 sec\n",
      "Epoch 85, Loss(train/val) 0.63260/0.67032. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62569/0.66633. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63210/0.66665. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63012/0.67022. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63295/0.67551. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62809/0.67173. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62810/0.66845. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62408/0.67024. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62155/0.67113. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62685/0.67002. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62451/0.66747. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61914/0.66589. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.61830/0.66647. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61823/0.66330. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62241/0.66664. Took 0.18 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69325/0.69320. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69332/0.69355. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69299/0.69378. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69282/0.69405. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69228/0.69431. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69284/0.69473. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69262/0.69505. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69238/0.69532. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69239/0.69559. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69164/0.69580. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69209/0.69608. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69208/0.69612. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69025/0.69657. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69139/0.69666. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69112/0.69663. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69026/0.69676. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69156/0.69743. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68992/0.69775. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69012/0.69816. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68923/0.69835. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68867/0.69897. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68911/0.69983. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68904/0.69999. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68913/0.70015. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68831/0.70043. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68729/0.70064. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68870/0.70085. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68832/0.70074. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68701/0.70177. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68422/0.70246. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68457/0.70374. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68690/0.70416. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68449/0.70487. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68268/0.70537. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68342/0.70651. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68251/0.70747. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68039/0.70798. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68045/0.70922. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68078/0.71148. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68040/0.71194. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67812/0.71432. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67944/0.71331. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67868/0.71665. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67693/0.71782. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67763/0.71789. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67671/0.71929. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67793/0.71941. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67601/0.71787. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67588/0.71916. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67307/0.72003. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67193/0.72211. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67424/0.72145. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67275/0.72249. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67318/0.72283. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67097/0.72683. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67406/0.72788. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67233/0.72620. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67005/0.72570. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67055/0.72725. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66847/0.72838. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66638/0.72922. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66764/0.72767. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66748/0.73008. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66345/0.72889. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66665/0.72948. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66666/0.73143. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66538/0.73367. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66634/0.73389. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66371/0.73448. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66204/0.73230. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66188/0.73338. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66267/0.73223. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66421/0.73240. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66024/0.73328. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66071/0.73606. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65896/0.73522. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65884/0.73569. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65799/0.73631. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65520/0.74173. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65889/0.73801. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65489/0.74072. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65481/0.74324. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65488/0.74179. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65258/0.74202. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65175/0.74249. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65228/0.74493. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65018/0.74411. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64893/0.74041. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64883/0.74592. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64849/0.74195. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64625/0.74963. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64566/0.74827. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64194/0.74861. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64399/0.75087. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64468/0.74440. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64524/0.75011. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64141/0.74880. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64149/0.75185. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63678/0.75362. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63315/0.75230. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69388/0.69449. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69277/0.69425. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69312/0.69416. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69210/0.69424. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69196/0.69436. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69119/0.69428. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69186/0.69419. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69126/0.69429. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69145/0.69431. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69088/0.69427. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69056/0.69449. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69098/0.69446. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69016/0.69437. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68943/0.69449. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68926/0.69448. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68954/0.69439. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68795/0.69446. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68869/0.69448. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68837/0.69434. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68759/0.69408. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68745/0.69397. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68722/0.69373. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68616/0.69377. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68673/0.69403. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68601/0.69379. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68573/0.69380. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68570/0.69353. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68484/0.69325. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68509/0.69303. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68614/0.69303. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68431/0.69263. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68436/0.69228. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68293/0.69191. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68338/0.69158. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68333/0.69101. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.68317/0.69065. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68154/0.69041. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68111/0.69075. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67984/0.69062. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67973/0.69038. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67958/0.69049. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67912/0.69025. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67937/0.68973. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67679/0.68917. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67576/0.68867. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67434/0.68776. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67311/0.68704. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67320/0.68766. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67184/0.68781. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66940/0.68660. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67041/0.68481. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66784/0.68504. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66794/0.68439. Took 0.22 sec\n",
      "Epoch 53, Loss(train/val) 0.66538/0.68415. Took 0.21 sec\n",
      "Epoch 54, Loss(train/val) 0.66360/0.68378. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66000/0.68193. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66182/0.68324. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66235/0.68349. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66032/0.68411. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.65307/0.68178. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65574/0.68200. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65742/0.68143. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65306/0.68354. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65154/0.68168. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65087/0.68105. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.65386/0.68312. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64938/0.68226. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64943/0.68161. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64746/0.68210. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64279/0.68089. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64394/0.68159. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64299/0.68296. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64120/0.68273. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64369/0.68372. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.63757/0.68800. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64110/0.68352. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63538/0.68778. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.63508/0.68982. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63533/0.69194. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63844/0.68747. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63288/0.69072. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.62699/0.68975. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63106/0.69556. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.62713/0.69735. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62531/0.69647. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62511/0.70090. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62631/0.69532. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.62160/0.70377. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62247/0.69887. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.62131/0.71033. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.61611/0.70524. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.61665/0.71234. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.61695/0.70416. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.61504/0.71265. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.61083/0.71949. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.61584/0.72145. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61031/0.72431. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61210/0.72462. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60632/0.72078. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.60741/0.72926. Took 0.21 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69319/0.69540. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69301/0.69514. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69271/0.69498. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69260/0.69477. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69229/0.69475. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69151/0.69447. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69126/0.69422. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69079/0.69411. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69133/0.69403. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69183/0.69382. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69091/0.69366. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.69040/0.69335. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68982/0.69307. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68989/0.69322. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68903/0.69229. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68867/0.69250. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68807/0.69190. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68802/0.69071. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68660/0.69041. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68614/0.69024. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68571/0.68965. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68389/0.68897. Took 0.22 sec\n",
      "Epoch 22, Loss(train/val) 0.68419/0.68818. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68236/0.68705. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68100/0.68762. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68083/0.68524. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67909/0.68451. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67863/0.68342. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.67637/0.68129. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67566/0.68059. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67473/0.68024. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67462/0.67804. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67115/0.67581. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67361/0.67524. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67126/0.67626. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.66921/0.67371. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.66972/0.67332. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66676/0.67428. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.66457/0.67252. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66285/0.66949. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.66241/0.66761. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.65913/0.66772. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.65898/0.66714. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.65774/0.66829. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.65499/0.66780. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65392/0.66897. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.65018/0.66601. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65052/0.66813. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.65112/0.66428. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.64872/0.66595. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.64666/0.66461. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.64567/0.66532. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.64506/0.66365. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.64019/0.66333. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.64155/0.66419. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.64251/0.66568. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.63505/0.66547. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.63755/0.66529. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.63722/0.66378. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.63591/0.66633. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.63586/0.66887. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.63382/0.66698. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.63084/0.66597. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.62642/0.66484. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.62760/0.66553. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.62750/0.66703. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.62397/0.66567. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.62315/0.66750. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.61914/0.66831. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.62113/0.67087. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.61704/0.67331. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.62252/0.67272. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.61653/0.67316. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.61453/0.67127. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.61574/0.67272. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.61503/0.67577. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.61487/0.67599. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.60730/0.68050. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.61362/0.68066. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.60817/0.68013. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.60819/0.67949. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.60951/0.67905. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.60412/0.68045. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.60263/0.68125. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.60032/0.68065. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.60375/0.68157. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.59605/0.68133. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.59333/0.68433. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 0.60313/0.68546. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.59750/0.68176. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.59548/0.68623. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.59185/0.68528. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.59676/0.68921. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.59648/0.69029. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.59382/0.68807. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.59189/0.68999. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.58970/0.68736. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.58919/0.68944. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.58449/0.68937. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.58101/0.69202. Took 0.19 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69487/0.69171. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69469/0.69118. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69412/0.69076. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69346/0.69035. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69237/0.69007. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69234/0.68959. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69271/0.68926. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69200/0.68902. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69136/0.68902. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69108/0.68869. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69134/0.68840. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69047/0.68847. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69103/0.68843. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69101/0.68822. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69047/0.68826. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69085/0.68825. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69034/0.68808. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68973/0.68776. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68963/0.68744. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68864/0.68725. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68890/0.68699. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68984/0.68667. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68889/0.68658. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68795/0.68623. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68789/0.68592. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68867/0.68585. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68824/0.68580. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68723/0.68573. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68785/0.68533. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68697/0.68496. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68663/0.68459. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68570/0.68388. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68588/0.68380. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68593/0.68349. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68529/0.68301. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.68422/0.68271. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68229/0.68177. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68317/0.68083. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68286/0.68036. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68130/0.67988. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68172/0.67911. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.68136/0.67848. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68186/0.67795. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68058/0.67775. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68009/0.67733. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.67878/0.67670. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67636/0.67567. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67773/0.67504. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67829/0.67425. Took 0.23 sec\n",
      "Epoch 49, Loss(train/val) 0.67625/0.67446. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67487/0.67371. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67681/0.67369. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67399/0.67313. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.67319/0.67211. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67106/0.67201. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67193/0.67173. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67379/0.67045. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66910/0.66972. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66869/0.67043. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66892/0.66977. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66745/0.66945. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66855/0.66843. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66530/0.66699. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 0.66523/0.66640. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66551/0.66704. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66118/0.66538. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66281/0.66514. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66121/0.66351. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65983/0.66283. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 0.65803/0.66349. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65998/0.66298. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66147/0.66270. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65621/0.66278. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66193/0.66258. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65698/0.66026. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.65679/0.66259. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65511/0.66061. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65715/0.66078. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65348/0.65913. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65132/0.65790. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64888/0.65746. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64840/0.65836. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64744/0.65800. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.65044/0.65825. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64840/0.65794. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64570/0.65492. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64586/0.65599. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64776/0.65699. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64379/0.65724. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64173/0.65353. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63948/0.65105. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63873/0.65694. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63689/0.65346. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63713/0.65436. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63835/0.65269. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63745/0.65555. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63502/0.65268. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63485/0.65140. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63592/0.65023. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63264/0.65293. Took 0.19 sec\n",
      "ACC: 0.65625\n",
      "Epoch 0, Loss(train/val) 0.69188/0.68735. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.69036/0.68727. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69057/0.68710. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68932/0.68696. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68918/0.68712. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68860/0.68725. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68890/0.68723. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68919/0.68737. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68816/0.68746. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68723/0.68748. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68826/0.68761. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68742/0.68761. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68721/0.68778. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68704/0.68798. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68651/0.68839. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68777/0.68859. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68632/0.68883. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68783/0.68906. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68457/0.68929. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68635/0.68970. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68529/0.68976. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68589/0.68986. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68486/0.69016. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68383/0.69066. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68400/0.69090. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68427/0.69142. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68317/0.69160. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68359/0.69177. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68440/0.69149. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68208/0.69175. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68182/0.69218. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68251/0.69210. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68258/0.69261. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68195/0.69282. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68111/0.69254. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68002/0.69219. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67929/0.69251. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67962/0.69217. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67870/0.69272. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67732/0.69260. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67788/0.69292. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67701/0.69314. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67810/0.69339. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67721/0.69323. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67431/0.69355. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67720/0.69315. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67558/0.69293. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67579/0.69340. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 0.67219/0.69395. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67329/0.69440. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67260/0.69383. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67279/0.69481. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67234/0.69430. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66923/0.69387. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67082/0.69579. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67068/0.69481. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66943/0.69573. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67054/0.69510. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66977/0.69696. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66829/0.69732. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66742/0.69693. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66539/0.69659. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66544/0.69554. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66690/0.69739. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66181/0.69766. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66295/0.69691. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66078/0.69779. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66467/0.69961. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66243/0.69924. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66190/0.69961. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66081/0.70153. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65713/0.70000. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65941/0.69921. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65605/0.70093. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65599/0.70330. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65776/0.70195. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65472/0.70247. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.65546/0.70256. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65863/0.70359. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65181/0.70077. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65348/0.70307. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65181/0.70325. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64738/0.70302. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65166/0.70604. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64711/0.70597. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64781/0.70796. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64833/0.70988. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.64747/0.70731. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64787/0.70895. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64475/0.70857. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64286/0.70972. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63899/0.71034. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64034/0.71192. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64208/0.71230. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63874/0.71413. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63643/0.71676. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63663/0.71461. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.63541/0.71856. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63315/0.71565. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63323/0.71554. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69347/0.68872. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69247/0.68911. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69112/0.68970. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69133/0.69031. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69120/0.69076. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69086/0.69115. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69044/0.69154. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69005/0.69195. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69008/0.69231. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68973/0.69252. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68894/0.69290. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68976/0.69307. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68835/0.69346. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68914/0.69366. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68864/0.69394. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68940/0.69423. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68769/0.69449. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68767/0.69482. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68711/0.69501. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68741/0.69549. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68768/0.69600. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68729/0.69636. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68623/0.69668. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68577/0.69717. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68674/0.69773. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68599/0.69820. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68493/0.69873. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68436/0.69896. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68364/0.69969. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68323/0.70021. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68254/0.70080. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68186/0.70145. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68202/0.70235. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68119/0.70319. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67924/0.70382. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67868/0.70433. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68093/0.70457. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67947/0.70552. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67972/0.70541. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67936/0.70610. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67836/0.70685. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67516/0.70716. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67634/0.70810. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67446/0.70806. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67474/0.70846. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67390/0.70896. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67233/0.70982. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67226/0.71024. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67151/0.71019. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66942/0.71047. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66992/0.71111. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66808/0.71140. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66804/0.71234. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66880/0.71280. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66702/0.71377. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66874/0.71398. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66319/0.71365. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66621/0.71349. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66429/0.71262. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66061/0.71307. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66013/0.71388. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66171/0.71442. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65934/0.71476. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65875/0.71493. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65734/0.71557. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65697/0.71490. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65489/0.71455. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65546/0.71417. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65357/0.71517. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65153/0.71522. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64867/0.71520. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64568/0.71526. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65252/0.71516. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64689/0.71604. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64560/0.71773. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64559/0.71728. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64469/0.71774. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64503/0.71819. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64011/0.71915. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64023/0.71881. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63746/0.71869. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63709/0.71880. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63880/0.71891. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63660/0.72105. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63730/0.72181. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63399/0.72063. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63236/0.72042. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62983/0.71970. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62503/0.72149. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.62960/0.72246. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62533/0.72374. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62397/0.72347. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62220/0.72381. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.61878/0.72461. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62416/0.72568. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62103/0.72229. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62000/0.72264. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61971/0.72526. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.61836/0.72395. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.61645/0.72523. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69338/0.68693. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69166/0.68781. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69154/0.68870. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69104/0.68947. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69071/0.69011. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68976/0.69068. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69026/0.69119. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69017/0.69163. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69023/0.69201. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68957/0.69250. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68952/0.69290. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68978/0.69317. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68938/0.69343. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68900/0.69378. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68870/0.69415. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68822/0.69444. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68890/0.69476. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68794/0.69515. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68810/0.69531. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68775/0.69557. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68719/0.69594. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68716/0.69620. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68710/0.69643. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68760/0.69664. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68679/0.69677. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68608/0.69703. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68600/0.69747. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68645/0.69758. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68602/0.69768. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68544/0.69761. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68483/0.69793. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68514/0.69811. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68466/0.69863. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68456/0.69890. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68494/0.69890. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68415/0.69893. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68398/0.69916. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68340/0.69956. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68435/0.69978. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68329/0.70014. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68283/0.70085. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68341/0.70094. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68289/0.70091. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68106/0.70206. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68212/0.70244. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68109/0.70293. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68158/0.70286. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68144/0.70256. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68074/0.70233. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67986/0.70228. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68037/0.70193. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68066/0.70239. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68117/0.70321. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68045/0.70317. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67963/0.70311. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68095/0.70296. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67852/0.70332. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67952/0.70354. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67956/0.70334. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67849/0.70339. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67928/0.70348. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67804/0.70370. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67783/0.70376. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67837/0.70357. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67770/0.70298. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67820/0.70289. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67728/0.70325. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67722/0.70334. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67723/0.70359. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67504/0.70384. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67552/0.70413. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67696/0.70355. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67522/0.70401. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67352/0.70371. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67490/0.70347. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67439/0.70343. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67380/0.70353. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67406/0.70361. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67149/0.70368. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.67270/0.70377. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67018/0.70455. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.67153/0.70404. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67114/0.70446. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66913/0.70453. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66986/0.70356. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66886/0.70412. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66858/0.70544. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66923/0.70605. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66629/0.70571. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66933/0.70662. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66753/0.70593. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66667/0.70584. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66835/0.70528. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66635/0.70575. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66549/0.70621. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66332/0.70685. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66596/0.70804. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66453/0.70796. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66367/0.70785. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.66043/0.70768. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69291/0.68849. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69139/0.68786. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69056/0.68771. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69013/0.68757. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69005/0.68743. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68935/0.68732. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68988/0.68724. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68886/0.68732. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68877/0.68729. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68903/0.68736. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68825/0.68753. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68849/0.68760. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68749/0.68767. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68743/0.68782. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68725/0.68787. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68671/0.68797. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68673/0.68791. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68674/0.68806. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68518/0.68835. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68678/0.68839. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68550/0.68847. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68607/0.68876. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68468/0.68900. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68446/0.68912. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68419/0.68910. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68460/0.68924. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68382/0.68947. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68301/0.68944. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68375/0.68965. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68251/0.68990. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68118/0.69015. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68155/0.69027. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68058/0.68999. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68091/0.69029. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68045/0.69054. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68058/0.69072. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67848/0.69162. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67878/0.69191. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67889/0.69188. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67859/0.69215. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67601/0.69294. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67645/0.69398. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67462/0.69467. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67427/0.69564. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67626/0.69527. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67699/0.69519. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67272/0.69583. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67319/0.69700. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67176/0.69746. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67259/0.69761. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67207/0.69711. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67001/0.69878. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66653/0.69880. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66810/0.69959. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66927/0.69954. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66747/0.70001. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66714/0.69955. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66749/0.70039. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66428/0.70180. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66505/0.70237. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66481/0.70126. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66266/0.70356. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66343/0.70384. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66152/0.70458. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.65979/0.70516. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65936/0.70477. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65829/0.70602. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65887/0.70570. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65815/0.70731. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 0.65365/0.70766. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65381/0.70630. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65321/0.70818. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65388/0.70884. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65205/0.71031. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65202/0.71009. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65171/0.71115. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65088/0.71109. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64809/0.71151. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64878/0.71221. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64621/0.71327. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64843/0.71457. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64620/0.71322. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64591/0.71322. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64881/0.71231. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64174/0.71409. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.64225/0.71591. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64279/0.71505. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64018/0.71532. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63650/0.71460. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63589/0.71755. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63797/0.71826. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.63636/0.71814. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63590/0.72077. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.63400/0.72041. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.63461/0.72002. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63616/0.72101. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.63331/0.72576. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63006/0.72172. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.62768/0.72340. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.62727/0.72698. Took 0.19 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69160/0.69199. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69006/0.69131. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69019/0.69049. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69046/0.68986. Took 0.22 sec\n",
      "Epoch 4, Loss(train/val) 0.68943/0.68929. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68922/0.68885. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68873/0.68859. Took 0.22 sec\n",
      "Epoch 7, Loss(train/val) 0.68845/0.68818. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68782/0.68791. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68824/0.68767. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68815/0.68757. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68784/0.68720. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 0.68669/0.68676. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68730/0.68668. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68623/0.68664. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68600/0.68649. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68577/0.68649. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68649/0.68641. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.68587/0.68679. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68552/0.68695. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68537/0.68687. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68472/0.68720. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68398/0.68751. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68414/0.68746. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68333/0.68763. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68414/0.68789. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68434/0.68827. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68254/0.68877. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68185/0.68908. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68315/0.68929. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68122/0.68959. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68214/0.69035. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68139/0.69044. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68024/0.69093. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67934/0.69145. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68029/0.69179. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67928/0.69213. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67954/0.69250. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67869/0.69287. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67976/0.69305. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67867/0.69350. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67775/0.69380. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67833/0.69439. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67606/0.69508. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67685/0.69525. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67481/0.69602. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67557/0.69607. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67649/0.69631. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67396/0.69743. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67390/0.69750. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67478/0.69817. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67340/0.69850. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67428/0.69905. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67261/0.69890. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67249/0.69957. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67169/0.70011. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67176/0.70057. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66937/0.70102. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66953/0.70063. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66851/0.70160. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67027/0.70282. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66813/0.70239. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66850/0.70270. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66832/0.70407. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66753/0.70380. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66494/0.70512. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66467/0.70591. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66489/0.70628. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66506/0.70736. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66294/0.70711. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66219/0.70831. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65921/0.70819. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66215/0.70846. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66070/0.70880. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66022/0.71053. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65915/0.71151. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65637/0.71125. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65792/0.71109. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65879/0.71310. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65931/0.71292. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65585/0.71365. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65532/0.71404. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65540/0.71367. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65427/0.71612. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65481/0.71774. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65196/0.71910. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65211/0.71707. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65071/0.71876. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65037/0.71901. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65075/0.71954. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64674/0.72032. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64611/0.72375. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64688/0.72350. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64972/0.72386. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64765/0.72473. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64341/0.72568. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64213/0.72709. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64207/0.72787. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64444/0.72914. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64327/0.72999. Took 0.19 sec\n",
      "ACC: 0.6041666666666666\n",
      "Epoch 0, Loss(train/val) 0.69746/0.69184. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69072/0.68564. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68997/0.68373. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68960/0.68244. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68900/0.68154. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68878/0.68091. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68901/0.68018. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68887/0.67967. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68828/0.67899. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68909/0.67817. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68874/0.67785. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68829/0.67711. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68755/0.67655. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68716/0.67637. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68758/0.67603. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68732/0.67585. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68742/0.67568. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68598/0.67499. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68563/0.67434. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68604/0.67421. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68487/0.67436. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68488/0.67419. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68580/0.67450. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68515/0.67410. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68523/0.67413. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68452/0.67464. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68422/0.67488. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68418/0.67465. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68509/0.67432. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68429/0.67470. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68375/0.67479. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68207/0.67441. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68315/0.67469. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68296/0.67530. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68177/0.67535. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68069/0.67545. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68144/0.67595. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68113/0.67649. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68098/0.67696. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68116/0.67739. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68062/0.67743. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67832/0.67762. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68132/0.67777. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67888/0.67834. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67669/0.67863. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67962/0.68023. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67821/0.68037. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67685/0.68046. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67682/0.68067. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67520/0.68147. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67780/0.68220. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67547/0.68422. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67440/0.68388. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67615/0.68382. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67395/0.68477. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67397/0.68370. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67286/0.68564. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67577/0.68599. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67258/0.68685. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67155/0.68738. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67120/0.69006. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67103/0.68933. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66880/0.69015. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66959/0.69054. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66928/0.69067. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66821/0.69255. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66853/0.69275. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66796/0.69406. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66672/0.69249. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66464/0.69290. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66716/0.69239. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66653/0.69408. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66219/0.69605. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66472/0.69601. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66260/0.69632. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66162/0.69536. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66115/0.69810. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66026/0.69829. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65974/0.69763. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66032/0.69530. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65925/0.69906. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66058/0.69648. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65959/0.69651. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65970/0.70196. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65618/0.70053. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65714/0.69935. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65481/0.69915. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65437/0.70186. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65392/0.70168. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65729/0.70017. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65274/0.70270. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65123/0.70196. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65323/0.70181. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65133/0.70306. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64806/0.70171. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64684/0.70416. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65038/0.70585. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64689/0.70353. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64717/0.70509. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64841/0.70570. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69027/0.69589. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68815/0.69854. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68799/0.69998. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68679/0.70063. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68703/0.70114. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68709/0.70127. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68736/0.70141. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68668/0.70184. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68734/0.70215. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68692/0.70233. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68640/0.70281. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68718/0.70303. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68599/0.70310. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68520/0.70350. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68587/0.70430. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68496/0.70460. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68566/0.70469. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68458/0.70529. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68466/0.70520. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68400/0.70557. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68437/0.70634. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68414/0.70686. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68274/0.70712. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68172/0.70746. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68300/0.70779. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68267/0.70735. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68270/0.70821. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68212/0.70778. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68053/0.70872. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68171/0.70842. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68054/0.70840. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68018/0.70852. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68059/0.70898. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67797/0.70935. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67774/0.70926. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67715/0.70940. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67638/0.70978. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67591/0.70873. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67638/0.70975. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67521/0.70863. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67318/0.71008. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67534/0.70916. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67502/0.70977. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67108/0.70929. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67378/0.70914. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67169/0.70970. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66926/0.70928. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67045/0.70852. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66864/0.70885. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66836/0.70878. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66468/0.71070. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66573/0.71101. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66624/0.71230. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66486/0.71097. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66389/0.71256. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66442/0.71222. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66195/0.71439. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66476/0.71422. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66314/0.71512. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65951/0.71508. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65926/0.71592. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65979/0.71566. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65824/0.71597. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65711/0.71798. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65763/0.71835. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65818/0.71834. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65909/0.72059. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65752/0.71937. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65405/0.72460. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65351/0.72321. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65174/0.72352. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64975/0.72603. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65327/0.72489. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65107/0.72866. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64976/0.72773. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65035/0.72865. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.64761/0.72846. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64963/0.72996. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64727/0.73044. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64674/0.73055. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64581/0.73156. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64224/0.73376. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64447/0.73451. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64164/0.73926. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64270/0.73787. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64043/0.73914. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64110/0.73917. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63821/0.73835. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64070/0.73933. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63877/0.73871. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63473/0.74204. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63543/0.73981. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63743/0.73834. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64019/0.74014. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63208/0.74196. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63151/0.74501. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62973/0.74806. Took 0.22 sec\n",
      "Epoch 97, Loss(train/val) 0.63483/0.74420. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63280/0.74719. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62823/0.74833. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69290/0.69524. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.68713/0.69735. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68665/0.69731. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68700/0.69677. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68638/0.69690. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68563/0.69691. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68454/0.69653. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68488/0.69667. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68420/0.69670. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68375/0.69699. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68375/0.69650. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68264/0.69635. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68244/0.69669. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68391/0.69632. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68271/0.69606. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68318/0.69622. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68318/0.69613. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68220/0.69587. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68129/0.69542. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68245/0.69527. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68099/0.69533. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68036/0.69487. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68019/0.69493. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68018/0.69502. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67983/0.69510. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67963/0.69481. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67898/0.69441. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67977/0.69380. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68008/0.69403. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67957/0.69364. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.67797/0.69317. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67837/0.69360. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67682/0.69370. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67770/0.69348. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67969/0.69307. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67648/0.69291. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.67556/0.69274. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.67605/0.69264. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67757/0.69240. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67455/0.69235. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67601/0.69381. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67464/0.69457. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67573/0.69336. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67317/0.69378. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67375/0.69427. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67389/0.69481. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67341/0.69429. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67406/0.69415. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67201/0.69384. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67126/0.69397. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67149/0.69390. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67085/0.69492. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67055/0.69534. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66861/0.69541. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66862/0.69529. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66835/0.69552. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66824/0.69704. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66667/0.69752. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66796/0.69784. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66572/0.69875. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66597/0.69779. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66471/0.69928. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66434/0.69784. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66462/0.69707. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66072/0.69892. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65946/0.70045. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65948/0.70021. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65851/0.70084. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65878/0.70185. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65688/0.70362. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65566/0.70408. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65533/0.70403. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65670/0.70299. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65374/0.70476. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65031/0.70443. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65162/0.70393. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65162/0.70466. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64978/0.70777. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64680/0.70651. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64837/0.70866. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64501/0.70915. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64646/0.70932. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64438/0.70951. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64411/0.70642. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64038/0.70928. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63982/0.70870. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64010/0.71167. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63759/0.71192. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63892/0.71087. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63752/0.71259. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63918/0.71045. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64211/0.71038. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63924/0.71303. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63545/0.71350. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63458/0.71233. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63104/0.71374. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63259/0.71491. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63044/0.71534. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62603/0.71864. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62624/0.71903. Took 0.20 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69032/0.69448. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68869/0.69598. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68733/0.69651. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68790/0.69656. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68732/0.69678. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68626/0.69711. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68560/0.69743. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68592/0.69748. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68526/0.69790. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68501/0.69843. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68527/0.69867. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68522/0.69871. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68405/0.69938. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68296/0.69947. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68354/0.70005. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68348/0.70002. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68181/0.70016. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68207/0.70044. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68227/0.70085. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68256/0.70110. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68257/0.70145. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68216/0.70156. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68206/0.70156. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68066/0.70199. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68101/0.70138. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68094/0.70193. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67962/0.70225. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67901/0.70251. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67726/0.70262. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67748/0.70260. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67737/0.70221. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67629/0.70346. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67559/0.70350. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.67437/0.70281. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67461/0.70346. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67260/0.70204. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66988/0.70231. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67161/0.70210. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67036/0.70232. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66791/0.70135. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66684/0.70012. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66522/0.69854. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66222/0.69739. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66026/0.69640. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66147/0.69528. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65732/0.69332. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.65512/0.69331. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65345/0.69218. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.65429/0.69296. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65403/0.69247. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65049/0.69485. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.64800/0.69371. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.64689/0.69302. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.64526/0.69459. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64609/0.69525. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64468/0.69637. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.64212/0.69771. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.63746/0.70009. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.63881/0.70131. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.64008/0.70215. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.63947/0.70344. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.63395/0.70334. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.63480/0.70561. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.62949/0.70750. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.63068/0.70707. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.63474/0.70756. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.62860/0.70805. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.62701/0.70969. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.62426/0.71082. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.63261/0.71054. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63221/0.71100. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.62568/0.71096. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62712/0.71606. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62719/0.71807. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.61720/0.72044. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.61694/0.71653. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62078/0.71725. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.61896/0.72007. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.61714/0.71992. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.61711/0.72347. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.61389/0.72412. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.60963/0.72512. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61757/0.72368. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61696/0.72339. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.61076/0.72448. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61161/0.72780. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61224/0.73381. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61315/0.72959. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.60568/0.72847. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.60667/0.73000. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60301/0.72919. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61447/0.72967. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.59976/0.73132. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60042/0.73250. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.60286/0.73439. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60013/0.73540. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59859/0.73590. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60156/0.73455. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.59743/0.73419. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59992/0.73243. Took 0.19 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.69492/0.68840. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69282/0.68840. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69189/0.68816. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69139/0.68799. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69074/0.68798. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69126/0.68795. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69063/0.68803. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69166/0.68814. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69023/0.68819. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69015/0.68848. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68928/0.68849. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68958/0.68866. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68840/0.68879. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68871/0.68895. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68907/0.68912. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68905/0.68922. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68818/0.68933. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68755/0.68953. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68868/0.68964. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68804/0.68964. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68799/0.68956. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68796/0.68949. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68717/0.68974. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68736/0.68960. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68731/0.68964. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68705/0.68970. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68562/0.68970. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68736/0.68959. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68657/0.68959. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68592/0.68961. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68560/0.68969. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68520/0.68999. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68556/0.68998. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68576/0.68992. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68573/0.68998. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68497/0.68999. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68356/0.69022. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68293/0.69021. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68400/0.69028. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68263/0.69015. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68256/0.69021. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68326/0.69022. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68064/0.69048. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68154/0.69080. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68149/0.69038. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68047/0.69042. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.68087/0.69122. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.68093/0.69103. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67980/0.69017. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67867/0.68995. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67959/0.69009. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67961/0.68978. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67986/0.68991. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67790/0.68995. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67836/0.68939. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67781/0.68959. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67874/0.68902. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67729/0.68932. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67714/0.68987. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67615/0.68913. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67545/0.68890. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67601/0.68880. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67607/0.68927. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67362/0.68861. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67616/0.68832. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67407/0.68791. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 0.67470/0.68815. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67285/0.68809. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67484/0.68806. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67304/0.68880. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.67281/0.68863. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67042/0.68789. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.67098/0.68747. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67184/0.68701. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.67057/0.68796. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67100/0.68668. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 0.66881/0.68762. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66970/0.68752. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66846/0.68819. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66803/0.68742. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66629/0.68717. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66553/0.68725. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66757/0.68846. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66504/0.68581. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.66299/0.68667. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66610/0.68665. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.66409/0.68755. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.66589/0.68680. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66570/0.68703. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66125/0.68711. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66168/0.68685. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66224/0.68626. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.66169/0.68706. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65781/0.68673. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65931/0.68681. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65994/0.68850. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.66062/0.68879. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65797/0.68779. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65584/0.68990. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65491/0.68957. Took 0.19 sec\n",
      "ACC: 0.65625\n",
      "Epoch 0, Loss(train/val) 0.69356/0.69451. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.69270/0.69371. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69204/0.69330. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69209/0.69302. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69157/0.69262. Took 0.22 sec\n",
      "Epoch 5, Loss(train/val) 0.69086/0.69236. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68945/0.69222. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69023/0.69199. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68903/0.69171. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68994/0.69162. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68946/0.69166. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68883/0.69163. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68979/0.69172. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68808/0.69170. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68835/0.69180. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68809/0.69182. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68718/0.69196. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68794/0.69222. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68736/0.69233. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68669/0.69263. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68663/0.69293. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68545/0.69329. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68694/0.69361. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68616/0.69403. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68532/0.69425. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68557/0.69465. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68508/0.69520. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68536/0.69557. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68506/0.69587. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68446/0.69629. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68386/0.69668. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 0.68381/0.69719. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68314/0.69778. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68323/0.69830. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68261/0.69905. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68246/0.69946. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68262/0.69995. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68101/0.70026. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67994/0.70062. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68076/0.70120. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68059/0.70096. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68052/0.70172. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68003/0.70199. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67890/0.70269. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67916/0.70269. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67749/0.70305. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67730/0.70306. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67598/0.70342. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67748/0.70385. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67650/0.70355. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67601/0.70364. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67537/0.70392. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67362/0.70449. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.67506/0.70425. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67400/0.70424. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67286/0.70381. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67432/0.70382. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67153/0.70398. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67363/0.70401. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67141/0.70471. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67362/0.70464. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67104/0.70527. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67137/0.70459. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66936/0.70453. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66980/0.70436. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66957/0.70358. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66687/0.70294. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66872/0.70370. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66900/0.70410. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66586/0.70390. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66401/0.70425. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66572/0.70383. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66382/0.70323. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66179/0.70318. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66069/0.70239. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66202/0.70284. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66042/0.70271. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66022/0.70231. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66002/0.70205. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65747/0.70170. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65780/0.70155. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65782/0.70171. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65782/0.70045. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65389/0.70042. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65350/0.69983. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64959/0.69851. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65233/0.69713. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65077/0.69792. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64798/0.69566. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64752/0.69759. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64677/0.69875. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64559/0.69650. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64548/0.69667. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64520/0.69929. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64328/0.69731. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64181/0.69744. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63675/0.69688. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63899/0.69932. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63911/0.69717. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63622/0.70012. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69528/0.69388. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69283/0.69398. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69208/0.69414. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69175/0.69428. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69127/0.69450. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69061/0.69466. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69026/0.69492. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69028/0.69521. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68960/0.69554. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68935/0.69601. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68866/0.69628. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68820/0.69671. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68874/0.69719. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68775/0.69776. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68863/0.69817. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68775/0.69859. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68718/0.69905. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68635/0.69947. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68622/0.70004. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68620/0.70061. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68525/0.70119. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68532/0.70160. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68528/0.70221. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68566/0.70248. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68575/0.70284. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68464/0.70316. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68398/0.70349. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68400/0.70380. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68381/0.70445. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68280/0.70506. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68331/0.70557. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68271/0.70620. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68289/0.70646. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68175/0.70697. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68262/0.70761. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68170/0.70801. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68215/0.70835. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68105/0.70882. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68031/0.70945. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67975/0.70998. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68044/0.71062. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67948/0.71097. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67962/0.71117. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68085/0.71136. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67756/0.71167. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67896/0.71255. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67789/0.71283. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67777/0.71341. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67816/0.71319. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67805/0.71364. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67604/0.71390. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67568/0.71425. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67573/0.71503. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67644/0.71569. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67476/0.71632. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67551/0.71655. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67472/0.71710. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67511/0.71656. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67403/0.71672. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67282/0.71746. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67173/0.71807. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67272/0.71854. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67060/0.71899. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67157/0.71923. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67094/0.71984. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66917/0.72046. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66902/0.72053. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66818/0.72024. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66658/0.72125. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66696/0.72024. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66716/0.72204. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66578/0.72300. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66409/0.72404. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66511/0.72538. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66472/0.72519. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66324/0.72489. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66153/0.72535. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66138/0.72616. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66084/0.72771. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66019/0.72751. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65921/0.72770. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65641/0.72836. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65496/0.72961. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65731/0.72987. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65663/0.73089. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65693/0.72988. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65445/0.73069. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65585/0.73142. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65098/0.73215. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65222/0.73336. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64840/0.73399. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64710/0.73541. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64439/0.73767. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64496/0.73905. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64729/0.74012. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64649/0.73901. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64550/0.74072. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64354/0.74176. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64213/0.74279. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64226/0.74509. Took 0.20 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69363/0.69502. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69194/0.69514. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69239/0.69496. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69279/0.69466. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69122/0.69433. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69177/0.69396. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69108/0.69390. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69133/0.69376. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69150/0.69360. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69119/0.69328. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69092/0.69304. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69037/0.69297. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69043/0.69291. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69024/0.69249. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69009/0.69210. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68961/0.69215. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68930/0.69209. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68911/0.69177. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68833/0.69177. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68865/0.69112. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68827/0.69067. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68820/0.69092. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68728/0.69074. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68627/0.69025. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68686/0.69052. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68535/0.69037. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68555/0.69024. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68467/0.69013. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68476/0.68973. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68365/0.68999. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68196/0.69017. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68071/0.69050. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68338/0.68958. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68190/0.68983. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68065/0.69049. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68203/0.69028. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67949/0.69029. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68007/0.68989. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67947/0.69063. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67929/0.69036. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67905/0.69068. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67828/0.69126. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67621/0.69196. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67672/0.69218. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67613/0.69250. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67712/0.69359. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67243/0.69201. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67497/0.69152. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67464/0.69238. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67402/0.69294. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67311/0.69425. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67475/0.69232. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67296/0.69208. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67047/0.69255. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67133/0.69301. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67025/0.69231. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66857/0.69233. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66845/0.69265. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66921/0.69335. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66594/0.69421. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66737/0.69467. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66482/0.69526. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66366/0.69501. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66611/0.69436. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66586/0.69432. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66350/0.69562. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66393/0.69535. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66280/0.69558. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66266/0.69533. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66377/0.69589. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66122/0.69588. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66158/0.69746. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66010/0.69732. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65888/0.69933. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66089/0.69979. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65875/0.69967. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65856/0.69903. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65578/0.69932. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65230/0.70127. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65529/0.70323. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65310/0.70341. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65218/0.70398. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65082/0.70388. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65389/0.70604. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65037/0.70709. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64933/0.70419. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65125/0.70424. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64887/0.70940. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64571/0.70993. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64501/0.70777. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64476/0.71108. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.64449/0.71209. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64517/0.71159. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64303/0.71437. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63755/0.71568. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63944/0.72164. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63895/0.71684. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63593/0.72210. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63532/0.72165. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63425/0.72280. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69480/0.69278. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69287/0.69281. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69202/0.69305. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69247/0.69355. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69126/0.69401. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69175/0.69460. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69123/0.69507. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69094/0.69564. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69106/0.69600. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68996/0.69665. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69017/0.69736. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68993/0.69793. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68859/0.69857. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68926/0.69935. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68856/0.69986. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68863/0.70075. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68851/0.70123. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68732/0.70160. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68803/0.70220. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68736/0.70296. Took 0.22 sec\n",
      "Epoch 20, Loss(train/val) 0.68752/0.70407. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68713/0.70431. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68800/0.70543. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68596/0.70593. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68613/0.70646. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68546/0.70782. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68497/0.70869. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68664/0.70948. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68589/0.70996. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68467/0.71064. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68405/0.71159. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68392/0.71242. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68293/0.71330. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68250/0.71384. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68127/0.71483. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68173/0.71613. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68152/0.71733. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67993/0.71749. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68007/0.71853. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68051/0.71929. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.68083/0.71988. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67976/0.72095. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68059/0.72155. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67759/0.72196. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67822/0.72362. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67818/0.72387. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67818/0.72427. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67710/0.72560. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67781/0.72730. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67616/0.72875. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67646/0.72882. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67538/0.72856. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67404/0.73043. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67366/0.73069. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67379/0.73343. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67294/0.73519. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67227/0.73492. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67117/0.73605. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67385/0.73746. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67082/0.73747. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67155/0.73868. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67134/0.74046. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66844/0.74061. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67084/0.74240. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66946/0.74413. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66892/0.74577. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66897/0.74619. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66773/0.74741. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66637/0.74727. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66567/0.74920. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66817/0.75011. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66313/0.74973. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66545/0.75205. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66465/0.75307. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66636/0.75395. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66503/0.75478. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66681/0.75666. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66224/0.75699. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66292/0.75716. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66209/0.75903. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66014/0.75936. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66016/0.76059. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65784/0.76263. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65785/0.76331. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65393/0.76367. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65574/0.76456. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65679/0.76592. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65339/0.77022. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65535/0.77014. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65592/0.77186. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64884/0.77219. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65331/0.77629. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65226/0.77458. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65306/0.77725. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65100/0.77701. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65117/0.77882. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64937/0.78005. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64997/0.78267. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64804/0.78237. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65026/0.78389. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69392/0.69154. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69337/0.69185. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69275/0.69214. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69223/0.69226. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69178/0.69240. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69201/0.69251. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69221/0.69272. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69101/0.69288. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69122/0.69297. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69074/0.69292. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69048/0.69295. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69076/0.69314. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69019/0.69338. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68998/0.69371. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68963/0.69400. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68919/0.69462. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68930/0.69530. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68806/0.69559. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68847/0.69595. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68841/0.69636. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68747/0.69669. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68694/0.69724. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68703/0.69779. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68711/0.69864. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68741/0.69933. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68648/0.69983. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68663/0.70059. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68599/0.70127. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68587/0.70223. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68545/0.70289. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68400/0.70379. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68569/0.70448. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68425/0.70539. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68394/0.70641. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68362/0.70706. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68239/0.70807. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68363/0.70889. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68127/0.70978. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68042/0.71073. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68216/0.71157. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67894/0.71254. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68083/0.71372. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67802/0.71453. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67886/0.71569. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67843/0.71690. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67694/0.71725. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67640/0.71715. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67530/0.71776. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67473/0.71828. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67389/0.71939. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67369/0.72130. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67299/0.72265. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67009/0.72282. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66840/0.72306. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66974/0.72392. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66853/0.72434. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66794/0.72636. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66732/0.72723. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66379/0.72751. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66395/0.72798. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66248/0.72799. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65899/0.73060. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66008/0.73183. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65811/0.73220. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65436/0.73410. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65443/0.73716. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65556/0.73694. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65293/0.73969. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65201/0.73890. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64692/0.73810. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64743/0.73992. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64514/0.74100. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64558/0.74356. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64113/0.74603. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64010/0.75157. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63659/0.74860. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63739/0.75192. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.63612/0.75243. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63558/0.75656. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62951/0.76248. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62580/0.76595. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62344/0.76765. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62510/0.77147. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.62185/0.77968. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61768/0.77774. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62032/0.78265. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61739/0.77735. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61303/0.79144. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61799/0.78942. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61093/0.79264. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60827/0.80537. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61025/0.80097. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60947/0.80309. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.60540/0.81342. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60089/0.81829. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60814/0.81400. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60473/0.81309. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60189/0.81059. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59367/0.83033. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.59990/0.82111. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69308/0.68368. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69327/0.68449. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69287/0.68496. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69242/0.68553. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69342/0.68569. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69191/0.68617. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69234/0.68630. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69281/0.68633. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69168/0.68640. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69172/0.68643. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69081/0.68631. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69084/0.68655. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68997/0.68691. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69004/0.68708. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69053/0.68765. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68993/0.68776. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68980/0.68802. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68936/0.68821. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68799/0.68865. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68852/0.68922. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68848/0.68964. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68703/0.69004. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68794/0.68958. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68709/0.68983. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68737/0.69024. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68766/0.69059. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68758/0.69113. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68654/0.69102. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68502/0.69145. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68491/0.69082. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68576/0.69060. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68563/0.69120. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68474/0.69091. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68558/0.69132. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68547/0.69155. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68540/0.69221. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68412/0.69317. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68429/0.69209. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68308/0.69155. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68304/0.69250. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68383/0.69124. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68228/0.69111. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68434/0.68975. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68268/0.69016. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68113/0.69183. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68212/0.69061. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68177/0.69028. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68076/0.69009. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68017/0.69072. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67936/0.69106. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67849/0.69254. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67868/0.69215. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67836/0.69004. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67948/0.69008. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67739/0.69076. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67599/0.69039. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67639/0.68951. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67751/0.68733. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67540/0.68798. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67610/0.68766. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67530/0.68939. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67579/0.68617. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67269/0.68713. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67310/0.68645. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67332/0.68830. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67020/0.68720. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.67412/0.68556. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67165/0.68595. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66909/0.68623. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66867/0.68511. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66859/0.68314. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66651/0.68697. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66728/0.68505. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66751/0.68326. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66613/0.68196. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66641/0.68219. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66416/0.68318. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66355/0.68182. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66185/0.68110. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66229/0.67817. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65896/0.68056. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65839/0.67848. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65785/0.68427. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65448/0.68160. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65730/0.68043. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65382/0.68139. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65614/0.68003. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65559/0.68016. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65311/0.67902. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64888/0.67762. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64813/0.68053. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64939/0.67704. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64972/0.67339. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64767/0.67416. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64764/0.67145. Took 0.22 sec\n",
      "Epoch 95, Loss(train/val) 0.64327/0.67512. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64473/0.67931. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64286/0.67248. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63982/0.67061. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64230/0.67034. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69301/0.69449. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69200/0.69382. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69140/0.69322. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69051/0.69281. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68944/0.69253. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68957/0.69222. Took 0.22 sec\n",
      "Epoch 6, Loss(train/val) 0.68874/0.69216. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68843/0.69179. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68781/0.69167. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68777/0.69174. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68637/0.69157. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68540/0.69166. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68632/0.69154. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68493/0.69168. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68412/0.69213. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68417/0.69190. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68267/0.69209. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68297/0.69194. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68223/0.69170. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68257/0.69142. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68256/0.69174. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68061/0.69110. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68014/0.69036. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68206/0.69080. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67991/0.68991. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.67987/0.69003. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67880/0.68912. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67828/0.68973. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67922/0.68918. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.67682/0.68920. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67800/0.68866. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67599/0.68811. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67570/0.68755. Took 0.22 sec\n",
      "Epoch 33, Loss(train/val) 0.67469/0.68744. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67544/0.68657. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.67465/0.68662. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67256/0.68699. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67429/0.68690. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67163/0.68684. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67137/0.68725. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67321/0.68637. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.67010/0.68736. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67065/0.68683. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67111/0.68683. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67074/0.68664. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66776/0.68674. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66798/0.68712. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66802/0.68705. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66701/0.68686. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66697/0.68702. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66766/0.68731. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66634/0.68725. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66438/0.68738. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.66176/0.68756. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66177/0.68829. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66352/0.68919. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65944/0.68866. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66224/0.68917. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66238/0.68835. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66115/0.68842. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65938/0.68810. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66237/0.68787. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65957/0.68669. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65668/0.68798. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65769/0.68775. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65621/0.68771. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65358/0.68793. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65111/0.68829. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65427/0.68831. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65329/0.68950. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65366/0.68904. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64893/0.68897. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65155/0.68900. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64854/0.69024. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64792/0.68944. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64826/0.69001. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64428/0.68981. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64756/0.69092. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64266/0.69094. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64771/0.69117. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64356/0.69171. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64147/0.69024. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63972/0.69081. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64180/0.69088. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63881/0.69011. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64052/0.69071. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63910/0.69143. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63808/0.68930. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63474/0.68747. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63735/0.68732. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63360/0.68796. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63295/0.68985. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63484/0.69108. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63406/0.69104. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62829/0.68923. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62742/0.68867. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62895/0.69149. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62615/0.69053. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62501/0.69044. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62420/0.68858. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69512/0.69504. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69296/0.69709. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69183/0.69893. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69047/0.70068. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68916/0.70243. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68827/0.70393. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68744/0.70574. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68625/0.70738. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68600/0.70890. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68513/0.71012. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68500/0.71070. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68505/0.71135. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68332/0.71207. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68393/0.71251. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68359/0.71312. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68324/0.71367. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68176/0.71469. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68218/0.71519. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68223/0.71572. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68190/0.71598. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68098/0.71627. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68041/0.71668. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68106/0.71685. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68018/0.71706. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67928/0.71720. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67921/0.71783. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67903/0.71812. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67738/0.71885. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67788/0.71832. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67823/0.71871. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67718/0.71943. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67789/0.71972. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67627/0.71981. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67633/0.72008. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67575/0.72014. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67510/0.72042. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67416/0.72091. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67502/0.72137. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67474/0.72123. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67579/0.72117. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67497/0.72227. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67385/0.72279. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67360/0.72304. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67247/0.72269. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67108/0.72250. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67297/0.72344. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67199/0.72480. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67181/0.72418. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66826/0.72448. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66928/0.72558. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66849/0.72485. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66811/0.72548. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66661/0.72600. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66883/0.72538. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66955/0.72708. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66614/0.72740. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66518/0.72659. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66586/0.72745. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66537/0.72750. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66526/0.72862. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66458/0.72850. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66442/0.72852. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66379/0.72931. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65987/0.73001. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66256/0.73035. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66104/0.73290. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66190/0.73214. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66258/0.73192. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65832/0.73224. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65988/0.73191. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65765/0.73304. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65744/0.73227. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65744/0.73287. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65677/0.73361. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65670/0.73476. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65713/0.73391. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65561/0.73565. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65516/0.73569. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65682/0.73712. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65376/0.73754. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65244/0.73761. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65071/0.73846. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64985/0.73889. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65095/0.74111. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65149/0.74035. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64684/0.74124. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64803/0.74498. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64597/0.74530. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64543/0.74446. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64667/0.74577. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64528/0.74640. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64122/0.74945. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64320/0.74937. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64425/0.75146. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64007/0.75046. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63916/0.74939. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63990/0.75100. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63536/0.75321. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63482/0.75614. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63794/0.75502. Took 0.18 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69312/0.69331. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68887/0.69489. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68716/0.69586. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68612/0.69760. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68441/0.69932. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68374/0.70104. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68273/0.70266. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68149/0.70395. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68051/0.70553. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.67930/0.70649. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68014/0.70751. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68016/0.70848. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.67917/0.70929. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67935/0.70968. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.67878/0.71040. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.67749/0.71110. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67798/0.71152. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67691/0.71196. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67715/0.71277. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67696/0.71298. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67546/0.71406. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67474/0.71496. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67600/0.71547. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67521/0.71555. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67484/0.71627. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67317/0.71726. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67321/0.71734. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67304/0.71844. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67159/0.71959. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67076/0.72026. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67219/0.71960. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67132/0.72141. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66990/0.72016. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66987/0.72345. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66875/0.72289. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66824/0.72404. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66733/0.72526. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66854/0.72481. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66719/0.72624. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66729/0.72712. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66637/0.72653. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66565/0.72855. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66602/0.72777. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66374/0.73047. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66243/0.72954. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66382/0.73116. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66246/0.73016. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 0.66191/0.73122. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66029/0.73285. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66055/0.73314. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66132/0.73344. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66016/0.73462. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65950/0.73521. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65764/0.73654. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65646/0.73605. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65639/0.73657. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65541/0.73905. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65364/0.74028. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65382/0.73983. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65233/0.73988. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65413/0.74225. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65220/0.74257. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65160/0.74228. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64882/0.74339. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65120/0.74683. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64861/0.74608. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64791/0.74737. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.64444/0.74653. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64920/0.74870. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.64498/0.74743. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64501/0.75093. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64867/0.75198. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64377/0.74868. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64148/0.75088. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64589/0.75078. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64104/0.75371. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64155/0.75135. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64056/0.75183. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.63933/0.75616. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63939/0.75652. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63718/0.75560. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63152/0.75516. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63929/0.75355. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63369/0.76049. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63460/0.75994. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63068/0.76698. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.63171/0.76873. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63297/0.76781. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63012/0.76806. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62834/0.76513. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63088/0.76683. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62903/0.77167. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62966/0.77407. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.62618/0.77203. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62765/0.77848. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62577/0.77635. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62736/0.77373. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62723/0.77904. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61977/0.77927. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61952/0.77989. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69284/0.69370. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69059/0.69171. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68938/0.69106. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68878/0.69098. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68689/0.69098. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68587/0.69101. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68514/0.69138. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68340/0.69182. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68391/0.69212. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68288/0.69253. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68288/0.69289. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68210/0.69336. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68136/0.69400. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68108/0.69433. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68110/0.69481. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68063/0.69486. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68003/0.69537. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.67978/0.69593. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67890/0.69610. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67837/0.69657. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67956/0.69668. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.67808/0.69720. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67792/0.69753. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67823/0.69784. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67817/0.69779. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67759/0.69793. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67750/0.69812. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67678/0.69856. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67728/0.69883. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67714/0.69915. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67685/0.69940. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67581/0.69964. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67631/0.69985. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67670/0.69983. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67542/0.69988. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67584/0.70014. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67569/0.69971. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67399/0.69998. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67482/0.70042. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67583/0.70027. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67371/0.70034. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67492/0.70014. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67466/0.70035. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67450/0.70034. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67392/0.70046. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67208/0.70059. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67248/0.70082. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67376/0.70085. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67330/0.70095. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67217/0.70097. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67151/0.70149. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67124/0.70128. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67232/0.70117. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67203/0.70107. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67078/0.70162. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67129/0.70177. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67143/0.70157. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.66969/0.70165. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66965/0.70193. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66959/0.70158. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67002/0.70122. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66936/0.70149. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66917/0.70165. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66982/0.70213. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66967/0.70235. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66710/0.70202. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66777/0.70186. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.66866/0.70173. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66760/0.70194. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66475/0.70271. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66512/0.70289. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66570/0.70252. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66841/0.70249. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66270/0.70286. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66561/0.70324. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.66475/0.70309. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66608/0.70295. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66483/0.70284. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66439/0.70289. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66132/0.70344. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66360/0.70401. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66258/0.70457. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66139/0.70423. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66099/0.70498. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66061/0.70503. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66049/0.70539. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66104/0.70616. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66007/0.70634. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66027/0.70742. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65981/0.70682. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65822/0.70681. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65824/0.70703. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65786/0.70731. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.65485/0.70744. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65604/0.70752. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65703/0.70836. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65678/0.70916. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65420/0.70951. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65266/0.70930. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65402/0.71032. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69345/0.69286. Took 0.31 sec\n",
      "Epoch 1, Loss(train/val) 0.69377/0.69261. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69265/0.69236. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69252/0.69222. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69260/0.69209. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69223/0.69205. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69216/0.69196. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69240/0.69194. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69176/0.69204. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69104/0.69210. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69122/0.69220. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69089/0.69228. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69095/0.69245. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69133/0.69249. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69074/0.69265. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68982/0.69269. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68949/0.69318. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68901/0.69352. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68848/0.69347. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68782/0.69361. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68856/0.69397. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68714/0.69440. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68714/0.69469. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68681/0.69540. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68646/0.69562. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68542/0.69601. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68596/0.69643. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68517/0.69680. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68319/0.69735. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68366/0.69731. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68288/0.69806. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68254/0.69845. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68165/0.69910. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68217/0.70014. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67933/0.70035. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68235/0.70196. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67950/0.70271. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67746/0.70321. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67632/0.70495. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67446/0.70637. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67542/0.70726. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67639/0.70809. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67384/0.70792. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67304/0.70873. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.66826/0.71015. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67150/0.71106. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67031/0.71250. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67078/0.71261. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66766/0.71388. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66917/0.71568. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.66610/0.71537. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66563/0.71680. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66424/0.71863. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66109/0.71921. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66554/0.71965. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66410/0.72076. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66327/0.72138. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65851/0.72317. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65473/0.72628. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65895/0.72765. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 0.65707/0.72720. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65624/0.72959. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65445/0.73134. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65415/0.73162. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65574/0.73377. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65181/0.73492. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65171/0.73590. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.64937/0.73633. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64787/0.73903. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65116/0.73870. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 0.64627/0.74027. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64992/0.74041. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.64301/0.74497. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64409/0.74420. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64261/0.74624. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64472/0.74687. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64248/0.74661. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64237/0.74881. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.64142/0.75072. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63510/0.75120. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63686/0.75118. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63201/0.75341. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63249/0.75451. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63399/0.75536. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63139/0.75453. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62672/0.75825. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.62973/0.75905. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63239/0.75983. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62593/0.75949. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.62586/0.76098. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63108/0.75886. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62341/0.76446. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62329/0.76631. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62154/0.76406. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.61954/0.76878. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62148/0.76973. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61761/0.77474. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.61597/0.77493. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.61926/0.77685. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61339/0.77859. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69631/0.69195. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69389/0.69348. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69231/0.69457. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69141/0.69535. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69211/0.69594. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69082/0.69636. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69152/0.69674. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69053/0.69715. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69058/0.69732. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69035/0.69752. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68976/0.69794. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68947/0.69816. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68998/0.69831. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68930/0.69847. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68942/0.69905. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68843/0.69934. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68862/0.69962. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68865/0.69978. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68826/0.69977. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68926/0.69982. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68775/0.69996. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68700/0.70029. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68751/0.70040. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68707/0.70067. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68730/0.70106. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68780/0.70109. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68664/0.70142. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68699/0.70178. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68673/0.70204. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68677/0.70210. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68593/0.70232. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68498/0.70228. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68520/0.70223. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68466/0.70208. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68480/0.70217. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68441/0.70247. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68427/0.70229. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68449/0.70201. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68364/0.70179. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68283/0.70171. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68268/0.70215. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68226/0.70275. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68330/0.70299. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68227/0.70272. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68079/0.70222. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68081/0.70230. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68056/0.70182. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67988/0.70119. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67979/0.70105. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67887/0.70082. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67938/0.70110. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67850/0.70042. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67781/0.70046. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67753/0.70030. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67677/0.70019. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67691/0.70047. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67653/0.70077. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67496/0.70056. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67578/0.70044. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67447/0.70009. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67468/0.70012. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67449/0.69967. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67349/0.69980. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67221/0.69974. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67317/0.69922. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67054/0.69932. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67194/0.69916. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67046/0.69974. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66955/0.69913. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66894/0.69947. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66826/0.69943. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66850/0.69929. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66605/0.69993. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66452/0.70009. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66475/0.69963. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66383/0.70093. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66283/0.70190. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66403/0.70229. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66023/0.70184. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65980/0.70199. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66146/0.70234. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65929/0.70370. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65794/0.70404. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65788/0.70476. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65662/0.70640. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65604/0.70621. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65404/0.70730. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65315/0.70835. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65195/0.70833. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65233/0.70995. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64939/0.71097. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.64936/0.71089. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65059/0.71087. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64808/0.71169. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64823/0.71348. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64807/0.71258. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64420/0.71237. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64567/0.71470. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64405/0.71446. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64073/0.71514. Took 0.20 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69720/0.69513. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69382/0.69381. Took 0.22 sec\n",
      "Epoch 2, Loss(train/val) 0.69241/0.69343. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69244/0.69333. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69113/0.69350. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69081/0.69355. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69051/0.69379. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69113/0.69408. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69002/0.69446. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69050/0.69488. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69008/0.69517. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68928/0.69568. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68937/0.69605. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68902/0.69653. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68875/0.69700. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68875/0.69751. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68792/0.69795. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68846/0.69834. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68803/0.69858. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68786/0.69887. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68789/0.69924. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68747/0.69952. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68653/0.69987. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68871/0.70021. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68641/0.70052. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68632/0.70087. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68624/0.70134. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68692/0.70166. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68591/0.70198. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68522/0.70233. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68589/0.70244. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68422/0.70260. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68424/0.70261. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68452/0.70301. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68447/0.70337. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68474/0.70346. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68410/0.70394. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68278/0.70448. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68224/0.70468. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68144/0.70505. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68185/0.70550. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68172/0.70602. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.68021/0.70671. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68063/0.70703. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67984/0.70704. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68005/0.70749. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.67885/0.70798. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67985/0.70839. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 0.67792/0.70827. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67707/0.70833. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67700/0.70858. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67674/0.70917. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67434/0.70956. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67472/0.70946. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67385/0.71050. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67350/0.71135. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 0.67436/0.71199. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.67556/0.71341. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67109/0.71395. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67236/0.71533. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67266/0.71601. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67203/0.71624. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 0.66740/0.71726. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66832/0.71801. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 0.66775/0.71873. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66844/0.71919. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66823/0.71971. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66615/0.72132. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66730/0.72162. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66579/0.72238. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66449/0.72345. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 0.66343/0.72397. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66130/0.72543. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66667/0.72336. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66205/0.72502. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66440/0.72588. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65994/0.72630. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65744/0.72740. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66001/0.72733. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66029/0.72877. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65762/0.73144. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65892/0.73266. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65693/0.73163. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65883/0.73287. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65656/0.73387. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65465/0.73598. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65340/0.73689. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65085/0.73682. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65078/0.73745. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65118/0.73863. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65195/0.74103. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64857/0.74117. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65021/0.74151. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65028/0.74184. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65328/0.74243. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64773/0.74429. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64834/0.74078. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64456/0.74396. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64538/0.74526. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64480/0.74308. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69340/0.69482. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69085/0.69682. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68997/0.69659. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68960/0.69619. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68983/0.69576. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69042/0.69541. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68989/0.69514. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68920/0.69536. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68917/0.69506. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68821/0.69472. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68906/0.69422. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68837/0.69392. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68940/0.69408. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68796/0.69413. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68703/0.69422. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68783/0.69410. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68810/0.69389. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68701/0.69337. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.68710/0.69313. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68670/0.69324. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68702/0.69239. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68582/0.69290. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68630/0.69263. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68668/0.69307. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68495/0.69245. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68365/0.69278. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68407/0.69291. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68307/0.69252. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68398/0.69337. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68355/0.69280. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68219/0.69312. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68167/0.69356. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68177/0.69285. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68144/0.69328. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68049/0.69374. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68035/0.69340. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67865/0.69246. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67983/0.69171. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68062/0.69151. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67993/0.69165. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67807/0.69181. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67747/0.69059. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67921/0.68932. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67629/0.69134. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67548/0.69034. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67210/0.69088. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67267/0.68934. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67311/0.69013. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67307/0.68897. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.66913/0.68954. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67182/0.68741. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66587/0.68706. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66879/0.68688. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66876/0.68295. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66881/0.68270. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.66384/0.68358. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66866/0.68240. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65980/0.68277. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66616/0.68381. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66200/0.67961. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66331/0.67819. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66066/0.67901. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66080/0.67656. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65783/0.67856. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65728/0.67486. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.65532/0.67619. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65645/0.67522. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65546/0.67577. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65156/0.67411. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.64845/0.67047. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65245/0.67223. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64979/0.67081. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65068/0.67202. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65037/0.66848. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65019/0.66650. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64428/0.67019. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64602/0.66927. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64539/0.66959. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64723/0.66735. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64491/0.66942. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64413/0.66906. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64344/0.66872. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64077/0.66511. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64214/0.66538. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63703/0.66731. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64257/0.66631. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64072/0.66890. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63467/0.67135. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63458/0.66563. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63389/0.66418. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63240/0.66442. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63327/0.66229. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.63319/0.66488. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62747/0.66714. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63326/0.66848. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63156/0.66694. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63082/0.66819. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62831/0.66692. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63045/0.66817. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62723/0.66813. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69495/0.71254. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69089/0.70381. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68986/0.70337. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68954/0.70230. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68997/0.70115. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68850/0.70107. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68811/0.70067. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68886/0.69991. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68859/0.69930. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68906/0.69939. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68869/0.69902. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68705/0.69886. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68827/0.69854. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68876/0.69814. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68749/0.69802. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68747/0.69749. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68704/0.69719. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68759/0.69710. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68701/0.69696. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68646/0.69632. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68706/0.69572. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68611/0.69586. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68644/0.69550. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68654/0.69527. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68629/0.69484. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68616/0.69540. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68570/0.69447. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68541/0.69490. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68607/0.69454. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68500/0.69365. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68535/0.69381. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68538/0.69421. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68465/0.69442. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68383/0.69375. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68459/0.69307. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68394/0.69300. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.68496/0.69285. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68362/0.69293. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68261/0.69229. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68262/0.69310. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68285/0.69238. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68122/0.69307. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68307/0.69290. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68070/0.69252. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.68069/0.69287. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68089/0.69333. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68071/0.69305. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67920/0.69418. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67908/0.69261. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68080/0.69350. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67811/0.69299. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67642/0.69427. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67636/0.69367. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67572/0.69350. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67763/0.69491. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67554/0.69521. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67527/0.69421. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67475/0.69491. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67442/0.69512. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67086/0.69573. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67298/0.69491. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67067/0.69473. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67179/0.69412. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67066/0.69383. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66919/0.69387. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66670/0.69694. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66755/0.69504. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66708/0.69479. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66545/0.69508. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66656/0.69707. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66513/0.69754. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66479/0.69687. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66357/0.69681. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66396/0.69558. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66042/0.69771. Took 0.22 sec\n",
      "Epoch 75, Loss(train/val) 0.66201/0.69742. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66088/0.70174. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65859/0.69720. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65828/0.69631. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65881/0.69668. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65669/0.69568. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65527/0.69558. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66021/0.69725. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65406/0.69672. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65187/0.69518. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65478/0.69597. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65148/0.69427. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65125/0.69841. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64979/0.69965. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64930/0.69820. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64900/0.69898. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64853/0.70098. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64472/0.70117. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64686/0.69941. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64835/0.70177. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64334/0.70062. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64371/0.70290. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64635/0.70233. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64114/0.70208. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63936/0.70214. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.70070/0.70000. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69272/0.69884. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69111/0.69977. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69023/0.70070. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68890/0.70155. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68825/0.70240. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68747/0.70311. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68712/0.70347. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68737/0.70382. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68668/0.70415. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68590/0.70443. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68666/0.70469. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68516/0.70471. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68500/0.70439. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68400/0.70403. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68429/0.70414. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68283/0.70381. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68288/0.70331. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68184/0.70267. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68132/0.70227. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68078/0.70172. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67905/0.70078. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67969/0.70035. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67758/0.69998. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67734/0.69926. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67632/0.69864. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67641/0.69817. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67492/0.69756. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67395/0.69641. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.67312/0.69496. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67099/0.69515. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67013/0.69321. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.66785/0.69232. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.66732/0.69111. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.66765/0.68985. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66412/0.68870. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.66336/0.68841. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66004/0.68840. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.66118/0.68750. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.65872/0.68612. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.65940/0.68715. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.65783/0.68448. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.65936/0.68388. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.65486/0.68608. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.65329/0.68440. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65269/0.68346. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.65026/0.68330. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65292/0.68293. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.65057/0.68423. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.64874/0.68380. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.64553/0.68544. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.64493/0.68344. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.64675/0.68631. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64575/0.68470. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.64427/0.68437. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64307/0.68242. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.64092/0.68244. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64115/0.68336. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64031/0.67998. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.64022/0.68105. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.64173/0.68616. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.63830/0.68000. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.63687/0.68122. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63488/0.68271. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63617/0.68109. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63317/0.68247. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63338/0.67747. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.63380/0.67797. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 0.63009/0.67934. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63278/0.68205. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63215/0.68052. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62935/0.68503. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62759/0.67817. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62997/0.68377. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62831/0.68449. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62357/0.68196. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62505/0.68301. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62032/0.68212. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.62236/0.68459. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62059/0.68299. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62345/0.68157. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61828/0.68350. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61609/0.68306. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.61742/0.68442. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62025/0.68853. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61504/0.68786. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61371/0.68644. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61451/0.68530. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61371/0.68802. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61184/0.68808. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.61334/0.68552. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.60903/0.68928. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.60703/0.68788. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.60554/0.69026. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.60872/0.68795. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60589/0.69232. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.60665/0.68668. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60453/0.69076. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.60583/0.69664. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60376/0.69457. Took 0.20 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69265/0.69129. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69079/0.69196. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69096/0.69212. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69084/0.69230. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68985/0.69256. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68985/0.69282. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68948/0.69303. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68881/0.69306. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68897/0.69319. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68835/0.69348. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68741/0.69386. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68764/0.69418. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68683/0.69449. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68594/0.69459. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68448/0.69487. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68604/0.69530. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68387/0.69570. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68338/0.69618. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68195/0.69640. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68064/0.69646. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68065/0.69681. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67713/0.69761. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67706/0.69801. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67612/0.69794. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67368/0.69768. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67492/0.69762. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67258/0.69739. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67114/0.69758. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67065/0.69824. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.66760/0.69807. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.66553/0.69843. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.66448/0.69824. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66307/0.69763. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66211/0.69641. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66000/0.69621. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.65762/0.69598. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.65797/0.69706. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.65806/0.69564. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.65081/0.69595. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.65303/0.69666. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.65394/0.69698. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.65155/0.69742. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.64932/0.69755. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.64721/0.69848. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.64303/0.69992. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.64619/0.70099. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.63873/0.70304. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.64436/0.70245. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.63687/0.70233. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.63449/0.70351. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.63305/0.70495. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.63477/0.70730. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.63106/0.70681. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.63164/0.71038. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.62517/0.71329. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.62376/0.71142. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.62371/0.71339. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.62162/0.71791. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.62072/0.71877. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.61869/0.71878. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.61616/0.71985. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.61501/0.72144. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.61282/0.72280. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.61424/0.72180. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.61068/0.72711. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.60873/0.72980. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.60747/0.73513. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.60861/0.73492. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.60131/0.73610. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.60684/0.73127. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.59840/0.73302. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.59652/0.73998. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.59867/0.74234. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.59775/0.74226. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.59437/0.74392. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.59237/0.74394. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.59114/0.74395. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.58926/0.75091. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.58840/0.75049. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.58454/0.75343. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.57909/0.75437. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.58411/0.75740. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.58417/0.75303. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.57537/0.75950. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.57763/0.75954. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.57582/0.76181. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.57415/0.76283. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.57135/0.75752. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.57212/0.77127. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.57001/0.76610. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.57082/0.76737. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.56938/0.76760. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.56650/0.77066. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.56363/0.76855. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.56076/0.77124. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.56302/0.76842. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.56235/0.77199. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.55877/0.77694. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.56072/0.77540. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.55191/0.77032. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69449/0.68903. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69243/0.68772. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69274/0.68766. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69311/0.68783. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69170/0.68802. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69195/0.68837. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69131/0.68882. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69055/0.68910. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69046/0.68949. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69042/0.69006. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68996/0.69069. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68929/0.69128. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68970/0.69176. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68948/0.69223. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68861/0.69280. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68901/0.69327. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68808/0.69386. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68723/0.69446. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68719/0.69484. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68819/0.69525. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68714/0.69581. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68677/0.69627. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68674/0.69639. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68754/0.69723. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68565/0.69669. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68545/0.69724. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68556/0.69767. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68471/0.69858. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68514/0.69829. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68372/0.69869. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68383/0.69891. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68379/0.69843. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68299/0.69842. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68239/0.69919. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68281/0.69977. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68172/0.69907. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68223/0.69891. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68046/0.69934. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68132/0.69890. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67923/0.69814. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67957/0.69880. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67968/0.69830. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67820/0.69811. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67954/0.69684. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67795/0.69660. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67566/0.69613. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67521/0.69539. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67630/0.69469. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67566/0.69412. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67580/0.69400. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67416/0.69321. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67193/0.69171. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67194/0.69180. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67239/0.69086. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67204/0.68960. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66943/0.68869. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66889/0.68914. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66841/0.68785. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67107/0.68692. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66834/0.68609. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66803/0.68633. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66719/0.68616. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66505/0.68270. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 0.66399/0.68301. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66232/0.68400. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66371/0.68141. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66319/0.68134. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66229/0.68090. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65968/0.68023. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 0.65736/0.68009. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65955/0.67917. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65823/0.67863. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65963/0.67565. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65470/0.67722. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65606/0.67731. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.65376/0.67579. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65187/0.67808. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65067/0.67501. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65082/0.67386. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.65239/0.67429. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64763/0.67561. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64857/0.67501. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64408/0.67649. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64723/0.67477. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64193/0.67265. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.64514/0.67556. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64323/0.67413. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64026/0.67431. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64279/0.67517. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64065/0.67493. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63861/0.67263. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63861/0.67595. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63849/0.67330. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63463/0.67442. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63809/0.67264. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63155/0.67066. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63365/0.67273. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63100/0.67171. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62876/0.67279. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63172/0.67167. Took 0.19 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69391/0.68997. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69263/0.68925. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69202/0.68887. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69112/0.68860. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69012/0.68849. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69059/0.68832. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68946/0.68809. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68979/0.68791. Took 0.25 sec\n",
      "Epoch 8, Loss(train/val) 0.68868/0.68776. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68851/0.68749. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68872/0.68732. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68789/0.68706. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68761/0.68701. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68721/0.68720. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68585/0.68722. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68624/0.68700. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68509/0.68698. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68558/0.68698. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68473/0.68676. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68198/0.68679. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68341/0.68647. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68233/0.68625. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 0.68205/0.68595. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68240/0.68600. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68104/0.68589. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68154/0.68589. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67899/0.68570. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67852/0.68620. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67981/0.68612. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67830/0.68594. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67694/0.68598. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67646/0.68644. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67619/0.68680. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67634/0.68711. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67427/0.68742. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67450/0.68898. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67385/0.68896. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67386/0.68908. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67428/0.68921. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67314/0.68967. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67284/0.69028. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67200/0.69030. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67325/0.69065. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67163/0.69067. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67024/0.69162. Took 0.22 sec\n",
      "Epoch 45, Loss(train/val) 0.67187/0.69094. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67105/0.69188. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66899/0.69277. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66825/0.69319. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66773/0.69365. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66879/0.69393. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66911/0.69533. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66485/0.69503. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66613/0.69604. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66668/0.69736. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66503/0.69723. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66672/0.69824. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66363/0.69763. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66450/0.69764. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66451/0.69987. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66482/0.70123. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.66250/0.70218. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66330/0.70216. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66434/0.70342. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65980/0.70322. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66126/0.70419. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66035/0.70500. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66285/0.70527. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65861/0.70506. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66320/0.70486. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65909/0.70535. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 0.65718/0.70666. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65684/0.70810. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65709/0.70843. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65686/0.70890. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65719/0.70829. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65341/0.70915. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.65258/0.70975. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65427/0.71138. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65602/0.71117. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65353/0.71191. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65655/0.71202. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65294/0.71211. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.65494/0.71280. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65231/0.71377. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65350/0.71461. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65217/0.71410. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65018/0.71571. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64927/0.71666. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64774/0.71699. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64919/0.71728. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64497/0.71954. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64629/0.72007. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.64819/0.71972. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64489/0.72181. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64606/0.72079. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64534/0.72179. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64501/0.72244. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64491/0.72655. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64430/0.72559. Took 0.19 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69437/0.68405. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69362/0.68333. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69247/0.68352. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69199/0.68388. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69148/0.68451. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69118/0.68521. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69013/0.68537. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68944/0.68613. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68970/0.68720. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68848/0.68796. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68804/0.68832. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68716/0.68918. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68718/0.69007. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68702/0.69039. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68513/0.69109. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68393/0.69158. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68510/0.69271. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68292/0.69318. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68312/0.69431. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68241/0.69519. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68162/0.69440. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68155/0.69536. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67976/0.69648. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67991/0.69680. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67796/0.69669. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67912/0.69735. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67780/0.69786. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67667/0.69915. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67461/0.69974. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67449/0.70078. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67462/0.69973. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67465/0.70203. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67462/0.70205. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67395/0.70207. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67294/0.70279. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67147/0.70308. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67295/0.70384. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67356/0.70391. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67323/0.70608. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67117/0.70602. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67175/0.70672. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67075/0.70633. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66976/0.70798. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67050/0.70863. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66923/0.70785. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66950/0.71018. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66778/0.71119. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66658/0.71075. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66642/0.71084. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66746/0.71087. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66457/0.71221. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66635/0.71337. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66380/0.71280. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66357/0.71322. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66151/0.71374. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66316/0.71419. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66358/0.71432. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66322/0.71539. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66249/0.71575. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66227/0.71566. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66138/0.71502. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65952/0.71617. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66002/0.71655. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65669/0.71600. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65972/0.71597. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65690/0.71501. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66057/0.71701. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65537/0.71531. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65463/0.71708. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65504/0.71752. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65712/0.72023. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65287/0.71958. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65304/0.71717. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65524/0.71909. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65275/0.71880. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64912/0.72046. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64925/0.71891. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.65101/0.71945. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65081/0.71967. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65022/0.72148. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65091/0.72160. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65025/0.72082. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64728/0.72002. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64820/0.71920. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64779/0.72178. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64863/0.72040. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64693/0.71967. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64709/0.72067. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64410/0.71954. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64474/0.71984. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64239/0.71926. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64274/0.72114. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64249/0.71939. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64415/0.71998. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64205/0.72093. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63931/0.72196. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64117/0.72070. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64062/0.72036. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64011/0.72123. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63843/0.72013. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69090/0.69331. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.68756/0.69570. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68742/0.69605. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68637/0.69586. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68618/0.69564. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68602/0.69539. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68552/0.69504. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68581/0.69454. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68466/0.69460. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68427/0.69440. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68525/0.69420. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68425/0.69383. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68407/0.69355. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68421/0.69337. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68249/0.69329. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68259/0.69312. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68204/0.69322. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68207/0.69295. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68139/0.69312. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68169/0.69276. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68094/0.69311. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68076/0.69232. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67939/0.69251. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67948/0.69246. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68059/0.69167. Took 0.23 sec\n",
      "Epoch 25, Loss(train/val) 0.67846/0.69158. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67762/0.69149. Took 0.22 sec\n",
      "Epoch 27, Loss(train/val) 0.67757/0.69135. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67732/0.69054. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67583/0.69068. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67575/0.68996. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67550/0.69021. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67322/0.69046. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.67528/0.68916. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67387/0.69038. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67420/0.68931. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67312/0.68976. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67197/0.68886. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67382/0.68856. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67150/0.68774. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66920/0.68754. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67106/0.68772. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67088/0.68796. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66874/0.68795. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66862/0.68797. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66714/0.68837. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66686/0.68871. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66728/0.68824. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66564/0.68764. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66383/0.68687. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66429/0.68644. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.66308/0.68620. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66121/0.68644. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66005/0.68614. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65932/0.68650. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65902/0.68599. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65879/0.68663. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65603/0.68476. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65602/0.68493. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65351/0.68417. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65516/0.68349. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65293/0.68407. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65253/0.68549. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64853/0.68475. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64664/0.68466. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64642/0.68452. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64707/0.68544. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.64556/0.68551. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64284/0.68496. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63744/0.68610. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63923/0.68539. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63732/0.68603. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64090/0.68551. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63907/0.68637. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63698/0.68761. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.63204/0.68900. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63648/0.68975. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63313/0.69189. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63212/0.69106. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62952/0.69286. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62860/0.69363. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62773/0.69374. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62589/0.69511. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.62794/0.69616. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62816/0.69525. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62447/0.69685. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62320/0.69636. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62263/0.69538. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62278/0.69631. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61931/0.69773. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62113/0.69336. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62054/0.69701. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61764/0.69844. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61885/0.69962. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61472/0.70157. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61594/0.70423. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61752/0.70245. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61327/0.70362. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61480/0.70493. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.61337/0.70384. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69217/0.68838. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69036/0.68584. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68913/0.68482. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68875/0.68433. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68910/0.68392. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68932/0.68378. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68883/0.68362. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68885/0.68325. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68835/0.68304. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68821/0.68275. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68811/0.68269. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68861/0.68252. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68640/0.68230. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68790/0.68217. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68731/0.68207. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68718/0.68193. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68670/0.68161. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68746/0.68147. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68661/0.68117. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68665/0.68108. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68559/0.68076. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68546/0.68054. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68508/0.68050. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68528/0.68036. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68474/0.68014. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68456/0.68018. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68347/0.67969. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68303/0.67942. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68322/0.67944. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68365/0.67933. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68241/0.67913. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68264/0.67880. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68170/0.67893. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68245/0.67841. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68145/0.67858. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68060/0.67796. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68092/0.67796. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68068/0.67796. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68023/0.67800. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67826/0.67806. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67978/0.67804. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67756/0.67804. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67727/0.67884. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67952/0.67823. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67752/0.67820. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67698/0.67843. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67635/0.67845. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67552/0.67833. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67548/0.67833. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67372/0.67854. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67509/0.67908. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67511/0.67925. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67366/0.67979. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67390/0.68035. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67243/0.68087. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67237/0.68077. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67122/0.68095. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67066/0.68087. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66968/0.68151. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66985/0.68198. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67119/0.68215. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66865/0.68295. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66860/0.68322. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66670/0.68417. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66689/0.68413. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66546/0.68511. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66225/0.68506. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66469/0.68612. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66198/0.68673. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66344/0.68683. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66139/0.68645. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66256/0.68724. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66230/0.68811. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.66035/0.68841. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65930/0.68865. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65677/0.69027. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65695/0.69133. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65886/0.69220. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65617/0.69235. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65529/0.69360. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65426/0.69408. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.65250/0.69446. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65424/0.69544. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65086/0.69597. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65052/0.69643. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64760/0.69859. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64570/0.69867. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64780/0.69890. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64520/0.69932. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64267/0.69927. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64530/0.70097. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.63987/0.70264. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63807/0.70442. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63995/0.70596. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.63575/0.70795. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63733/0.70794. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63499/0.71087. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63582/0.71364. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63346/0.71442. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63025/0.71729. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69083/0.68704. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68794/0.68606. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68707/0.68586. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68626/0.68610. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68633/0.68644. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68591/0.68684. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68549/0.68725. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68565/0.68766. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68525/0.68799. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68438/0.68833. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68419/0.68870. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68316/0.68906. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68353/0.68940. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68291/0.68968. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68154/0.69008. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68145/0.69028. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68073/0.69028. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68062/0.69017. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68086/0.69013. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68005/0.69006. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67883/0.68956. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67590/0.68905. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67852/0.68849. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67485/0.68781. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67636/0.68706. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67303/0.68638. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67193/0.68561. Took 0.22 sec\n",
      "Epoch 27, Loss(train/val) 0.67128/0.68482. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.66946/0.68337. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.66860/0.68309. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.66879/0.68212. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.66721/0.68092. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.66468/0.67971. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.66375/0.67909. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.66245/0.67898. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66193/0.67770. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.66139/0.67671. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.65819/0.67722. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.65775/0.67581. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.65648/0.67665. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.65622/0.67673. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.65773/0.67619. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.65683/0.67587. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.65747/0.67461. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.65467/0.67476. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.65414/0.67563. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65067/0.67506. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65345/0.67465. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.64768/0.67588. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.64823/0.67500. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.64564/0.67421. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.65061/0.67424. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.64682/0.67366. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64595/0.67284. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64308/0.67243. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.64170/0.67156. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64003/0.67230. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.63971/0.67203. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.63633/0.67146. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.63872/0.67097. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.63606/0.67388. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.63774/0.67353. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63329/0.67321. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63518/0.67270. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63060/0.67342. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63352/0.67391. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.62956/0.67465. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63085/0.67427. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63305/0.67447. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.63197/0.67428. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.62714/0.67365. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62672/0.67308. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62300/0.67215. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62647/0.67213. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62316/0.67201. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.61819/0.67310. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62091/0.67387. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.61759/0.67271. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62007/0.67266. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61660/0.67401. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61571/0.67494. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61880/0.67341. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61682/0.67226. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.61340/0.67289. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.60936/0.67295. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.61275/0.67512. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.60954/0.67513. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.60738/0.67501. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60883/0.67437. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60684/0.67499. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60512/0.67610. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.60106/0.67420. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.59986/0.67611. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.59982/0.67558. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60119/0.67480. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.59862/0.67611. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.59700/0.67631. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.59659/0.67793. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59674/0.67864. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.59056/0.67781. Took 0.18 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.68840/0.67980. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68657/0.67873. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68541/0.67882. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68577/0.67885. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68505/0.67896. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68458/0.67932. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68441/0.67954. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68384/0.67999. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68391/0.68050. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68292/0.68062. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68341/0.68101. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68203/0.68145. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68212/0.68208. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68174/0.68219. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68194/0.68269. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68100/0.68357. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67966/0.68443. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.67980/0.68524. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67927/0.68600. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67860/0.68688. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67781/0.68789. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67745/0.68874. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67642/0.68994. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67551/0.69085. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67555/0.69180. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.67522/0.69246. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67314/0.69369. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67130/0.69437. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67110/0.69506. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67115/0.69637. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.66929/0.69749. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.66870/0.69868. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66692/0.69966. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.66563/0.70099. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66390/0.70159. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66181/0.70252. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66093/0.70375. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66084/0.70422. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.65875/0.70434. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.65910/0.70508. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.65752/0.70520. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.65491/0.70739. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.65533/0.70897. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.65346/0.71015. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65245/0.71105. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65461/0.71045. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65193/0.71171. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65095/0.71159. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.64919/0.71427. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.64909/0.71480. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.64730/0.71629. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.64471/0.71702. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.64594/0.71842. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64349/0.71879. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64529/0.71963. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64073/0.72340. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64299/0.72300. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.63870/0.72572. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64052/0.72495. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.63915/0.72812. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64092/0.72815. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.63488/0.73075. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63675/0.72802. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63340/0.73379. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63539/0.73305. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.63058/0.73392. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63377/0.73729. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.63175/0.73731. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.62901/0.74030. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.62480/0.73952. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63083/0.73974. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62869/0.73896. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62709/0.74110. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62584/0.74261. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62457/0.74400. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.62555/0.75002. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62463/0.74408. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62128/0.74945. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62068/0.74874. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61911/0.75005. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61540/0.75267. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61527/0.75118. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61748/0.75432. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.61406/0.75609. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61411/0.75596. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61239/0.75809. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61316/0.76050. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61175/0.75868. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60858/0.75919. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60710/0.76328. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61256/0.76594. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.60547/0.76714. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60634/0.76686. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60683/0.76202. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.59732/0.76858. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60311/0.76541. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60277/0.76862. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.59791/0.77076. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59522/0.77328. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.59341/0.77354. Took 0.18 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69108/0.68121. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68421/0.67976. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68387/0.67955. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68277/0.67966. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68255/0.67960. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68167/0.67987. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68172/0.67990. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68132/0.68009. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68116/0.68035. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68098/0.68066. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68131/0.68093. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68071/0.68132. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.67973/0.68156. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67947/0.68203. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.67981/0.68233. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.67929/0.68281. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67836/0.68310. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.67842/0.68340. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67912/0.68385. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67849/0.68410. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67858/0.68431. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67829/0.68457. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67851/0.68509. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67679/0.68532. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67688/0.68575. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67741/0.68581. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67734/0.68629. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67634/0.68657. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67662/0.68689. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67644/0.68752. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67509/0.68797. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67487/0.68864. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67459/0.68877. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67503/0.68940. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67436/0.68947. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67505/0.68999. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67377/0.68989. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67354/0.69045. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67379/0.69095. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67440/0.69176. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67439/0.69199. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67262/0.69277. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67153/0.69303. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67371/0.69334. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67130/0.69338. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67030/0.69433. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67223/0.69444. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67164/0.69557. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67035/0.69633. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67000/0.69735. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67037/0.69710. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66991/0.69818. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66851/0.69874. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66914/0.69898. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66780/0.69976. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66877/0.70038. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66671/0.70131. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.66734/0.70162. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66665/0.70337. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66519/0.70411. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66573/0.70480. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66555/0.70577. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66249/0.70753. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66388/0.70795. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66186/0.70855. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66303/0.70882. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66134/0.71007. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66356/0.71141. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66190/0.71221. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66182/0.71376. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66209/0.71515. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66167/0.71554. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65910/0.71763. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65938/0.72006. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65827/0.72048. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65959/0.72039. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65815/0.72157. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65874/0.72192. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65775/0.72183. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65529/0.72297. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65598/0.72432. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65602/0.72569. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65339/0.72706. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65338/0.72746. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65296/0.72932. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65117/0.73090. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65231/0.73117. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.64912/0.73205. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65063/0.73255. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65017/0.73341. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64950/0.73510. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64873/0.73470. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65097/0.73765. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64672/0.73725. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64513/0.73864. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64796/0.74018. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64608/0.73925. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.64530/0.73830. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64571/0.73890. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64638/0.74188. Took 0.19 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.68388/0.69105. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68326/0.69097. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68298/0.69079. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68203/0.69068. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.68207/0.69045. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68195/0.69029. Took 0.22 sec\n",
      "Epoch 6, Loss(train/val) 0.68155/0.69002. Took 0.22 sec\n",
      "Epoch 7, Loss(train/val) 0.68103/0.68977. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68118/0.68953. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68052/0.68944. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68080/0.68910. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.67998/0.68883. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.67960/0.68861. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.67971/0.68837. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.67981/0.68826. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.67916/0.68814. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.67836/0.68800. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.67889/0.68764. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.67810/0.68732. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.67687/0.68708. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.67785/0.68693. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67824/0.68673. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67690/0.68662. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.67712/0.68672. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67564/0.68690. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67577/0.68698. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67510/0.68695. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67520/0.68687. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67352/0.68667. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67412/0.68630. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67375/0.68616. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 0.67226/0.68601. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67293/0.68588. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67375/0.68591. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67273/0.68613. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67243/0.68591. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67164/0.68579. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67199/0.68563. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67080/0.68582. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67035/0.68596. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67070/0.68607. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67081/0.68633. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66915/0.68641. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67012/0.68675. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66912/0.68690. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66696/0.68739. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66873/0.68736. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66706/0.68710. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66681/0.68701. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66561/0.68736. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66524/0.68834. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66587/0.68843. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66418/0.68907. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66447/0.68894. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66374/0.68891. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66496/0.68926. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66393/0.68936. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66272/0.68973. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66260/0.69005. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66246/0.68986. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66096/0.69018. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66052/0.69050. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66015/0.69057. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65969/0.69099. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65861/0.69090. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65968/0.69144. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65740/0.69111. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65760/0.69200. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65858/0.69121. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65650/0.69217. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65537/0.69344. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65644/0.69281. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65596/0.69332. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65572/0.69350. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65463/0.69454. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65444/0.69398. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65353/0.69514. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65271/0.69435. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65288/0.69573. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65192/0.69651. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64970/0.69821. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65155/0.69699. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64949/0.69588. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64942/0.69540. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64721/0.69643. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64894/0.69713. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64689/0.69855. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64565/0.69996. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64716/0.69827. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64383/0.70192. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64397/0.70110. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64397/0.70123. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64178/0.70098. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64227/0.70159. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64269/0.70313. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64409/0.70227. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64158/0.70183. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64084/0.70265. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63956/0.70467. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63884/0.70488. Took 0.18 sec\n",
      "ACC: 0.6145833333333334\n",
      "Epoch 0, Loss(train/val) 0.68722/0.68506. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68642/0.68392. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68605/0.68333. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.68488/0.68307. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68459/0.68298. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68447/0.68292. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68476/0.68294. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68461/0.68294. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68411/0.68295. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68333/0.68305. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68361/0.68307. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68314/0.68310. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68357/0.68326. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68411/0.68346. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68247/0.68362. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68281/0.68384. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68253/0.68397. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68266/0.68413. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68071/0.68428. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68168/0.68443. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68161/0.68463. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68047/0.68476. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68102/0.68518. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68081/0.68534. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68137/0.68556. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68034/0.68566. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68006/0.68592. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67942/0.68628. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67906/0.68650. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67894/0.68703. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67939/0.68750. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67808/0.68784. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67864/0.68805. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67683/0.68837. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67795/0.68860. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67713/0.68886. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67760/0.68947. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67698/0.68988. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67702/0.69002. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67558/0.69040. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67611/0.69118. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67584/0.69142. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67529/0.69122. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67427/0.69188. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67501/0.69267. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67583/0.69370. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67270/0.69463. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67468/0.69555. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67336/0.69573. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67304/0.69585. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67353/0.69617. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67268/0.69631. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67171/0.69638. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67243/0.69649. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67248/0.69705. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67138/0.69779. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66951/0.69811. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67047/0.69828. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67102/0.69837. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67035/0.69861. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66817/0.69930. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66952/0.69961. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66840/0.69936. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66853/0.69966. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66766/0.70039. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66669/0.70025. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66708/0.70050. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66674/0.70123. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66429/0.70202. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66629/0.70210. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66409/0.70287. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66239/0.70252. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66311/0.70268. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66375/0.70288. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65977/0.70287. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65815/0.70316. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65891/0.70320. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66118/0.70323. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65773/0.70393. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66107/0.70354. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66093/0.70245. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65845/0.70325. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65831/0.70245. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65528/0.70385. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65565/0.70431. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65469/0.70570. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65260/0.70366. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65183/0.70282. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65113/0.70330. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65108/0.70480. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65049/0.70458. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64767/0.70385. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64614/0.70538. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65304/0.70547. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64634/0.70478. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64613/0.70599. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64566/0.70476. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64640/0.70457. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64638/0.70534. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64147/0.70576. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.68885/0.69643. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68283/0.70007. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68105/0.70208. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68086/0.70294. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.67997/0.70319. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.67994/0.70309. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.67967/0.70212. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.67966/0.70179. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.67910/0.70142. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.67841/0.70171. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.67709/0.70197. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.67737/0.70161. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.67711/0.70123. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.67862/0.70064. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.67681/0.70114. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.67687/0.70187. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67542/0.70198. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67587/0.70213. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67736/0.70194. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67533/0.70185. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67592/0.70242. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67612/0.70275. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67587/0.70259. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67610/0.70281. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67629/0.70287. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67537/0.70278. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67534/0.70250. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67527/0.70295. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67419/0.70390. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67367/0.70402. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67382/0.70390. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 0.67393/0.70415. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67556/0.70401. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67350/0.70482. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67294/0.70447. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67244/0.70489. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67096/0.70503. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67212/0.70472. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67296/0.70569. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67111/0.70502. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67148/0.70524. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67001/0.70594. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67178/0.70594. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67148/0.70653. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67202/0.70690. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66957/0.70743. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66916/0.70735. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66807/0.70801. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66944/0.70905. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66699/0.71013. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66702/0.70903. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66761/0.70946. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66933/0.71077. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66751/0.71037. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66636/0.71017. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66544/0.71202. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66557/0.71167. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66600/0.71142. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66349/0.71051. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66180/0.71303. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66352/0.71134. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66346/0.71306. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66186/0.71397. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65892/0.71470. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66138/0.71556. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65848/0.71307. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66095/0.71661. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65801/0.71487. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65780/0.71906. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65716/0.71457. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65971/0.71888. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65797/0.71826. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65861/0.71669. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65520/0.72095. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65433/0.71800. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65481/0.71915. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65520/0.72020. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65459/0.72016. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64971/0.72158. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65243/0.71909. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65160/0.72404. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65288/0.72502. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65195/0.72144. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65021/0.72276. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64966/0.72444. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64743/0.72591. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64488/0.72361. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64638/0.72393. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64859/0.72061. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64749/0.72196. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64385/0.72031. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64579/0.72353. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64369/0.72240. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64275/0.72628. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64025/0.72919. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64049/0.72818. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64194/0.72598. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63527/0.72664. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64164/0.72859. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63906/0.72680. Took 0.20 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.68679/0.69601. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68445/0.69577. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68527/0.69447. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.68337/0.69286. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68293/0.69116. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68264/0.68986. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68192/0.68870. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68263/0.68737. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68147/0.68645. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68111/0.68570. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68166/0.68447. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68027/0.68412. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68024/0.68386. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68030/0.68354. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68062/0.68301. Took 0.22 sec\n",
      "Epoch 15, Loss(train/val) 0.67999/0.68347. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.67974/0.68296. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.67950/0.68286. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.67953/0.68302. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.67875/0.68263. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.67980/0.68229. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67919/0.68289. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 0.67803/0.68246. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67875/0.68219. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.67828/0.68226. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67870/0.68286. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67752/0.68195. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67776/0.68253. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67712/0.68287. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67773/0.68334. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67796/0.68356. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67769/0.68350. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67801/0.68456. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67830/0.68474. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67748/0.68461. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.67740/0.68441. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67598/0.68471. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67601/0.68492. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67746/0.68485. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67590/0.68589. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67643/0.68587. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67585/0.68605. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67561/0.68583. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67584/0.68631. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67619/0.68717. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67521/0.68671. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67521/0.68694. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67435/0.68587. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67560/0.68545. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67557/0.68616. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67357/0.68642. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67400/0.68703. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67421/0.68894. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67451/0.68714. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67333/0.68782. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67328/0.68910. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67268/0.68914. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67274/0.68993. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67290/0.68995. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67291/0.68936. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67148/0.68932. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67240/0.68944. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67003/0.69162. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67165/0.69198. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67211/0.69083. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67314/0.69091. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66975/0.69214. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66893/0.69324. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66937/0.69470. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66781/0.69489. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66991/0.69572. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66914/0.69457. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66879/0.69463. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66787/0.69594. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66966/0.69627. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66743/0.69692. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66760/0.69772. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66723/0.69699. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66659/0.69809. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66559/0.69914. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66538/0.69910. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66043/0.70090. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66481/0.70189. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66386/0.70255. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66288/0.70013. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66360/0.70249. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66478/0.70473. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66354/0.70330. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66241/0.70502. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65814/0.70763. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66069/0.70768. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66005/0.70886. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66195/0.70891. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66054/0.71200. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65954/0.71127. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65962/0.71417. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65938/0.71635. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65671/0.71152. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65744/0.71351. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.65704/0.71614. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.68926/0.68753. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68653/0.68828. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68567/0.68885. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68529/0.68933. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68451/0.68994. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68439/0.69062. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68377/0.69113. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68424/0.69178. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68386/0.69249. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68400/0.69290. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68391/0.69325. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68267/0.69355. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68339/0.69388. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68318/0.69424. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68254/0.69462. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68256/0.69517. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68273/0.69558. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68288/0.69587. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68198/0.69606. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68189/0.69661. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68208/0.69697. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68257/0.69725. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68113/0.69712. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68188/0.69708. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68241/0.69729. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68225/0.69758. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68208/0.69752. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68199/0.69781. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68238/0.69775. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68124/0.69782. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68112/0.69804. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68134/0.69853. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68063/0.69862. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68184/0.69866. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68003/0.69878. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68151/0.69872. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68147/0.69863. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68082/0.69897. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68095/0.69886. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67940/0.69923. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68068/0.69959. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67976/0.69963. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67937/0.69977. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67779/0.70036. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67976/0.70053. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67895/0.70064. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67893/0.70105. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67791/0.70149. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67809/0.70207. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67806/0.70288. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67876/0.70383. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67816/0.70416. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67879/0.70383. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67829/0.70415. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67840/0.70382. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67857/0.70450. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67755/0.70524. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67723/0.70527. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67711/0.70509. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67642/0.70474. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67487/0.70555. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67598/0.70629. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67450/0.70628. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67690/0.70664. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67418/0.70652. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67473/0.70759. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67413/0.70766. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67359/0.70822. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67427/0.70892. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67378/0.70872. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67611/0.70917. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67229/0.70914. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67168/0.70944. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67059/0.71034. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67226/0.70999. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67186/0.71101. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67194/0.71153. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66945/0.71159. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67141/0.71171. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66878/0.71278. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66806/0.71449. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66892/0.71545. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66902/0.71466. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66947/0.71427. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66797/0.71416. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66860/0.71437. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66592/0.71446. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66663/0.71460. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66737/0.71472. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66543/0.71448. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66462/0.71593. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66268/0.71699. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66536/0.71663. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66511/0.71840. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66300/0.71791. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.66291/0.71699. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66169/0.71744. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66101/0.71843. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65919/0.71798. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.66204/0.71820. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69595/0.69535. Took 0.35 sec\n",
      "Epoch 1, Loss(train/val) 0.69274/0.69665. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69071/0.69691. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68997/0.69715. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68986/0.69751. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68874/0.69760. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68780/0.69837. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68717/0.69861. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68720/0.69936. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68728/0.70006. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68612/0.70081. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68701/0.70158. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68627/0.70218. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68606/0.70286. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68396/0.70414. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68358/0.70502. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68435/0.70546. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68417/0.70574. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68466/0.70649. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68370/0.70710. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68335/0.70744. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68238/0.70786. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68283/0.70847. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68288/0.70874. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68268/0.70893. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68222/0.70922. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68272/0.70929. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68102/0.70977. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68198/0.71000. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68119/0.71046. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68108/0.71077. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68037/0.71123. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68028/0.71159. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68121/0.71114. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67828/0.71239. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67985/0.71227. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67926/0.71253. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67839/0.71271. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67742/0.71319. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67831/0.71360. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67750/0.71428. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67641/0.71443. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67844/0.71448. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67626/0.71529. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67585/0.71556. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67503/0.71604. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67522/0.71638. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67658/0.71743. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67531/0.71847. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67452/0.71835. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67448/0.71884. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67450/0.71900. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67305/0.71930. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67298/0.71913. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67217/0.71997. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.67058/0.72077. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67273/0.72035. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67193/0.72043. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66969/0.72155. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67019/0.72199. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67017/0.72126. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66895/0.72178. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66873/0.72242. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66963/0.72203. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66843/0.72326. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66638/0.72280. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66674/0.72395. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66901/0.72407. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66666/0.72353. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66420/0.72456. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66502/0.72472. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66389/0.72528. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66232/0.72598. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66116/0.72527. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66051/0.72549. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66362/0.72660. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66258/0.72696. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.66167/0.72731. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65888/0.72733. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65730/0.72723. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65972/0.72889. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65661/0.72975. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65642/0.72855. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65488/0.72924. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65431/0.73001. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65398/0.72986. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65549/0.72893. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65376/0.73015. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65254/0.72951. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65042/0.73055. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65121/0.72937. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65175/0.72914. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65196/0.72847. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64923/0.72973. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65164/0.72966. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 0.64730/0.73019. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64862/0.73045. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64600/0.73175. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64684/0.73066. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64345/0.72863. Took 0.20 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69158/0.69296. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.68844/0.69353. Took 0.22 sec\n",
      "Epoch 2, Loss(train/val) 0.68886/0.69400. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68900/0.69415. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.68843/0.69451. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68715/0.69495. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68730/0.69548. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68699/0.69581. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68631/0.69625. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68645/0.69701. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68579/0.69753. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68397/0.69809. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68356/0.69884. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68426/0.69937. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.68392/0.70008. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68351/0.70042. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68201/0.70121. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68177/0.70173. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68121/0.70223. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68022/0.70285. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.68112/0.70319. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67906/0.70322. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68070/0.70352. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68114/0.70328. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.67998/0.70377. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67980/0.70376. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67765/0.70391. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67779/0.70409. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67796/0.70417. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67742/0.70422. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67697/0.70416. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67663/0.70426. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67709/0.70451. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67655/0.70445. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67587/0.70483. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67454/0.70458. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67607/0.70449. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67434/0.70444. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67298/0.70530. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67339/0.70619. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 0.67368/0.70533. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67437/0.70570. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67325/0.70591. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67009/0.70569. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67224/0.70658. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67107/0.70646. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67015/0.70593. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66763/0.70669. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67153/0.70695. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66646/0.70701. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66595/0.70687. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66729/0.70687. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.66598/0.70675. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66376/0.70693. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66552/0.70739. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.66602/0.70773. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66364/0.70774. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66408/0.70743. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66502/0.70904. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66391/0.70848. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66003/0.70811. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66211/0.70819. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66134/0.70786. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65774/0.70676. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65619/0.70697. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65787/0.70826. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65906/0.70845. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.65456/0.70812. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65823/0.70787. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65657/0.70743. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65471/0.70930. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65411/0.70836. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65416/0.70889. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65088/0.70917. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65163/0.70909. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64744/0.70969. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.64774/0.71068. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64778/0.70983. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65033/0.71063. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64696/0.71106. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64432/0.71376. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64213/0.71137. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64767/0.71251. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64430/0.71369. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64152/0.71441. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64420/0.71493. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64066/0.71383. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64024/0.71376. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64016/0.71461. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63697/0.71702. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63408/0.72016. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63376/0.71991. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.63416/0.72227. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63030/0.72202. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62988/0.72405. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63305/0.72354. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63477/0.72234. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63298/0.72232. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63083/0.72456. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62936/0.72544. Took 0.20 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69233/0.69835. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69061/0.69981. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68905/0.69932. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68848/0.69861. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68891/0.69844. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68833/0.69833. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68747/0.69813. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68705/0.69840. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68754/0.69801. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68624/0.69761. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68647/0.69722. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68577/0.69740. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68522/0.69679. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68454/0.69714. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68453/0.69727. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68405/0.69759. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68378/0.69787. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68306/0.69865. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68297/0.69823. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68276/0.69762. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68299/0.69786. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68200/0.69726. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68151/0.69692. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68129/0.69758. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68053/0.69752. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68105/0.69723. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68016/0.69774. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67992/0.69772. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67972/0.69846. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67970/0.69868. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67748/0.69785. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67625/0.69833. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67661/0.69820. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67576/0.69835. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67362/0.69882. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67462/0.69818. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67232/0.69961. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67247/0.70018. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67302/0.70069. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67233/0.70145. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67019/0.70024. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67056/0.70136. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66984/0.70045. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66872/0.69992. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66655/0.70048. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.66818/0.70029. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.66602/0.70115. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66606/0.70127. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66591/0.70175. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66530/0.70229. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66567/0.70318. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66327/0.70311. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66305/0.70055. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66143/0.70292. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.65946/0.70110. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.65706/0.70382. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65835/0.70282. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65804/0.70355. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65646/0.70346. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65554/0.70212. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65464/0.70434. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65379/0.70366. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.65191/0.70434. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65278/0.70427. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65182/0.70517. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.64960/0.70623. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64737/0.70591. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.64627/0.70760. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64603/0.70804. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64586/0.70751. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.64789/0.70812. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64513/0.70815. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64468/0.70799. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64128/0.70769. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64301/0.70931. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63896/0.71049. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64157/0.70946. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63739/0.70998. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.63724/0.70999. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63655/0.71293. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63571/0.71364. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63475/0.71525. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.63535/0.71546. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63460/0.71462. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63274/0.71423. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63009/0.71677. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.63419/0.71443. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63019/0.71753. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63116/0.71781. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62811/0.71563. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62931/0.71608. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.62701/0.71390. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62784/0.71375. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62483/0.71543. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62169/0.71609. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62080/0.71923. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.62086/0.72196. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.61856/0.72109. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62117/0.72244. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61792/0.72279. Took 0.19 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69281/0.69353. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69163/0.69353. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69119/0.69357. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69065/0.69346. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69021/0.69345. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68929/0.69351. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68910/0.69346. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68889/0.69374. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68844/0.69387. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68808/0.69409. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68833/0.69428. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68731/0.69460. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68688/0.69488. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68625/0.69534. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68739/0.69544. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68626/0.69561. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68691/0.69579. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68624/0.69580. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68545/0.69574. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68453/0.69589. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68462/0.69614. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68486/0.69605. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68397/0.69638. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68390/0.69650. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68417/0.69680. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68364/0.69685. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68302/0.69714. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68224/0.69753. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68253/0.69746. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68181/0.69804. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68022/0.69867. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68148/0.69835. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68053/0.69865. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67986/0.69898. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67993/0.69933. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67966/0.69957. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67867/0.69952. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67730/0.70071. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67667/0.70079. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67653/0.70096. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67604/0.70102. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67527/0.70076. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67526/0.70101. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67578/0.70100. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67208/0.70140. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67374/0.70193. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67145/0.70197. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67163/0.70298. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67200/0.70389. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66985/0.70457. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67004/0.70479. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67008/0.70459. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66852/0.70459. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66867/0.70569. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66879/0.70561. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66810/0.70623. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66650/0.70636. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66486/0.70817. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66516/0.70758. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66458/0.70781. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66278/0.70878. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66278/0.70938. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66181/0.70878. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66193/0.71047. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66144/0.71031. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66056/0.71145. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65978/0.71045. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65963/0.71165. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65829/0.71105. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65689/0.71151. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65615/0.71399. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65404/0.71490. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65480/0.71594. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65472/0.71604. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65425/0.71617. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65165/0.71587. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65228/0.71751. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65047/0.71711. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64796/0.71722. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64915/0.71927. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64667/0.71836. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64582/0.71815. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64317/0.72128. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64242/0.72298. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64164/0.72274. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64389/0.72486. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.64157/0.72348. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63895/0.72262. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63648/0.72326. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63688/0.72508. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63553/0.72590. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63419/0.72983. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63264/0.72928. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63387/0.73000. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63297/0.72994. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62851/0.73088. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62748/0.73767. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62849/0.73570. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62362/0.73917. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62280/0.73560. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69366/0.69395. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69133/0.69395. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69021/0.69417. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68941/0.69438. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68848/0.69474. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68774/0.69532. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68745/0.69576. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68649/0.69619. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68748/0.69656. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68710/0.69697. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68653/0.69750. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68678/0.69804. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68552/0.69844. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68585/0.69897. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68405/0.69960. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68485/0.70018. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68500/0.70063. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68440/0.70107. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68457/0.70173. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68452/0.70215. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68390/0.70273. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68280/0.70324. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68317/0.70360. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68375/0.70397. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68262/0.70415. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68309/0.70429. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68224/0.70459. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68141/0.70501. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68200/0.70515. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68152/0.70539. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68188/0.70584. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68161/0.70662. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68184/0.70700. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.67963/0.70739. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68014/0.70759. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68001/0.70787. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67854/0.70783. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67869/0.70793. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67940/0.70832. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67897/0.70843. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67776/0.70886. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67785/0.70954. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67689/0.70958. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67467/0.70943. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67595/0.71038. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67489/0.71014. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67471/0.70988. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67527/0.70960. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67462/0.70996. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67253/0.70982. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67160/0.71033. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67116/0.71048. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67420/0.71023. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67082/0.71081. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67091/0.71070. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67250/0.71098. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67067/0.71126. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66950/0.71131. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67162/0.71151. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66806/0.71145. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66969/0.71215. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66665/0.71301. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66894/0.71369. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66644/0.71331. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66991/0.71273. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66633/0.71352. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66716/0.71352. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.66569/0.71394. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66500/0.71344. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66417/0.71353. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66496/0.71427. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66467/0.71471. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66559/0.71450. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66339/0.71477. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66340/0.71552. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66353/0.71522. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66102/0.71691. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66055/0.71597. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66142/0.71706. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65839/0.71793. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65867/0.71785. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65946/0.71851. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65991/0.71895. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65734/0.71951. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65865/0.71878. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65601/0.71948. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65694/0.71856. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65443/0.72094. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65473/0.72025. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65411/0.72094. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65425/0.72056. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65090/0.72100. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65336/0.72204. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64788/0.72294. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65255/0.72046. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65202/0.72068. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64873/0.72284. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64863/0.72182. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64925/0.72312. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64562/0.72457. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69246/0.69033. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69159/0.68933. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69051/0.68858. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69044/0.68806. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69001/0.68778. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69008/0.68754. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68859/0.68736. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68818/0.68724. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68838/0.68724. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68790/0.68731. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68674/0.68732. Took 0.22 sec\n",
      "Epoch 11, Loss(train/val) 0.68707/0.68730. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68609/0.68739. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68663/0.68726. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68615/0.68698. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68526/0.68706. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68563/0.68700. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68542/0.68706. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68440/0.68676. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68468/0.68677. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68376/0.68687. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68304/0.68735. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68198/0.68783. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68371/0.68766. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68337/0.68733. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68336/0.68812. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68355/0.68821. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68320/0.68795. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68193/0.68788. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68369/0.68788. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68198/0.68859. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68251/0.68923. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68222/0.68956. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68275/0.68927. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68200/0.68932. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68139/0.68976. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68053/0.68973. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68164/0.69067. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68042/0.69048. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68087/0.69022. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68038/0.69047. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68043/0.69105. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67990/0.69146. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67948/0.69257. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67837/0.69219. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68010/0.69272. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67823/0.69349. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67959/0.69343. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67677/0.69423. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67837/0.69380. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67598/0.69405. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67753/0.69446. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67730/0.69493. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67711/0.69518. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67657/0.69524. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67579/0.69535. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67618/0.69582. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67568/0.69616. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67432/0.69582. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67638/0.69592. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67261/0.69600. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67434/0.69643. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67234/0.69695. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66987/0.69771. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67238/0.69802. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67258/0.69818. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67051/0.69924. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66817/0.69791. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 0.67063/0.69927. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66652/0.69991. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66939/0.69897. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66572/0.69878. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66550/0.69974. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66799/0.70026. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66456/0.70031. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66679/0.69902. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66431/0.69982. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66611/0.69915. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66353/0.69978. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66194/0.70013. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66027/0.70081. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66140/0.70128. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65795/0.70144. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65957/0.70304. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65538/0.70379. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65683/0.70472. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65495/0.70634. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65491/0.70562. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 0.65555/0.70530. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65234/0.70539. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64992/0.70621. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64943/0.70553. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65392/0.70635. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64988/0.70667. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64437/0.70836. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64503/0.71109. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64553/0.71169. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64385/0.71214. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64468/0.71274. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64274/0.71518. Took 0.19 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69458/0.70414. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69045/0.71250. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68940/0.71306. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68842/0.71239. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68827/0.71273. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68768/0.71211. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68699/0.71167. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68642/0.71222. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68577/0.71293. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68567/0.71228. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68450/0.71200. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68467/0.71280. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68456/0.71320. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68413/0.71273. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68344/0.71323. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68316/0.71323. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68332/0.71395. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68273/0.71254. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68311/0.71401. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68196/0.71415. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68265/0.71442. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68133/0.71262. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68227/0.71504. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68060/0.71353. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68110/0.71554. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68166/0.71267. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67996/0.71414. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67851/0.71575. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68022/0.71524. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67909/0.71421. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67891/0.71569. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67781/0.71766. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67856/0.71556. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67829/0.71797. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67585/0.71446. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67551/0.71835. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67631/0.71977. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67639/0.71574. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67589/0.72042. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67359/0.71975. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67395/0.71951. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67478/0.71781. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67364/0.72137. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67302/0.71746. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67055/0.72330. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67093/0.72201. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67142/0.72275. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66905/0.71944. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66938/0.72434. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66864/0.72326. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66912/0.72452. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66682/0.72260. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66531/0.72618. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66533/0.72561. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66742/0.72152. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66187/0.72762. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66492/0.72456. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66194/0.72261. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65974/0.72973. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66158/0.72905. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66006/0.72956. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65882/0.73557. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66001/0.72874. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65849/0.73180. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65965/0.73181. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65747/0.72986. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65495/0.72898. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65418/0.73289. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65451/0.73381. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65254/0.73113. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65200/0.72965. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65124/0.73378. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65113/0.73130. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64784/0.73281. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64853/0.73580. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64608/0.73613. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64841/0.73852. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64538/0.73321. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64553/0.74028. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64343/0.73878. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64147/0.74075. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64222/0.74045. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63952/0.74486. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63816/0.74285. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63707/0.73570. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63817/0.74104. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63432/0.74441. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63582/0.74447. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63497/0.75256. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63268/0.74775. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63508/0.74906. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63175/0.74576. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63018/0.74858. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63358/0.74757. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63084/0.74735. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 0.63448/0.74335. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62963/0.74383. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62537/0.74928. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62459/0.75312. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62699/0.75328. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69686/0.68302. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69352/0.68765. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69234/0.69070. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69109/0.69267. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69036/0.69460. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68943/0.69650. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68741/0.69904. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68732/0.70099. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68654/0.70401. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68515/0.70741. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68558/0.70952. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68461/0.71164. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68282/0.71372. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68283/0.71648. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68231/0.71806. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68263/0.71892. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68168/0.72023. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.67962/0.72131. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67813/0.72350. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67767/0.72501. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67811/0.72609. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67998/0.72581. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67794/0.72678. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67664/0.72795. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.67804/0.72834. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67393/0.72978. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67636/0.73097. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67562/0.73000. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67612/0.73146. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.67512/0.73155. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67542/0.73064. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67393/0.73183. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67252/0.73293. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67354/0.73263. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67241/0.73222. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67206/0.73384. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67180/0.73322. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67163/0.73318. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66996/0.73442. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67184/0.73415. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66985/0.73558. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.66960/0.73505. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66939/0.73550. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66808/0.73607. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66846/0.73510. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66860/0.73662. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66677/0.73648. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66513/0.73805. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66369/0.73733. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.66527/0.73550. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66554/0.73682. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66309/0.73946. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66244/0.73958. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.66272/0.74112. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66098/0.74295. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65973/0.74182. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65987/0.74093. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66004/0.74124. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66170/0.74075. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66158/0.74052. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65556/0.74199. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65637/0.74435. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65767/0.74443. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65725/0.74504. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65482/0.74408. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65442/0.74655. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65638/0.74699. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65298/0.74793. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65338/0.74716. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65080/0.75004. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64757/0.75075. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64657/0.75269. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64745/0.75266. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64608/0.75344. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64626/0.75245. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64335/0.75584. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64605/0.75643. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.63979/0.75819. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64018/0.75850. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63907/0.76104. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64164/0.76085. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63620/0.76217. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63630/0.76380. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63755/0.76495. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63556/0.76622. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63259/0.76734. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63288/0.77000. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63246/0.76837. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63245/0.77193. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62591/0.77695. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62923/0.77333. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62412/0.77661. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62464/0.77731. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62690/0.78058. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62486/0.78099. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62404/0.78376. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61941/0.78042. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61844/0.78356. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61945/0.78275. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61552/0.78424. Took 0.18 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69328/0.69842. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69149/0.69839. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69029/0.69691. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68886/0.69560. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68864/0.69411. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68722/0.69229. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68579/0.69144. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68574/0.69031. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68396/0.68905. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68184/0.68755. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68225/0.68694. Took 0.22 sec\n",
      "Epoch 11, Loss(train/val) 0.68083/0.68655. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68077/0.68574. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.67942/0.68481. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.67988/0.68532. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.67765/0.68437. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.67831/0.68460. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.67738/0.68457. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.67623/0.68581. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.67619/0.68576. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.67429/0.68725. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67403/0.68666. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67480/0.68680. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67355/0.68704. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67300/0.68793. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67279/0.68865. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67210/0.68871. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67347/0.68976. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.67117/0.68941. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67080/0.69048. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.66936/0.69149. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.66881/0.69196. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.66962/0.69277. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66701/0.69130. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.66907/0.69319. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.66783/0.69305. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.66877/0.69387. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.66622/0.69414. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66621/0.69311. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.66620/0.69655. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66367/0.69750. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66526/0.69695. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66363/0.69675. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66429/0.69753. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66101/0.69806. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66145/0.69983. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66242/0.69900. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66339/0.69947. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66057/0.69896. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66148/0.69827. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66155/0.70052. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.65923/0.70065. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65966/0.69903. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66012/0.70152. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65850/0.70212. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65646/0.70235. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65741/0.70096. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65582/0.70474. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65773/0.70220. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65659/0.70119. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65678/0.70415. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65584/0.70344. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65472/0.70329. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65409/0.70502. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65246/0.70463. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65274/0.70582. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65231/0.70689. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65394/0.70971. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64984/0.70814. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64930/0.70883. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64954/0.70909. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64737/0.70798. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64739/0.70740. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64707/0.70934. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64622/0.71014. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64449/0.71006. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64687/0.71171. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64653/0.70877. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64549/0.70939. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64472/0.71415. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64247/0.71153. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64443/0.70989. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64425/0.71287. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64125/0.70944. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64335/0.71355. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63814/0.71636. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63690/0.71462. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63855/0.71504. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63705/0.71548. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63615/0.71185. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63274/0.71353. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63302/0.70848. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63462/0.71312. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63153/0.71226. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63598/0.71531. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63183/0.71531. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62766/0.71261. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.62908/0.71393. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62854/0.71027. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62938/0.71685. Took 0.19 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69515/0.68736. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69267/0.68736. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69218/0.68753. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69168/0.68775. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69059/0.68802. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68980/0.68828. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68962/0.68867. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68875/0.68887. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68789/0.68926. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68752/0.68984. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68669/0.69049. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68627/0.69132. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68500/0.69200. Took 0.22 sec\n",
      "Epoch 13, Loss(train/val) 0.68525/0.69255. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68431/0.69321. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68502/0.69392. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68306/0.69464. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68287/0.69515. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68213/0.69565. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67984/0.69628. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68091/0.69718. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68058/0.69773. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67935/0.69854. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67794/0.69924. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67786/0.69981. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67676/0.70027. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67660/0.70011. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67603/0.70050. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67563/0.70053. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67303/0.70009. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67132/0.69977. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67188/0.69886. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67105/0.69849. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66828/0.69857. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66728/0.69882. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66644/0.69756. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66507/0.69906. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.66402/0.69851. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66241/0.69770. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66011/0.69826. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66205/0.69695. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.65742/0.69561. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.65990/0.69602. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.65684/0.69526. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65521/0.69555. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65737/0.69523. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65563/0.69436. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65322/0.69420. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65216/0.69411. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65135/0.69719. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65116/0.69494. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.64979/0.69395. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65131/0.69320. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.64926/0.69441. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64974/0.69634. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.64776/0.69432. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64335/0.69470. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64482/0.69527. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64246/0.69364. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64102/0.69631. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64043/0.69607. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.63757/0.69411. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64102/0.69811. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63950/0.69557. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63901/0.69728. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63981/0.69698. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63147/0.69449. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63670/0.69674. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63642/0.69617. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63361/0.69669. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.63240/0.69472. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63266/0.69743. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63002/0.69809. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62956/0.69801. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62388/0.69656. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62539/0.69812. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62405/0.69960. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62576/0.69852. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62428/0.69424. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.62920/0.69980. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61994/0.70091. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62463/0.70303. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62127/0.69951. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62104/0.69892. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62224/0.69982. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61459/0.69565. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61813/0.69661. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.61699/0.69907. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61357/0.69856. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61269/0.70233. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61032/0.69809. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61565/0.69964. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61337/0.70279. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.60869/0.69968. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.60701/0.70053. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.60404/0.70976. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.60707/0.70559. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60645/0.70955. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60475/0.71091. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.59856/0.70511. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69509/0.69353. Took 0.39 sec\n",
      "Epoch 1, Loss(train/val) 0.69262/0.69375. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69139/0.69487. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69058/0.69592. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68964/0.69714. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68817/0.69855. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68803/0.69975. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68767/0.70116. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68662/0.70240. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68631/0.70361. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68664/0.70465. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68602/0.70565. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68450/0.70673. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68402/0.70812. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68517/0.70891. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68308/0.70966. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68386/0.71063. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68391/0.71150. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68279/0.71234. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68416/0.71267. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68166/0.71333. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68246/0.71425. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68240/0.71497. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68070/0.71575. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68102/0.71629. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68165/0.71707. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67877/0.71775. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.68007/0.71862. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68053/0.71889. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67953/0.71944. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68018/0.72013. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67926/0.72040. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67842/0.72078. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67894/0.72124. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67734/0.72166. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.67861/0.72267. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67922/0.72245. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67721/0.72304. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67801/0.72357. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67717/0.72391. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67746/0.72449. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67665/0.72520. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67606/0.72623. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67702/0.72726. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67569/0.72685. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67516/0.72734. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67690/0.72760. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67377/0.72855. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67371/0.72939. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67495/0.72935. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67408/0.72962. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67307/0.72935. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67372/0.73025. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.67277/0.73126. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67346/0.73135. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67268/0.73092. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67129/0.73233. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67209/0.73285. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67075/0.73392. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67035/0.73416. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67003/0.73408. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66972/0.73423. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66989/0.73505. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66892/0.73565. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66691/0.73687. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66772/0.73893. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66713/0.73941. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66654/0.73971. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66665/0.73986. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66658/0.74064. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66513/0.74113. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66387/0.74164. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66311/0.74308. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66362/0.74315. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66381/0.74543. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66130/0.74515. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66126/0.74472. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65982/0.74609. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66080/0.74594. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66151/0.74902. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65721/0.75020. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65771/0.75025. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65660/0.75077. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65498/0.75072. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65639/0.75292. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65420/0.75414. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65606/0.75443. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65189/0.75581. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65317/0.75744. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65209/0.76207. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65073/0.76320. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64948/0.76326. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64782/0.76341. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64836/0.76486. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64417/0.76775. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64500/0.76804. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64229/0.76743. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64453/0.76818. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64197/0.76757. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64115/0.77000. Took 0.18 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69538/0.68682. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69275/0.68452. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69164/0.68501. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69134/0.68627. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69039/0.68702. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69025/0.68765. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68956/0.68877. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68875/0.68965. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68843/0.69004. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68775/0.69134. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68812/0.69232. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68691/0.69255. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68701/0.69365. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68713/0.69490. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68592/0.69555. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68556/0.69615. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68556/0.69742. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68519/0.69782. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68386/0.69935. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68411/0.69997. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68301/0.70079. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68335/0.70105. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68383/0.70195. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68221/0.70298. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68250/0.70425. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68203/0.70454. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68096/0.70581. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68147/0.70679. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68102/0.70794. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68021/0.70826. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68092/0.70963. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67978/0.71081. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67974/0.71062. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67932/0.71207. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67904/0.71331. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67855/0.71337. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67870/0.71428. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67650/0.71411. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67733/0.71587. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67728/0.71610. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67685/0.71691. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67706/0.71691. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67683/0.71721. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67609/0.71762. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67605/0.71780. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67566/0.71816. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67474/0.71859. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67469/0.71899. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67572/0.71948. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67401/0.71964. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67434/0.72049. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67273/0.72078. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67284/0.72100. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67197/0.72132. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67180/0.72249. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67109/0.72169. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67248/0.72269. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67120/0.72330. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67016/0.72352. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66889/0.72393. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67075/0.72386. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66768/0.72462. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66824/0.72571. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66722/0.72569. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66774/0.72684. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66550/0.72532. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66612/0.72620. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66572/0.72598. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66290/0.72668. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66359/0.72569. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66184/0.72621. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66002/0.72625. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66317/0.72608. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66252/0.72675. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66105/0.72493. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65851/0.72707. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65693/0.72612. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65718/0.72561. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65403/0.72591. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65199/0.72633. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65450/0.72688. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64966/0.72592. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65173/0.72772. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64899/0.72542. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64806/0.72374. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64889/0.72633. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64354/0.72883. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64495/0.72878. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64238/0.72685. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64386/0.72811. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64348/0.72851. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63941/0.72379. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63705/0.72658. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63916/0.72569. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63856/0.72300. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63386/0.72447. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63388/0.72390. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63285/0.72582. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63120/0.72474. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63311/0.72689. Took 0.18 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69505/0.70110. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69164/0.69813. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69203/0.69816. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69075/0.69800. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69108/0.69779. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69015/0.69746. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68969/0.69727. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68963/0.69733. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68863/0.69726. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68854/0.69735. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68858/0.69721. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68766/0.69708. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68728/0.69702. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68726/0.69720. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68735/0.69693. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68682/0.69687. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68593/0.69662. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68603/0.69655. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68592/0.69681. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68489/0.69659. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68507/0.69653. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68459/0.69657. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68424/0.69655. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68276/0.69660. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68363/0.69720. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68229/0.69773. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68256/0.69793. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68096/0.69795. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68177/0.69831. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68105/0.69881. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68136/0.69941. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67975/0.69962. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68010/0.69998. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68007/0.70121. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67920/0.70150. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67788/0.70130. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67737/0.70177. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67622/0.70257. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67809/0.70293. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67644/0.70417. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67672/0.70539. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67472/0.70617. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67477/0.70688. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67473/0.70761. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67230/0.70869. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67277/0.70883. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67461/0.70966. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67112/0.71055. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67335/0.71156. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67016/0.71204. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67063/0.71109. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66859/0.71292. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67044/0.71377. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66836/0.71348. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66767/0.71451. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66754/0.71584. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66473/0.71635. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66472/0.71967. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66278/0.71986. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66433/0.72332. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66324/0.72008. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66281/0.71991. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66300/0.72427. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66228/0.72347. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65959/0.72109. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65783/0.72090. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65743/0.72131. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65906/0.72410. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65533/0.72348. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65698/0.72382. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65614/0.72308. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65434/0.72553. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65572/0.72481. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65085/0.72968. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65343/0.72858. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65169/0.72725. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65139/0.73243. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64900/0.72636. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64988/0.73203. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64918/0.73252. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64966/0.72978. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64810/0.72763. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64695/0.72895. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64182/0.73062. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64168/0.73052. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64340/0.73073. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64451/0.73227. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64420/0.73259. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63864/0.73245. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63880/0.73577. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63749/0.73356. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63409/0.73467. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63341/0.73471. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63703/0.73381. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63071/0.73578. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63286/0.73423. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.62992/0.73181. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62981/0.73269. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62861/0.73618. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62878/0.73174. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69933/0.69631. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69187/0.69417. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68986/0.69409. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68921/0.69430. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68902/0.69450. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68887/0.69472. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68862/0.69482. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68808/0.69495. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68762/0.69518. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68776/0.69558. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68739/0.69604. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68631/0.69662. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68732/0.69716. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68645/0.69775. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68636/0.69830. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68631/0.69871. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68587/0.69934. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68525/0.69996. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68520/0.70038. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68492/0.70066. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68427/0.70098. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68400/0.70120. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68388/0.70135. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68321/0.70141. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68294/0.70148. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68287/0.70159. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68189/0.70168. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68093/0.70158. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68128/0.70131. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68156/0.70129. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68026/0.70091. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68027/0.70073. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67935/0.70071. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67909/0.70034. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67924/0.69968. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67971/0.69940. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67703/0.69933. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67768/0.69893. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67787/0.69836. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67687/0.69815. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67559/0.69803. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67527/0.69724. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67421/0.69722. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67362/0.69657. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67246/0.69524. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67277/0.69599. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67001/0.69453. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67012/0.69359. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.66931/0.69422. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66972/0.69434. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 0.66785/0.69368. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66687/0.69228. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.66696/0.69199. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66375/0.69191. Took 0.21 sec\n",
      "Epoch 54, Loss(train/val) 0.66585/0.69110. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66099/0.69108. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 0.66237/0.69138. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66154/0.69116. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66182/0.69240. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66183/0.69191. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 0.66042/0.69185. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65975/0.69232. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65856/0.69299. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65749/0.69338. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65570/0.69353. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65550/0.69328. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65151/0.69407. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65492/0.69469. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.65325/0.69529. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65264/0.69567. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65203/0.69564. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65085/0.69704. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64908/0.69744. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64791/0.69859. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64815/0.69925. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64724/0.70029. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.64761/0.70124. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64527/0.70236. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64584/0.70414. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.64502/0.70557. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64375/0.70618. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64094/0.70855. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64234/0.70956. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63962/0.71097. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.63966/0.71067. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63875/0.71273. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63735/0.71419. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63564/0.71567. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63759/0.71613. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.63127/0.71801. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63570/0.71886. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63578/0.72038. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.63483/0.72190. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63173/0.72422. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.63031/0.72528. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.62404/0.72553. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62895/0.72902. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.62447/0.73073. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.62130/0.73325. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62528/0.73533. Took 0.19 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69205/0.69319. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68895/0.69502. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68914/0.69608. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68891/0.69652. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68741/0.69694. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68702/0.69716. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68732/0.69731. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68709/0.69742. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68694/0.69760. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68622/0.69787. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68596/0.69793. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68547/0.69788. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68569/0.69805. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68487/0.69830. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68394/0.69833. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68365/0.69869. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68358/0.69886. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68322/0.69879. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68319/0.69900. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68239/0.69896. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68232/0.69951. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68211/0.69991. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68262/0.69988. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68167/0.70011. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68221/0.69971. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68149/0.69977. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68202/0.69999. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68122/0.69925. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67905/0.69965. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68187/0.69953. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67967/0.69994. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68018/0.69938. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67899/0.69984. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67828/0.69980. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67857/0.69967. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67830/0.69913. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67664/0.69908. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67789/0.69963. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67752/0.69933. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67703/0.69925. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67714/0.69950. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67432/0.69971. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67680/0.69989. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67655/0.69931. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67515/0.69900. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67524/0.69888. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67515/0.69922. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67410/0.69955. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67272/0.69816. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67322/0.69785. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67268/0.69872. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67254/0.69853. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67226/0.69844. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67032/0.69903. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67134/0.69995. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66928/0.69928. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66909/0.70119. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66717/0.70067. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66895/0.70094. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66818/0.70044. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66831/0.70060. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66861/0.70131. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66589/0.69999. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66836/0.70149. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66655/0.70105. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66401/0.70224. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66637/0.70138. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66444/0.70248. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66350/0.70291. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66267/0.70206. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66495/0.70274. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66319/0.70218. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66370/0.70355. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66021/0.70336. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66317/0.70323. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66079/0.70314. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65989/0.70319. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66231/0.70322. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66068/0.70266. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66044/0.70306. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65798/0.70406. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65586/0.70426. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65817/0.70373. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65635/0.70435. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65390/0.70410. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65703/0.70308. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65561/0.70337. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65776/0.70371. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65502/0.70346. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65496/0.70325. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65335/0.70336. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64974/0.70407. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65009/0.70447. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65166/0.70638. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65031/0.70528. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64671/0.70547. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64954/0.70644. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64575/0.70482. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64713/0.70638. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64485/0.70384. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69735/0.69159. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69361/0.69117. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69134/0.69120. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68864/0.69159. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68814/0.69230. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68808/0.69316. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68683/0.69384. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68650/0.69435. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68674/0.69478. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68687/0.69514. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68635/0.69535. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68622/0.69575. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68593/0.69576. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68587/0.69587. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68539/0.69601. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68541/0.69611. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68613/0.69605. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68415/0.69604. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68573/0.69607. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68381/0.69631. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68376/0.69650. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68395/0.69647. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68289/0.69622. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68371/0.69632. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68242/0.69622. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68369/0.69607. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68176/0.69596. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68214/0.69602. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68247/0.69581. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68204/0.69569. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68139/0.69526. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68000/0.69492. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68133/0.69497. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68038/0.69460. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68055/0.69448. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67970/0.69415. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67914/0.69413. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68000/0.69404. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67872/0.69343. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67892/0.69297. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67533/0.69239. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67665/0.69225. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67707/0.69212. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67681/0.69179. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67594/0.69119. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67407/0.69078. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67436/0.69042. Took 0.22 sec\n",
      "Epoch 47, Loss(train/val) 0.67471/0.68992. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67390/0.69019. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.67454/0.68972. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67310/0.68895. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.67220/0.68847. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67171/0.68725. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67079/0.68700. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67115/0.68694. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67210/0.68690. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66964/0.68628. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.66969/0.68535. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66841/0.68349. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66782/0.68293. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66652/0.68208. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66624/0.68026. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66386/0.67934. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66530/0.67639. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66247/0.67569. Took 0.22 sec\n",
      "Epoch 65, Loss(train/val) 0.66436/0.67526. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66236/0.67408. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66013/0.67148. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66131/0.67048. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65964/0.67131. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65752/0.67125. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65454/0.66916. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65419/0.66736. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.65692/0.66495. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65310/0.66319. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65474/0.66229. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65282/0.66146. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65445/0.65992. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65173/0.65911. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.65115/0.65847. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64948/0.65733. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.64804/0.65735. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64578/0.65899. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64424/0.65860. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64044/0.65525. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64258/0.65540. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64025/0.65614. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64014/0.65375. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64221/0.65404. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64065/0.65262. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63705/0.65259. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63938/0.65478. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63673/0.65293. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63572/0.65039. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63318/0.65135. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63098/0.65342. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62943/0.65122. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63296/0.64902. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63171/0.65092. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62791/0.65107. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.68853/0.68759. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68650/0.68777. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68583/0.68821. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68571/0.68862. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68460/0.68903. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68481/0.68963. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68512/0.69008. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68352/0.69044. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68362/0.69055. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68357/0.69100. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68273/0.69136. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68308/0.69154. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68319/0.69189. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68244/0.69211. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68157/0.69229. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68193/0.69215. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68133/0.69221. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68063/0.69260. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68142/0.69253. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68091/0.69278. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68109/0.69288. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68060/0.69289. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67941/0.69236. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67988/0.69278. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67962/0.69264. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67931/0.69298. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67841/0.69275. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67920/0.69265. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67790/0.69279. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67848/0.69253. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67933/0.69287. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67899/0.69311. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67880/0.69318. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67813/0.69289. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67836/0.69292. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67714/0.69275. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67674/0.69276. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67820/0.69309. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67651/0.69308. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67676/0.69357. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67634/0.69383. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67669/0.69401. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67593/0.69446. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67562/0.69458. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67628/0.69429. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67527/0.69450. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67477/0.69437. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67463/0.69515. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67365/0.69576. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67538/0.69582. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67354/0.69618. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.67458/0.69556. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67307/0.69607. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67332/0.69652. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67233/0.69653. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.67175/0.69725. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67284/0.69678. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67198/0.69725. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67032/0.69779. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67107/0.69785. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67102/0.69778. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66968/0.69829. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66964/0.69806. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67027/0.69819. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66822/0.69874. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.66680/0.69912. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66747/0.69914. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66549/0.69978. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66647/0.70031. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66582/0.69974. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66444/0.70082. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 0.66397/0.70043. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66330/0.70125. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66620/0.70213. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66364/0.70113. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66296/0.70137. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65983/0.70193. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65997/0.70336. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66226/0.70310. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66098/0.70382. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65699/0.70410. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.66019/0.70352. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65786/0.70461. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66050/0.70514. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65803/0.70704. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.65686/0.70691. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65581/0.70882. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65328/0.70891. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65300/0.70926. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65253/0.70964. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65234/0.70930. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65132/0.71031. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64956/0.71040. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65214/0.71182. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64812/0.71231. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64502/0.71567. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64927/0.71344. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64359/0.71494. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64891/0.71509. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64401/0.71665. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.68721/0.69032. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68580/0.69050. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68504/0.69023. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68432/0.69007. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68403/0.69015. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68364/0.68998. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68306/0.68959. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68198/0.68908. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68219/0.68853. Took 0.22 sec\n",
      "Epoch 9, Loss(train/val) 0.68241/0.68778. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68167/0.68701. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68156/0.68630. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68150/0.68540. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68155/0.68490. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68095/0.68438. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68052/0.68430. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68014/0.68382. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.67975/0.68361. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68066/0.68322. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.67981/0.68283. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67925/0.68254. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.67949/0.68223. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.67859/0.68184. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67951/0.68107. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67830/0.68047. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.67838/0.68057. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67816/0.68069. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67862/0.68061. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67778/0.68046. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67678/0.68016. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67745/0.67974. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67671/0.67893. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67683/0.67853. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67617/0.67810. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67634/0.67769. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67601/0.67766. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.67535/0.67734. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67456/0.67726. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67445/0.67755. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67476/0.67754. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67291/0.67688. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67397/0.67583. Took 0.22 sec\n",
      "Epoch 42, Loss(train/val) 0.67326/0.67529. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67295/0.67444. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67316/0.67468. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67165/0.67457. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67248/0.67481. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67190/0.67479. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67167/0.67437. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67027/0.67457. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67189/0.67456. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67123/0.67447. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66816/0.67271. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66919/0.67268. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.66849/0.67245. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66908/0.67202. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66732/0.67178. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66576/0.67168. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66610/0.67111. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66507/0.67119. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66358/0.67047. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66497/0.67035. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66330/0.67099. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66436/0.67055. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66248/0.67069. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66073/0.67085. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65984/0.66980. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65907/0.66950. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66014/0.66894. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65839/0.66879. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65727/0.66888. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65593/0.66932. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65507/0.66949. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65408/0.66894. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65616/0.66909. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65230/0.66852. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65182/0.66990. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65206/0.66945. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64980/0.66977. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65053/0.66873. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64809/0.66926. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.64603/0.66906. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.64839/0.67018. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64733/0.67008. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.64756/0.66947. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64483/0.67022. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64439/0.67022. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64306/0.67083. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64311/0.67056. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64193/0.67114. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.64510/0.67261. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.63938/0.67333. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63754/0.67279. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63881/0.67220. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63980/0.67426. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.63615/0.67332. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63550/0.67461. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.63667/0.67468. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 0.63356/0.67532. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63165/0.67723. Took 0.19 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.68909/0.70430. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68647/0.70403. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68518/0.70388. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68472/0.70406. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68393/0.70426. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68375/0.70388. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68328/0.70372. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68268/0.70388. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68146/0.70420. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68140/0.70420. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68069/0.70359. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68169/0.70412. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68081/0.70391. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68060/0.70383. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68067/0.70356. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68046/0.70408. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68009/0.70325. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68017/0.70432. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.67919/0.70438. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67982/0.70392. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67926/0.70376. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67933/0.70366. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67939/0.70347. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67847/0.70380. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67832/0.70381. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67867/0.70411. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67859/0.70431. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67738/0.70471. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67748/0.70493. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67720/0.70524. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67719/0.70478. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67712/0.70496. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67702/0.70487. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67581/0.70555. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67633/0.70508. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67597/0.70512. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67621/0.70584. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67512/0.70569. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67485/0.70583. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67481/0.70590. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67399/0.70671. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67552/0.70722. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67461/0.70689. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67427/0.70697. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67446/0.70680. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67320/0.70753. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67350/0.70746. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67267/0.70659. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67214/0.70750. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67236/0.70789. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67318/0.70793. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67187/0.70800. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67151/0.70684. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67210/0.70744. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67090/0.70731. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67062/0.70667. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67039/0.70729. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66915/0.70857. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67132/0.70752. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66939/0.70871. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67096/0.70907. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66912/0.70860. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66844/0.70988. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66632/0.70970. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67012/0.70926. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66716/0.70989. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66724/0.71038. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66705/0.70878. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66580/0.71015. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66567/0.71062. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66382/0.71175. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66588/0.71197. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66486/0.71164. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66284/0.71193. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66524/0.71132. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66306/0.71309. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66289/0.71218. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.66287/0.71288. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66008/0.71175. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66075/0.71428. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66012/0.71297. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66004/0.71200. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65896/0.71446. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65848/0.71606. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66060/0.71471. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65842/0.71682. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65881/0.71479. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65839/0.71556. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65872/0.71679. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65489/0.71708. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65724/0.71709. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65488/0.71767. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65481/0.71708. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65424/0.71846. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65820/0.71946. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65402/0.71959. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65618/0.71732. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65666/0.72077. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65722/0.72003. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.65654/0.71834. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69100/0.68622. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68719/0.68449. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68691/0.68327. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68653/0.68240. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68581/0.68175. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68552/0.68121. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68511/0.68088. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68534/0.68069. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68449/0.68056. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68346/0.68054. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68291/0.68052. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68418/0.68070. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68420/0.68096. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68342/0.68106. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68318/0.68113. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68291/0.68127. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68303/0.68140. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68305/0.68154. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68224/0.68166. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68207/0.68177. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68186/0.68189. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68202/0.68199. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68061/0.68220. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68161/0.68236. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68142/0.68244. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68093/0.68265. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68102/0.68257. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68064/0.68228. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68019/0.68261. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67929/0.68270. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67919/0.68277. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67889/0.68289. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67844/0.68308. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67823/0.68351. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67888/0.68381. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67828/0.68385. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67773/0.68412. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67708/0.68440. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67795/0.68473. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67630/0.68464. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67770/0.68450. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67645/0.68473. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67495/0.68457. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67567/0.68530. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67625/0.68615. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.67386/0.68570. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67543/0.68638. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67429/0.68640. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67274/0.68653. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67357/0.68695. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67403/0.68759. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67365/0.68765. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67156/0.68750. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67264/0.68772. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67085/0.68902. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67210/0.69018. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67107/0.69026. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66938/0.69089. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67175/0.69167. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67043/0.69220. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66910/0.69234. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67044/0.69296. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66745/0.69343. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66903/0.69382. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66736/0.69417. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.66875/0.69481. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66821/0.69520. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66768/0.69544. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66746/0.69519. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66487/0.69669. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66675/0.69712. Took 0.22 sec\n",
      "Epoch 71, Loss(train/val) 0.66611/0.69723. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.66467/0.69781. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66304/0.69759. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66412/0.69818. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.66367/0.69828. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 0.66531/0.69796. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66334/0.69969. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66406/0.69926. Took 0.22 sec\n",
      "Epoch 79, Loss(train/val) 0.66432/0.69940. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66178/0.70018. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.66105/0.70005. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66318/0.70005. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.66242/0.70078. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.66180/0.70238. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66040/0.70270. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65945/0.70264. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65795/0.70295. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65959/0.70433. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65858/0.70439. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65905/0.70578. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.65619/0.70517. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65830/0.70708. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65772/0.70626. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65655/0.70641. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65713/0.70676. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65589/0.70805. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65462/0.70828. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65562/0.70872. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.65393/0.71010. Took 0.18 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69528/0.69229. Took 0.35 sec\n",
      "Epoch 1, Loss(train/val) 0.69186/0.69182. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69124/0.69178. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69105/0.69183. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69072/0.69186. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68962/0.69192. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68993/0.69190. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68950/0.69218. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68865/0.69234. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68821/0.69242. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68841/0.69254. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68706/0.69272. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68649/0.69293. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68688/0.69311. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68606/0.69321. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68595/0.69307. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68693/0.69313. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68550/0.69348. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68491/0.69397. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68561/0.69386. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68559/0.69393. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68422/0.69383. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68483/0.69384. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68372/0.69404. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68468/0.69428. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68360/0.69453. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68232/0.69465. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68306/0.69447. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68253/0.69479. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68178/0.69469. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68228/0.69470. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68161/0.69491. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68116/0.69508. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67991/0.69514. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68104/0.69515. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68012/0.69552. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67875/0.69585. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68047/0.69573. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67942/0.69567. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67892/0.69620. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67702/0.69592. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67683/0.69640. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67753/0.69677. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67700/0.69689. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67659/0.69641. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67608/0.69569. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67584/0.69672. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67339/0.69708. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67449/0.69766. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67437/0.69789. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67533/0.69759. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67307/0.69793. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67389/0.69889. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67299/0.69964. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66969/0.70063. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66965/0.69955. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67065/0.69960. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66999/0.69988. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66825/0.70130. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66941/0.70120. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66974/0.70127. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66912/0.70208. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66841/0.70279. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66388/0.70347. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66555/0.70271. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66544/0.70340. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66537/0.70411. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66494/0.70255. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66572/0.70262. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66104/0.70370. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66227/0.70277. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66128/0.70397. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66160/0.70524. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65778/0.70403. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66454/0.70320. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65750/0.70370. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65704/0.70536. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65456/0.70398. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65288/0.70722. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65383/0.70903. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65124/0.70652. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65271/0.70495. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65280/0.70732. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65044/0.70918. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65251/0.70531. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.64658/0.70862. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65133/0.70953. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64854/0.70602. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64903/0.70818. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64530/0.70687. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64360/0.70660. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64566/0.71022. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64363/0.70942. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64088/0.71056. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64495/0.71079. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64135/0.71259. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63892/0.71235. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64076/0.71211. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63483/0.70988. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64185/0.70650. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69228/0.69622. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69178/0.69587. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69135/0.69599. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69064/0.69600. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68941/0.69579. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68934/0.69552. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68934/0.69556. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68809/0.69571. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68756/0.69583. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68790/0.69585. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68713/0.69593. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68676/0.69600. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68617/0.69609. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68677/0.69588. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68618/0.69642. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68531/0.69590. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68559/0.69592. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68490/0.69620. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68481/0.69603. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68464/0.69623. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68456/0.69606. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68486/0.69650. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68451/0.69591. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68482/0.69636. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68368/0.69602. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68317/0.69630. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68389/0.69674. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68223/0.69570. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68242/0.69669. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68184/0.69552. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68223/0.69523. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68103/0.69527. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68107/0.69587. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68155/0.69496. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68187/0.69549. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67956/0.69529. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68017/0.69535. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67955/0.69453. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68053/0.69342. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.67920/0.69462. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67948/0.69453. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68083/0.69367. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67901/0.69426. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67990/0.69423. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67805/0.69353. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67853/0.69368. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67813/0.69370. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67834/0.69342. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67550/0.69381. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67805/0.69273. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67689/0.69321. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67602/0.69306. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67705/0.69284. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67537/0.69194. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67647/0.69272. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.67439/0.69273. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67505/0.69241. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67426/0.69298. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67331/0.69367. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67483/0.69270. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67454/0.69189. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67300/0.69303. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67410/0.69206. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67317/0.69228. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67138/0.69234. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66978/0.69380. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67024/0.69412. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66991/0.69454. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67035/0.69412. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66990/0.69421. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66753/0.69413. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66921/0.69542. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66829/0.69581. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66720/0.69638. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66790/0.69711. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66817/0.69746. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66748/0.69696. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66599/0.69628. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66564/0.69767. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66725/0.69778. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66344/0.69770. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66219/0.69853. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66310/0.69896. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66488/0.69866. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66312/0.69906. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66338/0.69913. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66162/0.70015. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66150/0.69989. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66158/0.70054. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66155/0.70032. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65938/0.70120. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66058/0.70204. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65947/0.70223. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65943/0.70288. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65717/0.70316. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65862/0.70332. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65482/0.70413. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65633/0.70475. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65657/0.70535. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.65510/0.70678. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69350/0.68768. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69229/0.68815. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69171/0.68843. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69107/0.68849. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69150/0.68876. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69042/0.68893. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69019/0.68923. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69051/0.68932. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69083/0.68951. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68989/0.68961. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68992/0.68956. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68935/0.68961. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69005/0.68972. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68960/0.68979. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68930/0.68985. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68846/0.69004. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68820/0.69030. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68863/0.69035. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68824/0.69051. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68767/0.69060. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68730/0.69076. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68824/0.69120. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68734/0.69109. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68594/0.69110. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68734/0.69141. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68580/0.69170. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68617/0.69184. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68602/0.69177. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68462/0.69245. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68622/0.69284. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68506/0.69279. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68509/0.69286. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68491/0.69367. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68394/0.69419. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68481/0.69436. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68353/0.69463. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68398/0.69510. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.68342/0.69516. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68221/0.69543. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 0.68252/0.69578. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68209/0.69612. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68114/0.69669. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68191/0.69727. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67942/0.69791. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68112/0.69864. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68058/0.69908. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67944/0.69917. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67849/0.69913. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67930/0.69929. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67810/0.69952. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67703/0.70104. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67554/0.70212. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67671/0.70224. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67675/0.70307. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67496/0.70365. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.67487/0.70451. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67327/0.70669. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67204/0.70703. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67333/0.70733. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.67309/0.70728. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67178/0.70849. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67138/0.70897. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67108/0.70859. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66943/0.70943. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66737/0.71115. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66605/0.71300. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67012/0.71309. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66657/0.71281. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66660/0.71512. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66306/0.71631. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66263/0.71655. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 0.66182/0.71887. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66112/0.72033. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66334/0.71785. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66081/0.71917. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65915/0.72184. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66116/0.72235. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65591/0.72308. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65433/0.72668. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65567/0.72743. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65536/0.72823. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65403/0.72959. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65207/0.73435. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65425/0.73580. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65361/0.73477. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65333/0.73309. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64989/0.73432. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64768/0.73730. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64845/0.74031. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.64619/0.74213. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64911/0.74447. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64737/0.74010. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64667/0.74171. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64269/0.74378. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64120/0.74434. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63924/0.74781. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63893/0.75122. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63879/0.74962. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63948/0.75283. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63796/0.75012. Took 0.19 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69525/0.69034. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69350/0.69107. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69311/0.69111. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69269/0.69097. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69254/0.69091. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69229/0.69067. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69150/0.69058. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69142/0.69057. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69195/0.69037. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69079/0.68981. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69040/0.68946. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69022/0.68922. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68967/0.68884. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68981/0.68838. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68908/0.68833. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68897/0.68817. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68822/0.68742. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68805/0.68702. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68722/0.68646. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68624/0.68678. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68655/0.68666. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68554/0.68594. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68545/0.68559. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68538/0.68520. Took 0.23 sec\n",
      "Epoch 24, Loss(train/val) 0.68385/0.68493. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68449/0.68465. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68114/0.68487. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68088/0.68417. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.68277/0.68267. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68201/0.68315. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67916/0.68179. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68005/0.68207. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67749/0.68179. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67629/0.68122. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67637/0.68059. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67484/0.68027. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.67512/0.68019. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67143/0.67926. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.67048/0.67899. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66855/0.67800. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67054/0.67837. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66868/0.67980. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66736/0.67975. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66719/0.67819. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66529/0.67759. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66307/0.67731. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.66137/0.67686. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66047/0.67800. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66163/0.67908. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66193/0.67694. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65851/0.67781. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65734/0.67810. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65428/0.67924. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.65416/0.68011. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65520/0.67945. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.65142/0.68079. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65063/0.68023. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64833/0.68321. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65055/0.68026. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.64594/0.68028. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.64674/0.68036. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64633/0.68099. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64471/0.68059. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.64117/0.68279. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64016/0.68348. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.64187/0.68082. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63551/0.68523. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63956/0.68237. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.63783/0.68035. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.63318/0.68214. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64039/0.68204. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63812/0.68184. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.63147/0.68223. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.63533/0.68452. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.63258/0.68136. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62771/0.68254. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62712/0.68282. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63046/0.68304. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62493/0.68609. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62466/0.68594. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62032/0.68886. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61778/0.68838. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61958/0.68593. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.62222/0.68801. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62229/0.68782. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61522/0.69038. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61816/0.69400. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61408/0.69026. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61547/0.68934. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61032/0.69158. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61469/0.69494. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.61224/0.69806. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60899/0.69620. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60796/0.70014. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60845/0.69857. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60223/0.69843. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60096/0.70528. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.59675/0.70538. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60586/0.70736. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.59870/0.70672. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69307/0.69378. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69294/0.69392. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69295/0.69404. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69270/0.69414. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69217/0.69426. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69260/0.69435. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69230/0.69446. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69210/0.69457. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69214/0.69471. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69191/0.69484. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69226/0.69496. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69195/0.69508. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69165/0.69522. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69175/0.69542. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69102/0.69561. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69076/0.69592. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69008/0.69611. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69048/0.69634. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69001/0.69668. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68950/0.69694. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68944/0.69725. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68868/0.69762. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68844/0.69788. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68929/0.69832. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68783/0.69869. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68783/0.69900. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68762/0.69940. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68639/0.69982. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68535/0.70053. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68470/0.70157. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68521/0.70160. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68468/0.70233. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68321/0.70291. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68319/0.70327. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68264/0.70447. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68227/0.70564. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67999/0.70577. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68141/0.70703. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68079/0.70837. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68038/0.71039. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67969/0.71023. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68025/0.71029. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67905/0.71141. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67773/0.71056. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67717/0.71151. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67481/0.71182. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67423/0.71285. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67690/0.71306. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67676/0.71251. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67442/0.71291. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67504/0.71295. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67274/0.71517. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67064/0.71704. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67187/0.71676. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67277/0.71665. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66893/0.71689. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67021/0.71849. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66870/0.71893. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66807/0.71857. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66800/0.71824. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66940/0.71908. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.66766/0.71921. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66789/0.71965. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66454/0.72107. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66338/0.72346. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66664/0.72365. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66132/0.72374. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66369/0.72415. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66301/0.72546. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66010/0.72737. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66049/0.72723. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66066/0.72807. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65776/0.72884. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65766/0.72990. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66010/0.73186. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65947/0.73199. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65620/0.73123. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65764/0.73357. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65524/0.73388. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65453/0.73460. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65693/0.73638. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65091/0.73647. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65275/0.73595. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64967/0.73708. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64930/0.73906. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64978/0.74164. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64868/0.74126. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64769/0.74391. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64286/0.74476. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64556/0.74709. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64472/0.74533. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64453/0.74608. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64181/0.74697. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64457/0.74694. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64644/0.74477. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64020/0.74677. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64072/0.75006. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63902/0.74990. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63913/0.75059. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63778/0.75204. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69364/0.69592. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69257/0.69628. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69318/0.69630. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69187/0.69679. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69225/0.69736. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69208/0.69800. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69154/0.69873. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69179/0.69926. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69140/0.69951. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69152/0.70016. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69172/0.70023. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69096/0.70088. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69134/0.70074. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69045/0.70172. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69108/0.70161. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69006/0.70182. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68986/0.70234. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68903/0.70286. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.69004/0.70356. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68867/0.70325. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68924/0.70314. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68918/0.70396. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68806/0.70509. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68924/0.70607. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68834/0.70506. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68819/0.70612. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68857/0.70639. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68740/0.70736. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68616/0.70760. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68801/0.70770. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68706/0.70817. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68783/0.70888. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68570/0.70889. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68674/0.71008. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68621/0.71067. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68439/0.71025. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68691/0.71132. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68569/0.71146. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68600/0.71159. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68509/0.71295. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68344/0.71429. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68478/0.71381. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68442/0.71505. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68435/0.71594. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68331/0.71715. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68281/0.71759. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68114/0.71820. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68334/0.71871. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67930/0.71867. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68031/0.71917. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68036/0.71929. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.68009/0.72000. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67977/0.71977. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67863/0.72063. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67906/0.72031. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67853/0.72191. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67750/0.72047. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67579/0.72244. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67600/0.72431. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67583/0.72357. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67656/0.72704. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67393/0.72603. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67553/0.72657. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67494/0.72647. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67234/0.72804. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67298/0.72849. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67102/0.72805. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66985/0.73046. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66913/0.73212. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67079/0.73120. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66923/0.73095. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66550/0.73259. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66677/0.73311. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66675/0.73397. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66789/0.73386. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66603/0.73557. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66534/0.73506. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.66129/0.73571. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66330/0.73631. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65967/0.73695. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66214/0.73806. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66037/0.73837. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.65933/0.73976. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65941/0.73942. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65557/0.73928. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65619/0.74188. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65644/0.74226. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65317/0.73886. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65583/0.74130. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65439/0.74709. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.65198/0.74416. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64935/0.74652. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.65047/0.74877. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64885/0.74991. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65045/0.74995. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64223/0.74373. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64895/0.74787. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64136/0.75082. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64165/0.75271. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64292/0.75248. Took 0.20 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69400/0.69868. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69288/0.69912. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69223/0.69877. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69205/0.69858. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69168/0.69859. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69110/0.69867. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69143/0.69814. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69039/0.69767. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69075/0.69728. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69034/0.69685. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69045/0.69659. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69020/0.69652. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68956/0.69624. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68963/0.69600. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68955/0.69623. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68922/0.69570. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68869/0.69545. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68821/0.69539. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 0.68828/0.69510. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68820/0.69545. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68720/0.69558. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68805/0.69584. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68715/0.69582. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68745/0.69550. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68716/0.69530. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68663/0.69523. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68617/0.69520. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68693/0.69533. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68699/0.69622. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68623/0.69611. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68653/0.69555. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68511/0.69645. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68645/0.69622. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68419/0.69636. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68442/0.69750. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68386/0.69709. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68301/0.69682. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68398/0.69742. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68273/0.69826. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68305/0.69858. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68122/0.69892. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68169/0.69926. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.68111/0.69940. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67951/0.69988. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67929/0.70078. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67727/0.70117. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67848/0.70146. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67599/0.70210. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67758/0.70188. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67609/0.70187. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67747/0.70395. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67569/0.70443. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67333/0.70610. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67364/0.70703. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67247/0.70650. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67334/0.70716. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67358/0.70842. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67125/0.70858. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67120/0.71035. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67172/0.71070. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67102/0.71119. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66993/0.71073. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66999/0.71183. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66860/0.71252. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66596/0.71249. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66861/0.71363. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66568/0.71538. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66528/0.71560. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66494/0.71686. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66505/0.71496. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66495/0.71719. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66213/0.71683. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66240/0.72047. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66008/0.71979. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65991/0.72044. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66253/0.72100. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65832/0.72116. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65733/0.72332. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65689/0.72488. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65494/0.72430. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65686/0.72293. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65546/0.72573. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65477/0.72741. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65441/0.72837. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65204/0.72734. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65674/0.72810. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65016/0.73049. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.64880/0.73039. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64827/0.72903. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.64731/0.73203. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64711/0.73594. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64703/0.73412. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64548/0.73504. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64751/0.73636. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64695/0.73567. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64219/0.73373. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64313/0.73664. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64026/0.73694. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63461/0.73761. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.63848/0.73953. Took 0.20 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69547/0.68979. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69377/0.68760. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69312/0.68750. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69289/0.68792. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69229/0.68830. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69185/0.68882. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69086/0.68911. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69062/0.68956. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68996/0.69004. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68975/0.69037. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68936/0.69077. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68985/0.69139. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68906/0.69203. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68884/0.69291. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68861/0.69339. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68761/0.69386. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68751/0.69433. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68739/0.69518. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68665/0.69584. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68530/0.69623. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68579/0.69729. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68521/0.69790. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68421/0.69810. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68380/0.69811. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68402/0.69887. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68368/0.69916. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68425/0.69947. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68314/0.70053. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68226/0.70119. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68177/0.70137. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68001/0.70143. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68055/0.70219. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67893/0.70290. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67850/0.70204. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67779/0.70317. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67780/0.70270. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67663/0.70317. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67493/0.70605. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67399/0.70434. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67431/0.70368. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67356/0.70413. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67068/0.70482. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67187/0.70414. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67098/0.70361. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67039/0.70125. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66692/0.70270. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66778/0.70247. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66643/0.70290. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66287/0.70359. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66231/0.70353. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66279/0.70613. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66199/0.70394. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66020/0.70419. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66030/0.70413. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66054/0.70386. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65873/0.70456. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65514/0.70309. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65327/0.70366. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65160/0.70297. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65153/0.70162. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65333/0.70431. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65129/0.70323. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64741/0.70482. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64641/0.70421. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64367/0.70355. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.64637/0.70510. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64225/0.70452. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64147/0.70380. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64574/0.70264. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63769/0.70248. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64165/0.70174. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63981/0.70253. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63840/0.70361. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63576/0.70572. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63554/0.70779. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.63516/0.70812. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63265/0.70510. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63471/0.70787. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62989/0.71008. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63129/0.70957. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62869/0.70942. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.62543/0.70883. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62611/0.71070. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62699/0.71169. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62311/0.71257. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62556/0.71494. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62428/0.71621. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62210/0.71657. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61889/0.71751. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61712/0.71680. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61255/0.71809. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.61888/0.71735. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61621/0.71865. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.61268/0.71899. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61386/0.71887. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61195/0.71972. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61303/0.72157. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61122/0.72351. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60557/0.72518. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.60721/0.72477. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69624/0.69859. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69308/0.69889. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69119/0.69771. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69050/0.69685. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68981/0.69591. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68907/0.69501. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68770/0.69438. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68776/0.69352. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68648/0.69319. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68694/0.69261. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68686/0.69222. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68526/0.69212. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68471/0.69233. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68385/0.69249. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68402/0.69225. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68429/0.69198. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68220/0.69210. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68371/0.69204. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68270/0.69234. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68367/0.69248. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68274/0.69320. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68209/0.69364. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68242/0.69379. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68210/0.69396. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68149/0.69432. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68129/0.69561. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68072/0.69483. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68100/0.69546. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67943/0.69562. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67882/0.69623. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67997/0.69621. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67801/0.69708. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67834/0.69702. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67663/0.69656. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67617/0.69738. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67625/0.69748. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67515/0.69768. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67526/0.69797. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67504/0.69806. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67534/0.69857. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67390/0.69883. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67342/0.69958. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67121/0.69864. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67265/0.69887. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67167/0.69900. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67029/0.69888. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66908/0.69984. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66963/0.69984. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66770/0.69992. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66754/0.69960. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66551/0.70067. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.66367/0.70083. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66619/0.70115. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.66298/0.70263. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66598/0.70276. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66264/0.70311. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66308/0.70408. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66062/0.70470. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65691/0.70614. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65774/0.70582. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66033/0.70777. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65613/0.70935. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.65561/0.70924. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.65332/0.71079. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65481/0.71191. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65086/0.71334. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65063/0.71299. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.64712/0.71415. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64998/0.71374. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64498/0.71502. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.64372/0.71526. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64321/0.71708. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64114/0.71859. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.63918/0.72380. Took 0.21 sec\n",
      "Epoch 74, Loss(train/val) 0.63682/0.72511. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64239/0.72507. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63870/0.72657. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63546/0.72742. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63601/0.72845. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.63500/0.72936. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.62911/0.73207. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.62912/0.73532. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.62854/0.73656. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62635/0.73502. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62855/0.73939. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62397/0.74146. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 0.62268/0.74182. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62024/0.74165. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.61863/0.74630. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62046/0.74737. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.61615/0.74781. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61453/0.75529. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.61506/0.75372. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61660/0.75642. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.61610/0.75930. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61143/0.75899. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.61059/0.76017. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60567/0.76380. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.61140/0.75989. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.60552/0.76241. Took 0.20 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69430/0.68998. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69256/0.69144. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69138/0.69131. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69008/0.69136. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68968/0.69150. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68845/0.69166. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68791/0.69165. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68837/0.69172. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68660/0.69210. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68704/0.69248. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68633/0.69301. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68620/0.69335. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68580/0.69391. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68621/0.69404. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68564/0.69444. Took 0.22 sec\n",
      "Epoch 15, Loss(train/val) 0.68477/0.69452. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68526/0.69496. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68451/0.69539. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68482/0.69615. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68503/0.69588. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68413/0.69590. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68386/0.69692. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68396/0.69712. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68335/0.69754. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68266/0.69803. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68198/0.69858. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68198/0.69889. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68247/0.69927. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68164/0.69958. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68234/0.69990. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68078/0.69999. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67931/0.70080. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68094/0.70147. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67950/0.70115. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67931/0.70160. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67932/0.70204. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67995/0.70206. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67939/0.70224. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67688/0.70237. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67600/0.70231. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67681/0.70266. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67647/0.70257. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67612/0.70367. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67696/0.70427. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67445/0.70469. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67489/0.70471. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67493/0.70546. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67256/0.70512. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67460/0.70404. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67295/0.70550. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66930/0.70498. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67166/0.70582. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67088/0.70615. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67128/0.70642. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66947/0.70780. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67046/0.70780. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66963/0.70840. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66984/0.70559. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66851/0.70786. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66548/0.70927. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66454/0.70763. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66644/0.71112. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66439/0.70894. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66558/0.71094. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66123/0.71255. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66431/0.71029. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66357/0.71065. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66146/0.71235. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66102/0.71185. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66205/0.71288. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66294/0.71384. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66056/0.71226. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66051/0.71352. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65892/0.71393. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65543/0.71565. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65595/0.71185. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65792/0.71325. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65491/0.71431. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65449/0.71328. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65496/0.71360. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65261/0.71247. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65013/0.71499. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64849/0.71415. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 0.65026/0.71461. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65066/0.71505. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64658/0.71718. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65072/0.71541. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64596/0.71459. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64659/0.71444. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63882/0.71604. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64003/0.71475. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63992/0.71625. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64211/0.71616. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63807/0.71720. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63597/0.71554. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63482/0.71478. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62917/0.71880. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62788/0.71976. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63099/0.72247. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62885/0.71696. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69317/0.70104. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.69251/0.69948. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69225/0.69845. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69180/0.69781. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69121/0.69757. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69101/0.69739. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68983/0.69746. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69031/0.69764. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68973/0.69781. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68886/0.69787. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68865/0.69856. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68840/0.69933. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68739/0.69962. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68755/0.70047. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68654/0.70151. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68679/0.70183. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68630/0.70204. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68582/0.70317. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68569/0.70338. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68542/0.70371. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68590/0.70441. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68465/0.70427. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68398/0.70410. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68457/0.70454. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68292/0.70520. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68265/0.70481. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68298/0.70467. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68249/0.70477. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68200/0.70473. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68069/0.70413. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67933/0.70289. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67950/0.70212. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67951/0.70140. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67924/0.70091. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67734/0.70005. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67839/0.69864. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67570/0.69862. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67259/0.69749. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67386/0.69631. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67302/0.69494. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67183/0.69378. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67044/0.69355. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66833/0.69090. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66577/0.69199. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66358/0.68832. Took 0.22 sec\n",
      "Epoch 45, Loss(train/val) 0.66361/0.68813. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.66299/0.68797. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66278/0.68770. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.65746/0.68296. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65711/0.68420. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65557/0.68160. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65572/0.67893. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 0.65085/0.67797. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65272/0.67834. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65019/0.67719. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64850/0.67811. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64769/0.67557. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64903/0.67576. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64334/0.67086. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64760/0.67298. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.64423/0.67116. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.63998/0.67413. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.63918/0.66966. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.64000/0.66705. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.63696/0.66649. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.63875/0.66256. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.63873/0.66845. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63223/0.66869. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.63283/0.66722. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.63201/0.66775. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.62984/0.66794. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.62902/0.66977. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62644/0.66578. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.62569/0.67017. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62792/0.66040. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62471/0.66094. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.62256/0.65814. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.62092/0.66127. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62098/0.66242. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.61859/0.66121. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.61643/0.65974. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61709/0.65847. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.60883/0.65964. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61198/0.66231. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.61263/0.66108. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61016/0.66220. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.60785/0.66293. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61108/0.65888. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.60695/0.66074. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.60771/0.65933. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60414/0.66017. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.60471/0.65712. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.60455/0.66489. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.60128/0.65761. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.60204/0.65628. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.59834/0.66045. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59366/0.65766. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.59082/0.65955. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.59438/0.66112. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59288/0.66181. Took 0.20 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69521/0.68837. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69307/0.68933. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69251/0.68931. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69187/0.68972. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69147/0.68997. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69180/0.69022. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69078/0.69059. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69063/0.69108. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68954/0.69109. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68962/0.69151. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68937/0.69194. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69002/0.69250. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68924/0.69289. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68909/0.69335. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68849/0.69349. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68849/0.69385. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68833/0.69403. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68745/0.69462. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68800/0.69511. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68777/0.69557. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68706/0.69613. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68784/0.69648. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68683/0.69689. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68600/0.69739. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68701/0.69759. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68573/0.69780. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68522/0.69796. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68590/0.69893. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68468/0.69871. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68402/0.69906. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68421/0.69915. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68367/0.69936. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68320/0.69957. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68102/0.70000. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68191/0.69975. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68219/0.70042. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68046/0.70044. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 0.68065/0.70037. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68039/0.70053. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68050/0.70095. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67950/0.70091. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68051/0.70128. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67893/0.70166. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67865/0.70133. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67732/0.70231. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67817/0.70237. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67496/0.70284. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67505/0.70290. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67461/0.70324. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67469/0.70416. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67541/0.70401. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67284/0.70420. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67188/0.70462. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67231/0.70455. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67002/0.70555. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67361/0.70458. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66998/0.70504. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66812/0.70674. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66965/0.70717. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66659/0.70703. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66983/0.70778. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66790/0.70672. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66725/0.70913. Took 0.22 sec\n",
      "Epoch 63, Loss(train/val) 0.66393/0.70999. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66585/0.70926. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66510/0.71004. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66324/0.71115. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66378/0.71135. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66084/0.71193. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65921/0.71232. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66142/0.71167. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65972/0.71292. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65819/0.71173. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66066/0.71210. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65866/0.71257. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65543/0.71241. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65728/0.71211. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66115/0.71213. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65828/0.71061. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65509/0.71312. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65583/0.71214. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65445/0.71269. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65548/0.71307. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65141/0.71485. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65013/0.71465. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65049/0.71413. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65170/0.71400. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64810/0.71523. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64939/0.71701. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.64863/0.71643. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64715/0.71604. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64351/0.71654. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64485/0.71570. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64557/0.71840. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64605/0.71743. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64233/0.71809. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64417/0.72106. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.64291/0.72179. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64172/0.72133. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63684/0.72004. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69458/0.69551. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69356/0.69429. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69278/0.69388. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.69341/0.69380. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69202/0.69392. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69213/0.69366. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69215/0.69380. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69194/0.69395. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69217/0.69445. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69136/0.69503. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69174/0.69534. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.69093/0.69536. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69030/0.69545. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69038/0.69582. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69091/0.69606. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69005/0.69630. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68904/0.69647. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68954/0.69690. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68977/0.69678. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68934/0.69696. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68896/0.69701. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68916/0.69709. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68810/0.69729. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68759/0.69752. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68863/0.69732. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68819/0.69772. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68822/0.69812. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68727/0.69810. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68664/0.69848. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68777/0.69835. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68740/0.69869. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68518/0.69866. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68579/0.69875. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68694/0.69899. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68557/0.69874. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68577/0.69871. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68470/0.69900. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68248/0.69927. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68402/0.69935. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68183/0.69863. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68172/0.69924. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68229/0.69830. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68220/0.69976. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68228/0.69886. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68247/0.69935. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68108/0.69968. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68050/0.69958. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68102/0.69957. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67971/0.70018. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68019/0.70006. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67992/0.69971. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67984/0.69963. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67878/0.70051. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67867/0.69864. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67697/0.69931. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67837/0.69957. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67811/0.69932. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67429/0.69956. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67698/0.70089. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67671/0.69942. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67618/0.70025. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67603/0.69995. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67416/0.70015. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67421/0.69924. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67411/0.70057. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67439/0.69985. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67074/0.69939. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67153/0.70075. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67063/0.69977. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67160/0.70035. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67175/0.69954. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67059/0.69980. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67092/0.69996. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66927/0.69920. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66688/0.69882. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67019/0.69983. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66585/0.69930. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66746/0.69858. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66519/0.69900. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66394/0.70020. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66495/0.69976. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66466/0.70101. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66602/0.70021. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66441/0.69957. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66432/0.69925. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66553/0.70028. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66363/0.69999. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66252/0.70099. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66215/0.70054. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66205/0.70105. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66096/0.70041. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66563/0.70191. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66029/0.70110. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66159/0.70236. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65786/0.70081. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66065/0.69968. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65815/0.69891. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65691/0.70341. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65692/0.70147. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.65357/0.69896. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69363/0.69224. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69259/0.69213. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69271/0.69188. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69209/0.69173. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69199/0.69166. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69169/0.69146. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69075/0.69124. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69109/0.69103. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69071/0.69083. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69085/0.69076. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69000/0.69043. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68942/0.69030. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68880/0.69016. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68816/0.68991. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68789/0.68971. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68758/0.68953. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68691/0.68965. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68584/0.68953. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68604/0.68919. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68688/0.68918. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68551/0.68934. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68533/0.68898. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68379/0.68892. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68440/0.68866. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68381/0.68855. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68284/0.68854. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68301/0.68833. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68223/0.68834. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68243/0.68808. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68240/0.68799. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68077/0.68808. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68031/0.68809. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67953/0.68792. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68031/0.68811. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67986/0.68813. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67845/0.68827. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67913/0.68836. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67671/0.68844. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67702/0.68857. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67792/0.68872. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67674/0.68910. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67508/0.68966. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67498/0.69020. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67456/0.69111. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67375/0.69233. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67302/0.69265. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67321/0.69284. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67339/0.69332. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67104/0.69406. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67005/0.69427. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67049/0.69577. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66864/0.69722. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66582/0.69780. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66794/0.69719. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66542/0.69971. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66570/0.70079. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66482/0.70382. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66594/0.70377. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66324/0.70538. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66415/0.70579. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66243/0.70814. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66291/0.70768. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66200/0.70993. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66166/0.70973. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66165/0.71193. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65891/0.71185. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65802/0.71506. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65550/0.71540. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65963/0.71573. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65818/0.71539. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65727/0.71510. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65716/0.71717. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65337/0.71997. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64933/0.71930. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65331/0.72002. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65307/0.71921. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65235/0.72158. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65203/0.72428. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65155/0.72648. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65305/0.72717. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65108/0.72650. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64656/0.72828. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64459/0.73126. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64154/0.73408. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64627/0.73545. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64580/0.73304. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64465/0.73449. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64659/0.73662. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63992/0.73705. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64079/0.73626. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63618/0.73912. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64074/0.73784. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63981/0.74106. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.64068/0.74027. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63779/0.74300. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63273/0.74471. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63872/0.74474. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63324/0.74521. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63440/0.74288. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63383/0.74404. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69267/0.67417. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69215/0.67567. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69191/0.67668. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69220/0.67727. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69121/0.67725. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69094/0.67786. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69090/0.67780. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69059/0.67796. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68963/0.67864. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68840/0.67888. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68832/0.67952. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68775/0.67948. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68733/0.67954. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68618/0.67936. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68566/0.67971. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68631/0.68072. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68562/0.67991. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68492/0.68133. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68479/0.68223. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68387/0.68226. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68364/0.68388. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68446/0.68241. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68158/0.68498. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68235/0.68609. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68221/0.68613. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68162/0.68675. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68035/0.68806. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68127/0.68790. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68137/0.68870. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68054/0.68841. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67948/0.68959. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67950/0.69012. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67714/0.69080. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67785/0.69463. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67790/0.69266. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67731/0.69462. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67706/0.69451. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67601/0.69628. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67711/0.69642. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67745/0.69802. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67544/0.70101. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67413/0.69985. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67472/0.70138. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67376/0.70196. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67271/0.70109. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67406/0.70353. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67251/0.70448. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67126/0.70401. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67039/0.70669. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67088/0.70779. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66977/0.70565. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66801/0.70852. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66961/0.70839. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66798/0.70851. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67022/0.71007. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66681/0.71117. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66751/0.71078. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66687/0.71176. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66668/0.71414. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66661/0.71584. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66401/0.71501. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66605/0.71692. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66422/0.71636. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66486/0.71365. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66518/0.71465. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66295/0.71663. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66201/0.71959. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66011/0.72138. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66262/0.72351. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66278/0.72033. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66076/0.72517. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66128/0.72323. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66043/0.72472. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65892/0.72550. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65933/0.72684. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66011/0.72906. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65908/0.73172. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65876/0.72912. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65696/0.72819. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65663/0.73072. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65731/0.73210. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65498/0.73535. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65633/0.73551. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65523/0.73330. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65378/0.73578. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65196/0.73541. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65274/0.73544. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65261/0.73804. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65242/0.73737. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65420/0.73715. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65441/0.74058. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65103/0.74132. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65117/0.74348. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65068/0.74189. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64937/0.74057. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64917/0.74354. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64893/0.74438. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65027/0.74058. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65046/0.74823. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64779/0.74312. Took 0.19 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69008/0.69203. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68855/0.69071. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68750/0.69068. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.68698/0.69047. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68654/0.69021. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68565/0.69030. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68413/0.69005. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68467/0.69003. Took 0.22 sec\n",
      "Epoch 8, Loss(train/val) 0.68360/0.68995. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68312/0.68957. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68250/0.68919. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68275/0.68925. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68175/0.68900. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68150/0.68873. Took 0.22 sec\n",
      "Epoch 14, Loss(train/val) 0.68120/0.68848. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68025/0.68798. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 0.67980/0.68759. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68022/0.68690. Took 0.23 sec\n",
      "Epoch 18, Loss(train/val) 0.67903/0.68668. Took 0.22 sec\n",
      "Epoch 19, Loss(train/val) 0.67867/0.68622. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.67908/0.68572. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67703/0.68480. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.67717/0.68459. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.67671/0.68405. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.67686/0.68352. Took 0.22 sec\n",
      "Epoch 25, Loss(train/val) 0.67632/0.68292. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67564/0.68194. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67493/0.68137. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67475/0.68074. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67496/0.67985. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67367/0.67917. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67217/0.67839. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67266/0.67762. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.67203/0.67714. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67183/0.67646. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67321/0.67599. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67128/0.67537. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.66863/0.67485. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66965/0.67415. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66943/0.67387. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66812/0.67260. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.66669/0.67158. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66723/0.67218. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66737/0.67256. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66682/0.67201. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66392/0.67136. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66522/0.67148. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66330/0.67048. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66399/0.67033. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66416/0.67023. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66282/0.66992. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66112/0.66885. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66105/0.66858. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.65635/0.66768. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65908/0.66858. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65691/0.67014. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65605/0.66800. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65677/0.66785. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65749/0.66828. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65333/0.66820. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65260/0.66737. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65319/0.66845. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65036/0.66746. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64888/0.66856. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65042/0.66900. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.64958/0.66724. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64638/0.67025. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64767/0.66826. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64627/0.66986. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64631/0.66717. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64360/0.66847. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64166/0.67034. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64246/0.66926. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64061/0.66881. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63902/0.66919. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64122/0.67079. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64059/0.67074. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63811/0.66821. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64120/0.66981. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63942/0.67054. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63600/0.67055. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63444/0.67114. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63692/0.66977. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63494/0.67135. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63669/0.67191. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63309/0.67272. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63602/0.67538. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62913/0.67282. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63076/0.67097. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62749/0.67199. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63135/0.67406. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63181/0.67245. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62826/0.67336. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.62823/0.67421. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62847/0.67217. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63303/0.67555. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62523/0.68103. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62376/0.67743. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62382/0.67738. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62790/0.67770. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69018/0.70291. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68803/0.70781. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68796/0.70954. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68766/0.70979. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68644/0.71009. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68603/0.70999. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68543/0.71019. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68548/0.71052. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68463/0.71079. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68415/0.71106. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68378/0.71115. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68366/0.71056. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68305/0.71052. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68305/0.71104. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68239/0.71146. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68217/0.71142. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68125/0.71114. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68155/0.71099. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68090/0.71166. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68118/0.71127. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68061/0.71084. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67908/0.71054. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67929/0.71117. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67905/0.71155. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67866/0.71162. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67922/0.71089. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67760/0.71102. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67725/0.71252. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67703/0.71115. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67720/0.71191. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67676/0.71255. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67646/0.71200. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67567/0.71192. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67603/0.71380. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67441/0.71378. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67455/0.71300. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67353/0.71237. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67231/0.71330. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67244/0.71313. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67119/0.71370. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67346/0.71266. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67046/0.71402. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66996/0.71514. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66997/0.71451. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66957/0.71532. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66791/0.71585. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66894/0.71741. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66783/0.71805. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66789/0.71674. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66815/0.71793. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66748/0.71828. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66673/0.71968. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66571/0.71966. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66594/0.72016. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66557/0.72138. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66426/0.72139. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66467/0.72102. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66344/0.72114. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66296/0.72094. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66106/0.72311. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66253/0.72384. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66017/0.72167. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65954/0.72313. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65939/0.72356. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65776/0.72380. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65844/0.72593. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65788/0.72292. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65608/0.72664. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65558/0.72580. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65496/0.72694. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65438/0.72492. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65327/0.72710. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65295/0.72666. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65110/0.72640. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65027/0.72696. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64955/0.72612. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64983/0.72636. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65126/0.72427. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64564/0.72308. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64800/0.72521. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64325/0.72496. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64535/0.72544. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64683/0.72169. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64215/0.72106. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64191/0.72356. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64140/0.72057. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63970/0.72170. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63724/0.72355. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63750/0.71949. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63638/0.72346. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63643/0.72149. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63444/0.72062. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63357/0.72258. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63240/0.72211. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63030/0.72027. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63160/0.72348. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62972/0.72005. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63307/0.71642. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62853/0.72340. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63231/0.71554. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69204/0.69273. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69080/0.69351. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68983/0.69454. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68908/0.69548. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68893/0.69626. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68915/0.69677. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68787/0.69744. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68798/0.69816. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68875/0.69848. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68867/0.69876. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68689/0.69908. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68678/0.69935. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68708/0.69969. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68683/0.69987. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68613/0.70040. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68589/0.70086. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68522/0.70148. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68590/0.70194. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68409/0.70237. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68325/0.70266. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68330/0.70309. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68338/0.70349. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68333/0.70421. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68238/0.70434. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68391/0.70404. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68175/0.70413. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68268/0.70460. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68239/0.70487. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68041/0.70517. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68065/0.70518. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67898/0.70537. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67817/0.70575. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67983/0.70552. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67933/0.70562. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67743/0.70642. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67779/0.70713. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67624/0.70716. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67595/0.70702. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67677/0.70763. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67779/0.70749. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67741/0.70730. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67516/0.70766. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67578/0.70771. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67434/0.70829. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67360/0.70862. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67324/0.70836. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67179/0.70850. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67165/0.70887. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67215/0.70825. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67106/0.70886. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67476/0.70921. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67146/0.70836. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66828/0.70778. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67232/0.70816. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66905/0.70859. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67052/0.70889. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66894/0.70748. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66967/0.70788. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66856/0.70774. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66479/0.70903. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66538/0.71022. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66477/0.71101. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66129/0.71010. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66326/0.71086. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66049/0.71185. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66249/0.71134. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66563/0.71327. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66164/0.70995. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66035/0.70979. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66157/0.71108. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66065/0.71153. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65865/0.71102. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65823/0.71217. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65615/0.71257. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65669/0.71043. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65648/0.71174. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65593/0.71334. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65216/0.71520. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65216/0.71271. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65107/0.71399. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65188/0.71506. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65188/0.71365. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64999/0.71634. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64803/0.71654. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64677/0.71583. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64981/0.71593. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64776/0.71438. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64665/0.71608. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64994/0.71875. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64652/0.71931. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.64694/0.71689. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64349/0.72010. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.64579/0.71762. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63947/0.71282. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64199/0.71790. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64241/0.71688. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63622/0.71732. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63716/0.71962. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63566/0.72057. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63705/0.72458. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69289/0.69907. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.68985/0.70187. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68911/0.70292. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68922/0.70336. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68856/0.70361. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68866/0.70393. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68815/0.70437. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68801/0.70470. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68746/0.70552. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68798/0.70597. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68675/0.70636. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68651/0.70675. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68684/0.70712. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68684/0.70780. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68603/0.70813. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68557/0.70850. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68611/0.70875. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68547/0.70907. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68543/0.70992. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68453/0.71002. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68372/0.71048. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68488/0.71092. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68471/0.71100. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68345/0.71173. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68370/0.71189. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68282/0.71257. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 0.68319/0.71269. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68262/0.71376. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68224/0.71376. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68080/0.71431. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.68055/0.71524. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68104/0.71588. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 0.68018/0.71646. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67939/0.71639. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68064/0.71702. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67973/0.71756. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.67863/0.71809. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67947/0.71912. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.67824/0.71974. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67836/0.72064. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67754/0.72143. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67644/0.72231. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67520/0.72290. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67740/0.72318. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67565/0.72373. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67576/0.72486. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.67421/0.72535. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67616/0.72641. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67513/0.72601. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67397/0.72710. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67425/0.72701. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67260/0.72776. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67267/0.72816. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67177/0.72815. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.67214/0.72766. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67233/0.72843. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67070/0.72942. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66988/0.73026. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67145/0.72847. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66904/0.73039. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67054/0.73098. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66746/0.73142. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.67007/0.73320. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66704/0.73267. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66655/0.73338. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66430/0.73438. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66712/0.73548. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66482/0.73559. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66604/0.73519. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66477/0.73853. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66403/0.73918. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66717/0.73919. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66409/0.74095. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66596/0.73860. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66195/0.73872. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66237/0.74341. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66140/0.74115. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66078/0.74261. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66262/0.74353. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66103/0.74398. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.66025/0.74608. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65989/0.74656. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65958/0.74690. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.66017/0.74582. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65818/0.74819. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.66048/0.74883. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65893/0.74770. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65720/0.74636. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65733/0.74895. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65719/0.74834. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65165/0.75152. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65400/0.74920. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65249/0.75155. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65369/0.75294. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65449/0.75480. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65070/0.75571. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65248/0.75542. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.65129/0.75735. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65334/0.75448. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65065/0.75506. Took 0.20 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69192/0.69475. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69127/0.69460. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69068/0.69448. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.69054/0.69426. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69023/0.69404. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69007/0.69388. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68912/0.69381. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68981/0.69375. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68898/0.69386. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68877/0.69389. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68832/0.69389. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68919/0.69383. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68814/0.69403. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68751/0.69417. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68701/0.69446. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68692/0.69469. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68704/0.69501. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68629/0.69521. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68527/0.69560. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68505/0.69579. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68500/0.69594. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68325/0.69605. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68354/0.69590. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68302/0.69574. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68209/0.69612. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68077/0.69658. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67944/0.69557. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68009/0.69529. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67837/0.69563. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67612/0.69456. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67421/0.69383. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67313/0.69383. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67365/0.69326. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67343/0.69265. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67082/0.69178. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66807/0.69150. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.66586/0.69092. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66679/0.69077. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66360/0.69123. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66364/0.69048. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66092/0.69030. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.66201/0.68855. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66195/0.69030. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.65822/0.69098. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65851/0.69001. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65656/0.69032. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65542/0.69028. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65367/0.69165. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.65139/0.69073. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65241/0.69037. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.65347/0.69131. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.64951/0.69187. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65204/0.69052. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65159/0.69216. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64811/0.69212. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.64685/0.69280. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64596/0.69386. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64665/0.69565. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64549/0.69529. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64555/0.69524. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64497/0.69573. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64343/0.69539. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64271/0.69689. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.63946/0.69528. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63971/0.69470. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64089/0.69538. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63899/0.69619. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63629/0.69413. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63718/0.69467. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.63516/0.69525. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.63319/0.69819. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63464/0.69631. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63420/0.69869. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.63335/0.70200. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62861/0.69735. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62719/0.70170. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63274/0.70000. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.62691/0.69969. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63117/0.69887. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62903/0.69864. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62280/0.69870. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62589/0.69893. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62514/0.70074. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62196/0.69750. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62104/0.69983. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62063/0.70022. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61849/0.70219. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61390/0.69864. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61456/0.70030. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62231/0.69812. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61462/0.70431. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61298/0.70795. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61418/0.70418. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.61086/0.70502. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61142/0.70563. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60788/0.70693. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60801/0.71192. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60770/0.71134. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60497/0.70689. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60509/0.70852. Took 0.18 sec\n",
      "ACC: 0.5833333333333334\n",
      "Epoch 0, Loss(train/val) 0.69488/0.69443. Took 0.34 sec\n",
      "Epoch 1, Loss(train/val) 0.69306/0.69269. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69283/0.69212. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69210/0.69177. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69157/0.69153. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69109/0.69128. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69154/0.69125. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69153/0.69105. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69032/0.69087. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68996/0.69086. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68878/0.69062. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68908/0.69051. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68865/0.69050. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68849/0.69047. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68846/0.69058. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68772/0.69059. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68740/0.69067. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68664/0.69084. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68635/0.69076. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68629/0.69086. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68718/0.69078. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68569/0.69057. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68572/0.69064. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68538/0.69050. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68401/0.69044. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68484/0.69034. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68401/0.69051. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68306/0.69060. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68346/0.69073. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68407/0.69086. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68394/0.69084. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68216/0.69085. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68182/0.69086. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68174/0.69081. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68295/0.69056. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68204/0.69055. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68007/0.69057. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68016/0.69034. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.67970/0.69042. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67879/0.69042. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67853/0.69029. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68012/0.69031. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67992/0.69033. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67836/0.69016. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 0.67764/0.68980. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67643/0.69005. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67705/0.69041. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67647/0.69034. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67827/0.69025. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67536/0.69039. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67611/0.69035. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67519/0.69052. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67371/0.69071. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67491/0.69059. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67403/0.69050. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67365/0.69104. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67428/0.69101. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67256/0.69118. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67117/0.69081. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66894/0.69085. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66943/0.69195. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67077/0.69197. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66915/0.69226. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66859/0.69254. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66801/0.69287. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66683/0.69344. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66683/0.69251. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66518/0.69246. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66477/0.69280. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66076/0.69314. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66157/0.69408. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65976/0.69439. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66149/0.69389. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65948/0.69472. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65869/0.69488. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65842/0.69317. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65722/0.69524. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65531/0.69446. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65746/0.69591. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65620/0.69565. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65357/0.69561. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65053/0.69738. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65428/0.69798. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.64613/0.69725. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65158/0.69663. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64740/0.69738. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64598/0.69873. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64156/0.69961. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64614/0.70178. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64211/0.70023. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64336/0.70212. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.64296/0.70129. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63957/0.70078. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.63737/0.70233. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.63818/0.70543. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64180/0.70348. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63580/0.70234. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63239/0.70543. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63731/0.70352. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.62885/0.70448. Took 0.19 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69423/0.69550. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69274/0.69380. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69259/0.69269. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69241/0.69206. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69204/0.69169. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69294/0.69141. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69181/0.69092. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69147/0.69037. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69205/0.69013. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69103/0.69001. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69115/0.68982. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69047/0.68956. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.69028/0.68932. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.69048/0.68905. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.69002/0.68881. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.69050/0.68860. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68866/0.68837. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68932/0.68790. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68932/0.68794. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68845/0.68750. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68826/0.68731. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68804/0.68693. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68828/0.68714. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68723/0.68687. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68691/0.68656. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68792/0.68667. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68642/0.68606. Took 0.22 sec\n",
      "Epoch 27, Loss(train/val) 0.68606/0.68613. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68568/0.68615. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68539/0.68654. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68426/0.68614. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68498/0.68609. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68529/0.68602. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68560/0.68617. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68436/0.68653. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68484/0.68662. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68234/0.68733. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68307/0.68754. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68316/0.68800. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68191/0.68804. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68118/0.68769. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68231/0.68828. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67959/0.68872. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68007/0.68878. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67948/0.68880. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67995/0.68947. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67928/0.69011. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67879/0.68992. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67731/0.69081. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67715/0.69123. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67696/0.69158. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67525/0.69147. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67439/0.69190. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67494/0.69215. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67438/0.69280. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67281/0.69278. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67277/0.69318. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67296/0.69371. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67383/0.69372. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.67280/0.69361. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67162/0.69384. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66817/0.69432. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66787/0.69392. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66763/0.69347. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66598/0.69298. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66333/0.69264. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65938/0.69294. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66321/0.69345. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66270/0.69496. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65952/0.69553. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65928/0.69506. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66119/0.69491. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65420/0.69619. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65790/0.69606. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65043/0.69649. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65465/0.69644. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64985/0.69530. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65315/0.69671. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64976/0.69940. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64904/0.70058. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64702/0.70095. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64216/0.70246. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64321/0.70166. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64543/0.70264. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64136/0.70381. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64055/0.70286. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64191/0.70343. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63619/0.70380. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63544/0.70431. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63839/0.70568. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63332/0.70812. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63713/0.70739. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63181/0.70792. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63026/0.70850. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63116/0.70940. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62544/0.70993. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62854/0.71257. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62309/0.71182. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63009/0.71138. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62262/0.71182. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69387/0.69263. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69220/0.69234. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69094/0.69214. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69126/0.69187. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69082/0.69163. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69086/0.69155. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68970/0.69147. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68925/0.69142. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68964/0.69141. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68841/0.69154. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68882/0.69165. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68917/0.69179. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68782/0.69203. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68832/0.69225. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68810/0.69235. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68737/0.69248. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68693/0.69278. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68712/0.69293. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68774/0.69304. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68611/0.69323. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68636/0.69354. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68634/0.69366. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68650/0.69402. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68675/0.69406. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68646/0.69420. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68665/0.69447. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68640/0.69474. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68536/0.69487. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68625/0.69504. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68511/0.69522. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68427/0.69590. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68404/0.69625. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68379/0.69641. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68362/0.69652. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68246/0.69716. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68201/0.69741. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68321/0.69776. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68223/0.69787. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68180/0.69805. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68019/0.69883. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67981/0.69921. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67831/0.69956. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67913/0.70022. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67844/0.70028. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67710/0.70104. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67926/0.70157. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67707/0.70188. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67570/0.70206. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67616/0.70257. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67380/0.70368. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67375/0.70387. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67474/0.70541. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67149/0.70565. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67469/0.70567. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66973/0.70667. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67099/0.70682. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66787/0.70658. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66926/0.70887. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66722/0.70940. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66662/0.71023. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66601/0.71029. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66263/0.71107. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66417/0.71103. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66230/0.71303. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65977/0.71446. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66222/0.71560. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65925/0.71463. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65866/0.71609. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65709/0.71598. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65608/0.71517. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65547/0.71752. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65491/0.71939. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65102/0.71973. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65142/0.71946. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65082/0.72215. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64831/0.72164. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64830/0.72373. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64921/0.72475. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64715/0.72505. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64383/0.72454. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64457/0.72393. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64302/0.72326. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64233/0.72901. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64034/0.72927. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63881/0.73074. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63748/0.73343. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63376/0.73258. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63294/0.73436. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63597/0.73424. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63494/0.74030. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63024/0.73717. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62745/0.73780. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63219/0.74318. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62801/0.74144. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62648/0.74646. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62227/0.74727. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62493/0.74850. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62246/0.74772. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61997/0.74963. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62368/0.74735. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69423/0.68658. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69210/0.68559. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69216/0.68582. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69099/0.68605. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69056/0.68645. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69025/0.68668. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69077/0.68732. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69055/0.68757. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69022/0.68767. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68965/0.68814. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68928/0.68872. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69019/0.68897. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68929/0.68944. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68965/0.68965. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68897/0.68974. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68948/0.68982. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68869/0.69020. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68802/0.69037. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68824/0.69058. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68878/0.69094. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68781/0.69128. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68802/0.69138. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68732/0.69135. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68680/0.69168. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68769/0.69216. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68746/0.69232. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68745/0.69264. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68721/0.69268. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68695/0.69242. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68642/0.69284. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68754/0.69291. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68597/0.69312. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68656/0.69327. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68571/0.69350. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68577/0.69363. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68480/0.69400. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68570/0.69478. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68547/0.69502. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68583/0.69491. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68577/0.69492. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68498/0.69528. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68464/0.69603. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68424/0.69649. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68424/0.69704. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68380/0.69718. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68339/0.69705. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68404/0.69699. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68370/0.69678. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68417/0.69764. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68344/0.69895. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68337/0.69870. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68323/0.69957. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68115/0.69977. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.68214/0.69991. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68041/0.70055. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68010/0.70180. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68084/0.70236. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.68151/0.70194. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68107/0.70262. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67960/0.70286. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67925/0.70211. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67786/0.70247. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67828/0.70340. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67830/0.70453. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67829/0.70496. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67758/0.70498. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67828/0.70629. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67751/0.70571. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67698/0.70525. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67783/0.70666. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67672/0.70628. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67481/0.70732. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67593/0.70786. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67535/0.70863. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67363/0.70919. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.67272/0.71115. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67448/0.71064. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67289/0.71176. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67306/0.71238. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67147/0.71229. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67042/0.71317. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66921/0.71308. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66807/0.71451. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.67112/0.71429. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66745/0.71504. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66993/0.71585. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66863/0.71616. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66602/0.71668. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66869/0.71712. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66660/0.71905. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66464/0.71933. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66666/0.72070. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66496/0.72220. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66728/0.72202. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66195/0.72243. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66450/0.72299. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66170/0.72187. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66300/0.72523. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65930/0.72672. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65916/0.72751. Took 0.19 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69126/0.68844. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69103/0.68796. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.68996/0.68809. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68985/0.68855. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69002/0.68921. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68999/0.69004. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68902/0.69075. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68863/0.69147. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68863/0.69226. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68798/0.69326. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68833/0.69406. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68830/0.69489. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68814/0.69579. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68755/0.69667. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68733/0.69789. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68609/0.69888. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68731/0.70002. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68627/0.70122. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68646/0.70210. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68655/0.70300. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68663/0.70395. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68502/0.70522. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68459/0.70592. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68474/0.70696. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68408/0.70807. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68415/0.70903. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68449/0.70977. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68315/0.71045. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.68366/0.71144. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68295/0.71180. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68385/0.71259. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68309/0.71384. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68153/0.71452. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68157/0.71447. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 0.68241/0.71528. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67986/0.71618. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68030/0.71687. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68006/0.71741. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67927/0.71793. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67982/0.71811. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67858/0.71872. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67677/0.71927. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.67856/0.71997. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67732/0.72020. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67687/0.72013. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67721/0.72093. Took 0.22 sec\n",
      "Epoch 46, Loss(train/val) 0.67753/0.72125. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67553/0.72107. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67567/0.72188. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67293/0.72184. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67581/0.72225. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67340/0.72273. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67294/0.72323. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67200/0.72415. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67114/0.72414. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67053/0.72502. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66778/0.72604. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66756/0.72740. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66557/0.72825. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66592/0.72872. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66647/0.72814. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66533/0.72922. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.66289/0.72981. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66141/0.73191. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66237/0.73248. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66078/0.73374. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.65942/0.73375. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66035/0.73495. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65694/0.73680. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.65555/0.73919. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65219/0.74110. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65341/0.74107. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65120/0.74414. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.65259/0.74575. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.64788/0.74760. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64882/0.74862. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.64742/0.74884. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64432/0.75259. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.64365/0.75308. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64354/0.75352. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63901/0.75502. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.63771/0.75612. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.63663/0.75853. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.63653/0.76059. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63648/0.76276. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.63398/0.75940. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63059/0.76806. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.63269/0.77220. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63286/0.76850. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.62851/0.77206. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.62718/0.77215. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62351/0.78174. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62412/0.77860. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.62476/0.78166. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62268/0.77926. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.61938/0.78633. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61892/0.78825. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.61526/0.78651. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.62163/0.78798. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.61467/0.78877. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69092/0.68914. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68993/0.68905. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68984/0.68894. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68899/0.68888. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68927/0.68884. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68822/0.68883. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68791/0.68877. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68747/0.68875. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68744/0.68878. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68767/0.68889. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68722/0.68900. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68538/0.68937. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68516/0.68976. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68578/0.69039. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68470/0.69117. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68507/0.69179. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68274/0.69253. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68346/0.69351. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68444/0.69441. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68342/0.69521. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68271/0.69598. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68220/0.69689. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68263/0.69797. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68223/0.69858. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68185/0.69951. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68129/0.70023. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68211/0.70094. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68162/0.70131. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68055/0.70135. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68078/0.70183. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68168/0.70242. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68043/0.70295. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67876/0.70322. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67864/0.70417. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67886/0.70404. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67759/0.70531. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67758/0.70642. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67597/0.70811. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67908/0.70807. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67734/0.70877. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67677/0.70919. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67697/0.70956. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67695/0.70950. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67609/0.71017. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67465/0.71039. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67348/0.71162. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67350/0.71238. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67496/0.71257. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67408/0.71284. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67403/0.71371. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67436/0.71391. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67251/0.71388. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67296/0.71406. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67160/0.71450. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67103/0.71538. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67151/0.71670. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67087/0.71737. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66949/0.71695. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66754/0.71721. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66776/0.71872. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66643/0.71802. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66777/0.71888. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66794/0.71972. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 0.66601/0.72085. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66354/0.72207. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66595/0.72270. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66308/0.72374. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66093/0.72595. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66205/0.72770. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66308/0.72808. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66210/0.72819. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66070/0.72749. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65919/0.72954. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66137/0.72986. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66088/0.72874. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65881/0.73030. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65707/0.73051. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65566/0.73020. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65637/0.73063. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65494/0.73410. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65524/0.73392. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65298/0.73404. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65266/0.73389. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65113/0.73780. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65011/0.73502. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65031/0.73898. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65193/0.73917. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64979/0.74240. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64929/0.73963. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65043/0.73844. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64676/0.73924. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64708/0.74281. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64599/0.74164. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64672/0.74439. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64485/0.74495. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64461/0.74495. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63903/0.74993. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64447/0.74655. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64194/0.75200. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63835/0.75134. Took 0.19 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69045/0.68476. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68873/0.68369. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68846/0.68306. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68790/0.68249. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68718/0.68197. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68704/0.68153. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68716/0.68127. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68650/0.68093. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68550/0.68089. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68511/0.68085. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68539/0.68084. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68476/0.68084. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68501/0.68099. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68445/0.68128. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68395/0.68141. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68415/0.68142. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68441/0.68172. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68330/0.68174. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68455/0.68182. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68376/0.68186. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68238/0.68218. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68322/0.68265. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68359/0.68297. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68247/0.68337. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68246/0.68361. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68207/0.68363. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68113/0.68379. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68158/0.68441. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68115/0.68436. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68140/0.68518. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68054/0.68554. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68136/0.68574. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68019/0.68638. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67947/0.68674. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67908/0.68726. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67959/0.68789. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67934/0.68831. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67823/0.68879. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67909/0.68904. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67788/0.68946. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67735/0.69040. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67502/0.69110. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67781/0.69172. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67718/0.69228. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67700/0.69287. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67656/0.69298. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67572/0.69334. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67468/0.69517. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67384/0.69515. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67538/0.69600. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67290/0.69612. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67345/0.69728. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67207/0.69823. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67164/0.70047. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67160/0.70043. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67187/0.70173. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66920/0.70207. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67184/0.70348. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66892/0.70539. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66856/0.70612. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66745/0.70683. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67040/0.70694. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66659/0.71042. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66778/0.70956. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66772/0.71043. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66615/0.71013. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66569/0.71272. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66511/0.71180. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66362/0.71550. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66445/0.71746. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66353/0.71411. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66308/0.71579. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66171/0.71741. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66272/0.71830. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66323/0.71648. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66039/0.71751. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66017/0.72065. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65892/0.71992. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65994/0.72304. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65847/0.72249. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65719/0.72551. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65706/0.72319. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65628/0.72266. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65716/0.72647. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65623/0.72493. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65529/0.72647. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65432/0.72921. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65337/0.72917. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65255/0.72842. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65322/0.72964. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65287/0.73411. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64826/0.73163. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65076/0.73211. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64831/0.74068. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64794/0.73808. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64541/0.73492. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64535/0.74299. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64146/0.74516. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64776/0.74389. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64313/0.74636. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69335/0.68985. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68995/0.69048. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68797/0.69101. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68790/0.69174. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68757/0.69236. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68701/0.69296. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68629/0.69356. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68572/0.69439. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68549/0.69498. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68428/0.69565. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68461/0.69655. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68389/0.69714. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68451/0.69761. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68431/0.69839. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68296/0.69917. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68295/0.69979. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68255/0.70086. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68236/0.70158. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68285/0.70217. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68220/0.70292. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68182/0.70333. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68252/0.70365. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68229/0.70433. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68104/0.70527. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68085/0.70532. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68152/0.70553. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68078/0.70576. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68170/0.70604. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68014/0.70642. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67972/0.70650. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67993/0.70700. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67862/0.70756. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67904/0.70779. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67933/0.70796. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67943/0.70790. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67806/0.70834. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67949/0.70886. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67799/0.70888. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67718/0.70946. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67649/0.70959. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67632/0.71057. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67621/0.71115. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67642/0.71142. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.67697/0.71097. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67432/0.71128. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67423/0.71243. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67643/0.71236. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67444/0.71268. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67308/0.71355. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.67285/0.71399. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67341/0.71445. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67110/0.71481. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67219/0.71509. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66978/0.71570. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67229/0.71524. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67292/0.71548. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67012/0.71583. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.67009/0.71572. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66753/0.71723. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66997/0.71799. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66917/0.71825. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66765/0.71796. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66706/0.71880. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66680/0.71871. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66687/0.71802. Took 0.22 sec\n",
      "Epoch 65, Loss(train/val) 0.66759/0.71883. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66641/0.71934. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66583/0.72032. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66677/0.72147. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66306/0.72143. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66203/0.72190. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.66239/0.72419. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66149/0.72369. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.66077/0.72290. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65965/0.72383. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65633/0.72448. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65866/0.72579. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65836/0.72697. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65617/0.72592. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65643/0.72592. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65654/0.72613. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65434/0.72733. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65526/0.72789. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65212/0.72755. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65182/0.73079. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65358/0.73121. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65152/0.73496. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64889/0.73572. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64724/0.73645. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65069/0.73844. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64681/0.73881. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64549/0.73658. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64847/0.73637. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64624/0.73893. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64396/0.74242. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64200/0.74178. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64279/0.74191. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63996/0.74183. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64153/0.74237. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64098/0.74351. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69283/0.70427. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68992/0.70197. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68893/0.69945. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68785/0.69766. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68698/0.69604. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68621/0.69468. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68537/0.69341. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68530/0.69260. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68441/0.69204. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68451/0.69107. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68399/0.69086. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68311/0.68997. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68359/0.69042. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68323/0.68990. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68277/0.68964. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68243/0.69005. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68289/0.68949. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68095/0.68995. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68160/0.68988. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68112/0.68990. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68132/0.69043. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68062/0.68993. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68088/0.69045. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68108/0.68995. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68117/0.69024. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68081/0.69040. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68033/0.68998. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67994/0.69045. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68022/0.69020. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67956/0.69073. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68032/0.69076. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67890/0.69049. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67911/0.69054. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67941/0.69076. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67895/0.69085. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67935/0.69103. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67854/0.69097. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67786/0.69120. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67767/0.69178. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67809/0.69185. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67758/0.69200. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67672/0.69225. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67664/0.69252. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67648/0.69243. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67615/0.69260. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67527/0.69287. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67652/0.69393. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67574/0.69385. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67718/0.69436. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67504/0.69426. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67539/0.69483. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67477/0.69559. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67462/0.69462. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67493/0.69549. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67441/0.69605. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67344/0.69578. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67360/0.69691. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67420/0.69680. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67188/0.69749. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67359/0.69678. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67334/0.69742. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67137/0.69792. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67251/0.69771. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67149/0.69890. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67157/0.69787. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67066/0.69868. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67129/0.69831. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67136/0.69884. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67005/0.70013. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67051/0.69795. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67028/0.70038. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67070/0.69965. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66918/0.69952. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66972/0.70052. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66700/0.70009. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66833/0.70000. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66822/0.69898. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66614/0.69785. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66705/0.69719. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66714/0.69692. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66477/0.69791. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66624/0.69870. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66638/0.69900. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66523/0.69909. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66518/0.69583. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66287/0.69755. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66283/0.69835. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66171/0.69863. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65993/0.69889. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65941/0.69850. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66078/0.69983. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65893/0.69826. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65959/0.69775. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65857/0.69812. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65588/0.69737. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65639/0.69855. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65416/0.69344. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65484/0.69383. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65478/0.69494. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65425/0.69541. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69042/0.69364. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68940/0.69358. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68941/0.69379. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68933/0.69409. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68906/0.69440. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68925/0.69471. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68903/0.69503. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68753/0.69541. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68859/0.69575. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68786/0.69606. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68760/0.69638. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68639/0.69684. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68683/0.69740. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68793/0.69774. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68582/0.69800. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68596/0.69847. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68458/0.69881. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68490/0.69916. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68608/0.69946. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68497/0.69985. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68540/0.69995. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68456/0.70017. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68336/0.70068. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68300/0.70116. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68426/0.70141. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68330/0.70169. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68202/0.70214. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68345/0.70228. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68286/0.70246. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68245/0.70284. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67998/0.70331. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68055/0.70363. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68306/0.70395. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68043/0.70384. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68035/0.70398. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67966/0.70397. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67912/0.70406. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67996/0.70439. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67766/0.70462. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67944/0.70477. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67645/0.70492. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67709/0.70467. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67592/0.70458. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67660/0.70487. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67334/0.70541. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67588/0.70577. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67423/0.70554. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67446/0.70536. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67336/0.70535. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.67145/0.70543. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67274/0.70647. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67142/0.70615. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66987/0.70682. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66837/0.70731. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66880/0.70692. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66962/0.70782. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66878/0.70860. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66642/0.70954. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66877/0.71061. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66715/0.70957. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66447/0.71041. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66480/0.71012. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66404/0.71083. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66419/0.71027. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66158/0.71100. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66354/0.71095. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66015/0.71155. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65988/0.71233. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66095/0.71346. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66054/0.71347. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65854/0.71398. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65702/0.71431. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65786/0.71358. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65408/0.71459. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65564/0.71545. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65341/0.71737. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65232/0.71737. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65383/0.71932. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65074/0.72219. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65411/0.72262. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65075/0.72127. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64969/0.72154. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64965/0.72080. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64730/0.72580. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64823/0.72054. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64364/0.72573. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64388/0.72579. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64094/0.73036. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64615/0.72606. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64369/0.72334. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63921/0.72514. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64071/0.72810. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64028/0.72733. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64061/0.73167. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63699/0.73255. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63773/0.73142. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63787/0.73232. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63645/0.73085. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63550/0.73313. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63126/0.73704. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69490/0.69800. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.69322/0.69646. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69206/0.69512. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69196/0.69412. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69065/0.69337. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69103/0.69276. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68998/0.69217. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69017/0.69157. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68960/0.69103. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68969/0.69061. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68902/0.69026. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68900/0.69000. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68849/0.68977. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68847/0.68955. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68802/0.68934. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68723/0.68925. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68701/0.68924. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68661/0.68906. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68696/0.68918. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68663/0.68916. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68625/0.68927. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68602/0.68927. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68559/0.68928. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68615/0.68931. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68502/0.68921. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68558/0.68939. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68501/0.68947. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68468/0.68965. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68471/0.68973. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68407/0.68980. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68397/0.68995. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68568/0.69022. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68441/0.69028. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68405/0.69039. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68414/0.69030. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68390/0.69032. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68258/0.69037. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68378/0.69055. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68255/0.69067. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68250/0.69134. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68264/0.69135. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68239/0.69157. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68144/0.69213. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68171/0.69208. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68154/0.69212. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68073/0.69279. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.68040/0.69304. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68018/0.69359. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68105/0.69398. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.67974/0.69448. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67937/0.69441. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67833/0.69511. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67831/0.69581. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67703/0.69643. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67665/0.69719. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67740/0.69744. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67666/0.69871. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.67580/0.69946. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67555/0.70063. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67415/0.70134. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67465/0.70261. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67209/0.70286. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67530/0.70365. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67388/0.70344. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67298/0.70451. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.67243/0.70465. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67099/0.70615. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67233/0.70647. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67096/0.70778. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67121/0.70678. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67380/0.70926. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67243/0.70944. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66935/0.70988. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66797/0.71072. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66749/0.71072. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66805/0.71252. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66853/0.71177. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66761/0.71336. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66875/0.71308. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66676/0.71339. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66474/0.71431. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66577/0.71514. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66425/0.71588. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66778/0.71661. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66521/0.71811. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66557/0.71919. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66265/0.71816. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66297/0.72054. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66024/0.72058. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66057/0.72265. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65997/0.72264. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66139/0.72384. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66159/0.72278. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65703/0.72492. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66166/0.72574. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65869/0.72570. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65529/0.72680. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65722/0.72742. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65695/0.72892. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65785/0.72817. Took 0.19 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69353/0.69350. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69313/0.69351. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69268/0.69382. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69290/0.69379. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69205/0.69389. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69231/0.69389. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69247/0.69372. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69247/0.69357. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69154/0.69378. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69154/0.69387. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69080/0.69398. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69065/0.69406. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68998/0.69398. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69010/0.69430. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69014/0.69414. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.69047/0.69396. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.69101/0.69418. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68981/0.69466. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68948/0.69525. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68875/0.69526. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68832/0.69565. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68809/0.69621. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68742/0.69676. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68730/0.69740. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68724/0.69778. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68664/0.69762. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68629/0.69830. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68634/0.69910. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68659/0.69985. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68617/0.70004. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68529/0.70083. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68505/0.70182. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68535/0.70239. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68550/0.70308. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68565/0.70359. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68349/0.70450. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68555/0.70488. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68353/0.70592. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68471/0.70633. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68342/0.70747. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68271/0.70819. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68161/0.70897. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68017/0.70949. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68154/0.71044. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67951/0.71082. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68076/0.71143. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68122/0.71168. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68108/0.71199. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67936/0.71248. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67865/0.71309. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68113/0.71369. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.68025/0.71466. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67955/0.71540. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67878/0.71540. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67813/0.71604. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67798/0.71597. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67693/0.71678. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67606/0.71714. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67549/0.71761. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67526/0.71824. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67542/0.71948. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67418/0.72012. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67444/0.71999. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67365/0.71983. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67281/0.72026. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67501/0.72096. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67313/0.72139. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67335/0.72159. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67286/0.72213. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67432/0.72346. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67122/0.72346. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67155/0.72381. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66813/0.72522. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66743/0.72552. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66921/0.72608. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66785/0.72693. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66730/0.72719. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66869/0.72792. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66850/0.72867. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66657/0.72858. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66633/0.72994. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66754/0.73088. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66501/0.73159. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66497/0.73128. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66586/0.73247. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66407/0.73362. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66085/0.73433. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66229/0.73524. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66110/0.73486. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66178/0.73535. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66240/0.73621. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.66113/0.73740. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66015/0.73688. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66094/0.73771. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66057/0.73685. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66176/0.73907. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65959/0.73871. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65922/0.74052. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65910/0.74083. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.65992/0.73949. Took 0.18 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69487/0.69486. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69262/0.69531. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69254/0.69565. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69190/0.69605. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69184/0.69650. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69015/0.69701. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69045/0.69739. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69123/0.69750. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68954/0.69791. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69070/0.69808. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68959/0.69839. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68970/0.69863. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68920/0.69885. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68859/0.69893. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68925/0.69894. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68860/0.69931. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68840/0.69939. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68737/0.69936. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68865/0.69933. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68809/0.69936. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68693/0.69935. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68699/0.69921. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68641/0.69914. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68639/0.69888. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68621/0.69853. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68629/0.69846. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68621/0.69864. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68418/0.69816. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68415/0.69817. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68401/0.69840. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68344/0.69820. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68300/0.69806. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68239/0.69754. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68157/0.69763. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68259/0.69765. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68094/0.69653. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68072/0.69615. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67933/0.69569. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67951/0.69595. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67937/0.69529. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67802/0.69517. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67733/0.69438. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67630/0.69397. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67681/0.69379. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67532/0.69227. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.67398/0.69334. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67305/0.69368. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67359/0.69264. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67381/0.69344. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67086/0.69269. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67089/0.69379. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66997/0.69464. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66651/0.69428. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66807/0.69413. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66576/0.69362. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66406/0.69448. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66409/0.69380. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66407/0.69308. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66426/0.69210. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66000/0.69377. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66000/0.69492. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65741/0.69256. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65669/0.69433. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65557/0.69525. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65128/0.69476. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65103/0.69940. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65040/0.69910. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65130/0.69960. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65101/0.69740. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64572/0.69675. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64455/0.69936. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64644/0.69755. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64727/0.70327. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64402/0.69981. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63959/0.70096. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64081/0.70391. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64505/0.70357. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64065/0.70269. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63936/0.70773. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63761/0.70527. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63756/0.70062. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63933/0.70598. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63220/0.70488. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63309/0.70564. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62702/0.70613. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63370/0.70538. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62914/0.71146. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63408/0.70752. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62785/0.70466. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62944/0.70386. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62435/0.70751. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63177/0.71104. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62228/0.70449. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62352/0.70992. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62331/0.71391. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61636/0.71037. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62059/0.71028. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61776/0.70476. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61769/0.71512. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62053/0.70924. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69337/0.69584. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69283/0.69509. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69223/0.69477. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69215/0.69466. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69196/0.69445. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69206/0.69470. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69078/0.69469. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69096/0.69477. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69073/0.69501. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69037/0.69519. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68972/0.69552. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68929/0.69603. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68962/0.69639. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68910/0.69661. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68874/0.69746. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68798/0.69789. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68810/0.69844. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68885/0.69921. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68776/0.69927. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68761/0.69958. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68815/0.70039. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68843/0.70069. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68643/0.70137. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68677/0.70185. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68753/0.70244. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68678/0.70269. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68685/0.70271. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68653/0.70338. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68657/0.70368. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68654/0.70368. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68503/0.70446. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68542/0.70457. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68616/0.70480. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68562/0.70587. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68504/0.70666. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68489/0.70679. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68528/0.70759. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68388/0.70842. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68397/0.70833. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68325/0.70820. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68244/0.70872. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68261/0.70849. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68311/0.70980. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68255/0.71082. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68148/0.71070. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68032/0.71167. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68081/0.71168. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68058/0.71304. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67941/0.71313. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67757/0.71490. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67833/0.71393. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.68041/0.71437. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67806/0.71535. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67693/0.71597. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67687/0.71593. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67569/0.71654. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67443/0.71676. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.67492/0.71797. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67307/0.71782. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67509/0.71767. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67176/0.71804. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67049/0.71939. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67381/0.71874. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66978/0.71956. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66897/0.71975. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.66519/0.71987. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66734/0.72092. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66642/0.72224. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66327/0.72187. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66343/0.72442. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66359/0.72349. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 0.66119/0.72463. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66104/0.72461. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65804/0.72396. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65891/0.72325. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66109/0.72446. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65856/0.72527. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65758/0.72480. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65510/0.72465. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.65659/0.72596. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65332/0.72495. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65216/0.72339. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65057/0.72680. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64750/0.72747. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65071/0.72724. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.64659/0.72625. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64452/0.72783. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64570/0.73029. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64422/0.72706. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64090/0.73110. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64509/0.72398. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64183/0.72410. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63574/0.72512. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63866/0.72965. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63845/0.72758. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63870/0.72628. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63398/0.72681. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63540/0.72506. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62735/0.72537. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63608/0.72165. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69201/0.69351. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69031/0.69374. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69077/0.69338. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69029/0.69323. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68915/0.69328. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68965/0.69341. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68937/0.69342. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68962/0.69372. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68853/0.69373. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68866/0.69385. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68818/0.69410. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68758/0.69431. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68820/0.69483. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68742/0.69488. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68830/0.69494. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68758/0.69522. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68725/0.69561. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68732/0.69581. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68720/0.69643. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68786/0.69648. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68636/0.69679. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68655/0.69723. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68648/0.69740. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.68667/0.69777. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68695/0.69793. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68619/0.69798. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68593/0.69854. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68607/0.69901. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68660/0.69921. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68625/0.69946. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68466/0.69988. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68649/0.70014. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68536/0.70051. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68628/0.70078. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68440/0.70110. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68366/0.70106. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68443/0.70150. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68378/0.70180. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68442/0.70215. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68353/0.70264. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68379/0.70260. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68265/0.70288. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68285/0.70347. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68276/0.70417. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68342/0.70456. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68194/0.70434. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68292/0.70437. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68182/0.70483. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68376/0.70474. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68261/0.70495. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67986/0.70553. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.68085/0.70579. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67881/0.70611. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68186/0.70629. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67893/0.70715. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68103/0.70692. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68010/0.70705. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67734/0.70762. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67945/0.70838. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67859/0.70840. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67659/0.71004. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67762/0.70938. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67733/0.71011. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67797/0.71020. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67710/0.71075. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67639/0.71084. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67563/0.71129. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67576/0.71204. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67408/0.71225. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67645/0.71213. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67430/0.71254. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67458/0.71304. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67116/0.71232. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67260/0.71340. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67208/0.71340. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67250/0.71392. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67217/0.71447. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67142/0.71520. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.67114/0.71442. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67209/0.71421. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67051/0.71589. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67075/0.71639. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67020/0.71774. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66798/0.71858. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66821/0.71838. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66982/0.72001. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66901/0.72023. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66613/0.72097. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66823/0.72168. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66663/0.72206. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66321/0.72498. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66541/0.72196. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66482/0.72402. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66648/0.72429. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66809/0.72405. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66383/0.72533. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66139/0.72494. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66261/0.72540. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66053/0.72659. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.66228/0.72706. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69278/0.69261. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69137/0.69289. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69172/0.69409. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69093/0.69487. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69134/0.69574. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69043/0.69638. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68996/0.69701. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69005/0.69741. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68950/0.69788. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69013/0.69850. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69028/0.69883. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68973/0.69879. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68990/0.69888. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68915/0.69893. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68965/0.69955. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68971/0.69962. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68903/0.69968. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68846/0.69945. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68759/0.69988. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68805/0.70031. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68788/0.70041. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68855/0.70032. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68784/0.70049. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68735/0.70063. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68715/0.70056. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68698/0.70084. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68688/0.70079. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68716/0.70034. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68701/0.70049. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68737/0.70026. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68634/0.70042. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68592/0.70067. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68527/0.70060. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68611/0.70030. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68584/0.70000. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68484/0.69960. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68457/0.69988. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68567/0.70015. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68449/0.69979. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68431/0.69972. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68462/0.70024. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68450/0.70084. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68435/0.70039. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68335/0.70015. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68264/0.69979. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68341/0.69955. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68213/0.69975. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68310/0.69975. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68212/0.69894. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.68240/0.69907. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68165/0.69901. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.68088/0.69915. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68126/0.69945. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67998/0.69964. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67917/0.69938. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67924/0.69960. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67985/0.69981. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.68033/0.69985. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.68035/0.69973. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67851/0.69937. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67853/0.69948. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67779/0.69986. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67796/0.69970. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67484/0.69965. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67540/0.70009. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67690/0.69900. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67558/0.69917. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67469/0.69945. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67519/0.69942. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67296/0.69995. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67356/0.69957. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67425/0.69968. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67201/0.69956. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67258/0.69964. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67111/0.69983. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67023/0.69992. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67132/0.69969. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66884/0.69912. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66794/0.69933. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66968/0.69973. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66761/0.69917. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66627/0.69867. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66810/0.69858. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66441/0.69932. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66452/0.69984. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66355/0.70004. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66584/0.70052. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66397/0.69979. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66309/0.69984. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66574/0.69969. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66199/0.69914. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66005/0.69843. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66051/0.69861. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65776/0.70072. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65788/0.70076. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65760/0.70043. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65786/0.69990. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65474/0.70001. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65623/0.70013. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65291/0.70238. Took 0.18 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69291/0.68848. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69256/0.68846. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69190/0.68845. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69203/0.68855. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69191/0.68868. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69185/0.68901. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69197/0.68929. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69103/0.68960. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69090/0.68999. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69097/0.69054. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69043/0.69103. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69086/0.69172. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69022/0.69232. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69033/0.69311. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68900/0.69383. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68939/0.69479. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68887/0.69547. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68933/0.69705. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68795/0.69837. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68813/0.69979. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68797/0.70084. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68766/0.70171. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68754/0.70282. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68610/0.70423. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68634/0.70540. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68612/0.70616. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68495/0.70726. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68529/0.70809. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68512/0.70833. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68540/0.71000. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68384/0.71060. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68420/0.71096. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68418/0.71076. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68367/0.71092. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68372/0.71069. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68481/0.71012. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68240/0.71094. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68276/0.71117. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68224/0.71148. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.68158/0.71170. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68140/0.71197. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68254/0.71223. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68009/0.71228. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.68109/0.71143. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68013/0.71120. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.68069/0.71150. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.67985/0.71152. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67872/0.71162. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67901/0.71171. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67780/0.71090. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.67832/0.71190. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67936/0.71160. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67587/0.71300. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67495/0.71192. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67574/0.71127. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67608/0.71065. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67570/0.71118. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67529/0.70823. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.67478/0.70826. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67384/0.70969. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67368/0.71045. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67267/0.71043. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67286/0.70941. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67200/0.71094. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67183/0.71025. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67053/0.71111. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.66974/0.71271. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66966/0.71430. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66826/0.71429. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66698/0.71422. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66971/0.71376. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66808/0.71221. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 0.66385/0.71179. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66800/0.71284. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.66539/0.71494. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66724/0.71553. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66421/0.71802. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66460/0.71737. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66543/0.71784. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66248/0.71580. Took 0.22 sec\n",
      "Epoch 80, Loss(train/val) 0.66148/0.71585. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.66366/0.71680. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.66009/0.71911. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66175/0.72011. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.66092/0.71955. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66186/0.71910. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65998/0.71891. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65892/0.72136. Took 0.22 sec\n",
      "Epoch 88, Loss(train/val) 0.65459/0.72303. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65718/0.72234. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.65593/0.72170. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65433/0.72364. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.65185/0.72463. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65271/0.72310. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.65510/0.72406. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65166/0.72534. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64963/0.72521. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65164/0.72476. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65195/0.72615. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64772/0.72656. Took 0.20 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69124/0.69763. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.68833/0.70150. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68854/0.70252. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68787/0.70262. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68658/0.70282. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68612/0.70351. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68610/0.70368. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68459/0.70463. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68416/0.70540. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68334/0.70635. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68295/0.70652. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68322/0.70728. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68341/0.70784. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68255/0.70818. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68277/0.70938. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68274/0.70986. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68222/0.71094. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68215/0.71082. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68177/0.71062. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68197/0.71108. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68164/0.71193. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68051/0.71235. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68120/0.71319. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68151/0.71300. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68081/0.71393. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68136/0.71344. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68134/0.71352. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68007/0.71409. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68069/0.71444. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67947/0.71446. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67985/0.71525. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68006/0.71456. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67912/0.71533. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67925/0.71506. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67867/0.71547. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67924/0.71626. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67814/0.71579. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67781/0.71641. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67799/0.71607. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67752/0.71709. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67746/0.71722. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67599/0.71701. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67668/0.71796. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67743/0.71757. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67659/0.71770. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67698/0.71927. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67597/0.71902. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67502/0.71882. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67568/0.71979. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67462/0.71991. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67446/0.71995. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67315/0.72039. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67352/0.72117. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67294/0.72153. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67255/0.72275. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67200/0.72190. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67123/0.72432. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67124/0.72322. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66914/0.72363. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67053/0.72431. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66868/0.72420. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66912/0.72565. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66997/0.72473. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66770/0.72560. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66741/0.72554. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66727/0.72719. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66675/0.72690. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66437/0.72775. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66634/0.72738. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66427/0.72833. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66354/0.72670. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66412/0.72880. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66336/0.72932. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66307/0.72747. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66094/0.73250. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66132/0.73147. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65813/0.73048. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66081/0.72995. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65944/0.73158. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65847/0.73245. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65857/0.73256. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65554/0.73347. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65286/0.73374. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65496/0.73353. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65513/0.73392. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65379/0.73232. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65187/0.73688. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65000/0.73502. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65147/0.73519. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64886/0.73405. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64901/0.74030. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64374/0.73398. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64343/0.73685. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64420/0.73622. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64470/0.73673. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64413/0.73569. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63964/0.73824. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.64158/0.73602. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63904/0.73564. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63553/0.73566. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69265/0.69146. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69176/0.69015. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69076/0.68896. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69064/0.68788. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68896/0.68677. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68889/0.68570. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68866/0.68492. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68835/0.68410. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68730/0.68340. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68733/0.68289. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68661/0.68227. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68651/0.68212. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68585/0.68178. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68617/0.68173. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68540/0.68156. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68586/0.68148. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68505/0.68143. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68530/0.68138. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68475/0.68161. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68446/0.68137. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68455/0.68162. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68446/0.68159. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68443/0.68154. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68434/0.68172. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68329/0.68182. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68280/0.68218. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68270/0.68242. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68338/0.68231. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68264/0.68205. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68363/0.68244. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68321/0.68253. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68256/0.68248. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68249/0.68259. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68180/0.68287. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68229/0.68293. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68150/0.68303. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68180/0.68337. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68114/0.68341. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68176/0.68349. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68201/0.68324. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68086/0.68344. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67988/0.68304. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68078/0.68294. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67980/0.68287. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68021/0.68286. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68111/0.68275. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67927/0.68309. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68052/0.68316. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67862/0.68348. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67884/0.68395. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67914/0.68411. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67824/0.68444. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67832/0.68448. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67697/0.68466. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67746/0.68473. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67820/0.68483. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67834/0.68436. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67655/0.68400. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67680/0.68497. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67587/0.68494. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67531/0.68573. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67535/0.68581. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67526/0.68689. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67659/0.68721. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67636/0.68653. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67556/0.68673. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67458/0.68636. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67344/0.68660. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67365/0.68697. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67355/0.68646. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67291/0.68681. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.67121/0.68700. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67108/0.68707. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67250/0.68719. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67128/0.68823. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67101/0.68936. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67194/0.68877. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66891/0.68961. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67003/0.69043. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66906/0.69041. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.67011/0.69067. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66752/0.69090. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66988/0.69117. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66785/0.69185. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66917/0.69169. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66745/0.69146. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66753/0.69233. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66561/0.69217. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66613/0.69311. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66397/0.69322. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66516/0.69392. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66337/0.69561. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66152/0.69690. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66316/0.69755. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66302/0.69827. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66439/0.69789. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.66104/0.69933. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66000/0.69810. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66094/0.69909. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66032/0.70101. Took 0.18 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69371/0.69271. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69112/0.69186. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69050/0.69108. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68973/0.69026. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68966/0.68975. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68892/0.68930. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68858/0.68904. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68852/0.68893. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68708/0.68857. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68748/0.68842. Took 0.22 sec\n",
      "Epoch 10, Loss(train/val) 0.68752/0.68863. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68654/0.68880. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68656/0.68919. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68640/0.68941. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68541/0.69010. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68491/0.69083. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68486/0.69168. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68502/0.69198. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68406/0.69254. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68384/0.69322. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68462/0.69408. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68367/0.69453. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68276/0.69490. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68403/0.69566. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68363/0.69629. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68354/0.69656. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68257/0.69721. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68303/0.69796. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68250/0.69850. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.68316/0.69896. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68154/0.69927. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68193/0.70019. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68311/0.70044. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68216/0.70070. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68257/0.70106. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68266/0.70128. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68032/0.70201. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.68173/0.70263. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68088/0.70323. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67906/0.70346. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68083/0.70365. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.68002/0.70384. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.67882/0.70454. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.67922/0.70512. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67961/0.70563. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67909/0.70631. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67825/0.70778. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67848/0.70844. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67733/0.70889. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67807/0.70896. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67819/0.70854. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67819/0.70960. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67769/0.71028. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67646/0.71078. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67656/0.71136. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67544/0.71120. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67621/0.71201. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67612/0.71280. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67445/0.71321. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67572/0.71434. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67524/0.71429. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67491/0.71458. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67616/0.71486. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.67520/0.71591. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 0.67385/0.71578. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.67371/0.71653. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.67305/0.71620. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67299/0.71676. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67249/0.71780. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.67214/0.71846. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.66984/0.71741. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66872/0.71769. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.67291/0.71673. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66949/0.71771. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.67026/0.71868. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66914/0.71823. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66913/0.71871. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66778/0.71934. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66864/0.71975. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.66688/0.71974. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 0.66516/0.71914. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66615/0.72037. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.66519/0.72080. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66677/0.72060. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.66479/0.72347. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66637/0.71956. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.66263/0.72012. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66503/0.72020. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.66361/0.72079. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66393/0.72034. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66015/0.72122. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65926/0.72130. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.66012/0.71957. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66072/0.71865. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65894/0.71879. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65788/0.72195. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.65860/0.71850. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65542/0.71827. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.65541/0.71744. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65437/0.72015. Took 0.20 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69480/0.68740. Took 0.34 sec\n",
      "Epoch 1, Loss(train/val) 0.69226/0.68657. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69051/0.68577. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68941/0.68509. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68902/0.68457. Took 0.22 sec\n",
      "Epoch 5, Loss(train/val) 0.68802/0.68412. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68684/0.68357. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68673/0.68322. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68685/0.68309. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68655/0.68273. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68575/0.68232. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68551/0.68221. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68568/0.68196. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68520/0.68185. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68543/0.68184. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68509/0.68178. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68434/0.68171. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68451/0.68157. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68487/0.68160. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68418/0.68172. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68474/0.68191. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68375/0.68176. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68376/0.68169. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68337/0.68173. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68287/0.68178. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68294/0.68188. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68332/0.68205. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68320/0.68224. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68180/0.68247. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68187/0.68250. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68167/0.68256. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68158/0.68282. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68135/0.68292. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68073/0.68304. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68209/0.68304. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68115/0.68308. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68120/0.68313. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68042/0.68332. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68048/0.68349. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67940/0.68350. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67981/0.68356. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68043/0.68352. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67972/0.68410. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67902/0.68429. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67877/0.68470. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67803/0.68464. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67827/0.68461. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67740/0.68461. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67766/0.68473. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.67775/0.68478. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67873/0.68475. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67750/0.68464. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67716/0.68458. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67695/0.68525. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67615/0.68568. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67665/0.68581. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67563/0.68555. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67477/0.68584. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67626/0.68592. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67475/0.68575. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67595/0.68581. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67428/0.68570. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67588/0.68615. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67439/0.68611. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67388/0.68546. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67329/0.68542. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67262/0.68665. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.67310/0.68683. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67151/0.68630. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67353/0.68689. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67226/0.68736. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67058/0.68645. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66931/0.68758. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66949/0.68761. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67058/0.68740. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67025/0.68723. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67040/0.68600. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66964/0.68577. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67002/0.68725. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66921/0.68639. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66834/0.68704. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66952/0.68844. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66821/0.68821. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66644/0.68716. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66840/0.68655. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66912/0.68747. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66629/0.68850. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66634/0.68798. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66404/0.68741. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66537/0.68927. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66534/0.68717. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66562/0.68738. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66456/0.68813. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66397/0.68729. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66382/0.68737. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66323/0.68714. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66226/0.68570. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66081/0.68500. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66161/0.68494. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.66097/0.68705. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69108/0.68919. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69045/0.68830. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68875/0.68796. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68841/0.68775. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68839/0.68755. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68689/0.68747. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 0.68664/0.68750. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68620/0.68764. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68574/0.68770. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68476/0.68800. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68413/0.68824. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68312/0.68849. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68305/0.68863. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68263/0.68894. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68194/0.68932. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68214/0.68934. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68158/0.68933. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68049/0.68944. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68025/0.68972. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68009/0.68983. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67941/0.69038. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67945/0.69096. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67811/0.69106. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67790/0.69151. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67698/0.69207. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67644/0.69242. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67583/0.69294. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67620/0.69323. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67474/0.69368. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67335/0.69448. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67249/0.69526. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67085/0.69602. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67211/0.69672. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67002/0.69772. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66880/0.69861. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66861/0.69926. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66865/0.70041. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66557/0.70075. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66537/0.70233. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66544/0.70396. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66029/0.70651. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66219/0.70848. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66011/0.70947. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.65956/0.71138. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.65775/0.71361. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.65996/0.71525. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.65693/0.71757. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.65628/0.72006. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65271/0.72332. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.64979/0.72726. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.64919/0.73034. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.64946/0.73322. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.64921/0.73453. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.64588/0.73629. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.64456/0.73975. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.64702/0.74242. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.64269/0.74557. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64018/0.74924. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.63835/0.75463. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.63872/0.75675. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.63643/0.75949. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.63881/0.76323. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.63181/0.76779. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.63301/0.77145. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63156/0.77320. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.62936/0.77372. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.62973/0.77578. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.62800/0.78184. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.62319/0.78527. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.62764/0.78659. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.62786/0.78999. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.62577/0.79133. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62446/0.79874. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.61721/0.80332. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62075/0.80734. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.61752/0.80717. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.61677/0.80550. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.61651/0.81184. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.61661/0.81041. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.61465/0.81290. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.61022/0.81559. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.60798/0.82903. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.60845/0.83065. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.60499/0.83187. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.61033/0.83202. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.60774/0.83360. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.60554/0.83651. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.60444/0.83858. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.60118/0.83925. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.60074/0.83951. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60142/0.84036. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.59886/0.84562. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.59604/0.85242. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.59419/0.85020. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.59649/0.85359. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.59189/0.85814. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59358/0.85793. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.58875/0.86319. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.59535/0.86783. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.58700/0.87017. Took 0.19 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69311/0.69943. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69113/0.70090. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69103/0.70037. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69001/0.70058. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68966/0.70099. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68951/0.70154. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68856/0.70150. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68814/0.70130. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68805/0.70161. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68768/0.70147. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68802/0.70183. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68722/0.70199. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68579/0.70231. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68672/0.70258. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68699/0.70268. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68554/0.70256. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68595/0.70220. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68624/0.70308. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68535/0.70310. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68513/0.70317. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68545/0.70321. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68473/0.70316. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68367/0.70403. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68432/0.70343. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68299/0.70377. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68279/0.70404. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68213/0.70421. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68264/0.70483. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68173/0.70422. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68088/0.70458. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68029/0.70528. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67923/0.70505. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67792/0.70528. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67911/0.70555. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67836/0.70616. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67771/0.70416. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67696/0.70491. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67654/0.70465. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67687/0.70481. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67437/0.70639. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67508/0.70532. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67260/0.70610. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67298/0.70802. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67284/0.70637. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67216/0.70583. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67378/0.70618. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66907/0.70839. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67059/0.70804. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67175/0.70664. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66985/0.70616. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66910/0.70589. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66838/0.70887. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66905/0.70537. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66581/0.70678. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66721/0.70547. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66440/0.70746. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66800/0.70534. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66357/0.70561. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 0.66383/0.70567. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66309/0.70783. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66275/0.70481. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66194/0.70424. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65852/0.70407. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65868/0.70520. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65779/0.70570. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65956/0.70455. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66043/0.70684. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65553/0.70384. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65634/0.70710. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65410/0.70417. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65643/0.70649. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65196/0.70836. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65572/0.70473. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65242/0.70577. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64885/0.70722. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65337/0.70690. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64962/0.70936. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64702/0.70864. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64749/0.70803. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64742/0.70892. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.64585/0.70874. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64568/0.70869. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.64485/0.70765. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64283/0.70806. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64183/0.71113. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.64299/0.70759. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64176/0.70823. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63753/0.70942. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.63779/0.70700. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.63716/0.71100. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63947/0.71079. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64054/0.71070. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63325/0.71213. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63362/0.71022. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62787/0.71180. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63551/0.71274. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63240/0.71178. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.63256/0.71311. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63443/0.71564. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62767/0.71190. Took 0.19 sec\n",
      "ACC: 0.4479166666666667\n",
      "Epoch 0, Loss(train/val) 0.69435/0.70088. Took 0.25 sec\n",
      "Epoch 1, Loss(train/val) 0.69404/0.69975. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69331/0.70057. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69183/0.70114. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69205/0.70170. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69181/0.70221. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69127/0.70284. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69125/0.70353. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69126/0.70425. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68993/0.70439. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68970/0.70500. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68940/0.70593. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68962/0.70643. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68890/0.70706. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68877/0.70788. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68727/0.70797. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68824/0.70916. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68753/0.71005. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68639/0.71029. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68706/0.71081. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68650/0.71136. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68657/0.71244. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68525/0.71235. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68568/0.71295. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68547/0.71400. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68454/0.71459. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68431/0.71553. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68314/0.71610. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68343/0.71609. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68315/0.71663. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68366/0.71688. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68215/0.71683. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68048/0.71700. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67981/0.71764. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68199/0.71958. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68133/0.71952. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67989/0.72079. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67939/0.72110. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67906/0.72336. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67871/0.72198. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67906/0.72324. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67692/0.72416. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67772/0.72370. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67425/0.72476. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67658/0.72706. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67491/0.72594. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67438/0.72712. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67559/0.72849. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67183/0.72910. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67180/0.73112. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67201/0.72976. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67289/0.73198. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67005/0.73368. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67183/0.73014. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66991/0.73210. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66881/0.73200. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66806/0.73474. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66801/0.73463. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66707/0.73608. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66811/0.73467. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66532/0.73445. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66645/0.73825. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66587/0.73726. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66389/0.73758. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66479/0.73920. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66389/0.74192. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66379/0.74346. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66152/0.74535. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66221/0.74419. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66149/0.74699. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66027/0.74716. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66003/0.74791. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65920/0.74954. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65400/0.74870. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65731/0.75322. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65610/0.74889. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65756/0.75116. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65745/0.75280. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65693/0.75390. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65338/0.75827. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65377/0.75634. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65136/0.75511. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65070/0.75656. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65157/0.75760. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65095/0.75975. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64889/0.75745. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65278/0.76107. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64905/0.76229. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64838/0.76385. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64995/0.76398. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64800/0.76570. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64647/0.76833. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64227/0.76875. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64244/0.76880. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.64243/0.77215. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64251/0.76976. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64102/0.77448. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64498/0.77297. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64033/0.77547. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63989/0.77376. Took 0.18 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69382/0.70040. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69236/0.70104. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69216/0.70138. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69184/0.70182. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69116/0.70230. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69132/0.70253. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69070/0.70273. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69056/0.70313. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69035/0.70364. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68959/0.70390. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68908/0.70436. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68911/0.70468. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68928/0.70505. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68764/0.70506. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68786/0.70598. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68772/0.70625. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68758/0.70686. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68710/0.70698. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68627/0.70762. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68702/0.70760. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68646/0.70822. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68648/0.70868. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68593/0.70948. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68522/0.70966. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68491/0.71017. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68457/0.71103. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68432/0.71111. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68397/0.71111. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68349/0.71213. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68256/0.71310. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68229/0.71384. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68239/0.71384. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68104/0.71455. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68122/0.71569. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68006/0.71643. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67953/0.71570. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68019/0.71642. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68002/0.71762. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67837/0.71740. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67868/0.71869. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67662/0.71894. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67532/0.72008. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67528/0.72280. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67552/0.72262. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67357/0.72351. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67438/0.72439. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67500/0.72573. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67305/0.72680. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67004/0.72772. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67081/0.72925. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67065/0.73057. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67163/0.73065. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66812/0.73177. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66753/0.73296. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66732/0.73599. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66553/0.73594. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66734/0.73768. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.66547/0.73625. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66289/0.73878. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66310/0.73948. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66172/0.74281. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66325/0.74213. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66282/0.74457. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66002/0.74424. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65790/0.74687. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65825/0.74855. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65816/0.74895. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65651/0.75184. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65561/0.75510. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65606/0.75726. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65601/0.75762. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65228/0.75760. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65348/0.75744. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65229/0.76080. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64919/0.76138. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65005/0.76536. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64969/0.76818. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.64885/0.77149. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64722/0.77133. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64716/0.77455. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64376/0.77381. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64276/0.77755. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64596/0.77992. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64507/0.78213. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64129/0.78162. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64043/0.78246. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63412/0.78737. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63752/0.78856. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63909/0.79245. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63703/0.79440. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.63386/0.79671. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63388/0.79476. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62937/0.79896. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63031/0.79905. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62967/0.80423. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63109/0.80774. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62774/0.81402. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62836/0.81014. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62715/0.81760. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62561/0.81893. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69331/0.69633. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69129/0.69758. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69019/0.69893. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69049/0.70047. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68951/0.70165. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68989/0.70300. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68864/0.70429. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68877/0.70549. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68728/0.70669. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68790/0.70726. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68741/0.70768. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68678/0.70854. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68657/0.70939. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68655/0.70991. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68687/0.71022. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68583/0.71097. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68470/0.71138. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68482/0.71160. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68457/0.71224. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68318/0.71288. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68415/0.71295. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68326/0.71303. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68330/0.71314. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68312/0.71355. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68171/0.71390. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68221/0.71429. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68055/0.71483. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67930/0.71573. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68072/0.71623. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67998/0.71678. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67900/0.71648. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67958/0.71659. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67761/0.71773. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67761/0.71825. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67782/0.71862. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67672/0.71915. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67539/0.71966. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67557/0.72081. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67495/0.72188. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67514/0.72204. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67515/0.72222. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67382/0.72329. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67406/0.72419. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67250/0.72427. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67182/0.72509. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67271/0.72603. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66961/0.72752. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67177/0.72713. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67190/0.72677. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67094/0.72761. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66975/0.72799. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66687/0.72908. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66919/0.73013. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66984/0.73004. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66552/0.73007. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66625/0.73048. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66355/0.73135. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66674/0.73283. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66496/0.73460. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66530/0.73448. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66224/0.73506. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66321/0.73493. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66256/0.73493. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66134/0.73751. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66146/0.73814. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66207/0.73904. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66020/0.73985. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66019/0.74025. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65813/0.74087. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65747/0.74190. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66047/0.74312. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65826/0.74363. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65653/0.74213. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65701/0.74440. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65357/0.74576. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.65611/0.74613. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65587/0.74529. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65495/0.74563. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65384/0.74717. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.65249/0.74826. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65141/0.74919. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65118/0.74948. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64941/0.75231. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64943/0.75032. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64946/0.75292. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.64911/0.75209. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64801/0.75282. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64788/0.75588. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64760/0.75542. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64441/0.75539. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64591/0.75502. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64530/0.75631. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64134/0.75758. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64324/0.75718. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64165/0.75749. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 0.64235/0.75796. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.63930/0.75827. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63690/0.75939. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63756/0.76081. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.63520/0.76149. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69543/0.69322. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 0.69362/0.69222. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69342/0.69221. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69192/0.69189. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.69148/0.69177. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69017/0.69172. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68972/0.69191. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68892/0.69217. Took 0.22 sec\n",
      "Epoch 8, Loss(train/val) 0.68964/0.69254. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68907/0.69289. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68904/0.69320. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68839/0.69333. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68697/0.69375. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68681/0.69408. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.68594/0.69473. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68671/0.69536. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68639/0.69589. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68449/0.69638. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68494/0.69702. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68447/0.69757. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 0.68333/0.69828. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68362/0.69906. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68338/0.69951. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68277/0.70035. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.68127/0.70132. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68168/0.70229. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68048/0.70290. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67973/0.70348. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67999/0.70469. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67935/0.70540. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67889/0.70631. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.67656/0.70714. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67634/0.70816. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67543/0.70911. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67469/0.71015. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.67368/0.71081. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67344/0.71107. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.67252/0.71264. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67349/0.71286. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.67158/0.71269. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67047/0.71245. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66951/0.71352. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66889/0.71413. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66517/0.71466. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66614/0.71499. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66483/0.71481. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.66360/0.71579. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.66338/0.71525. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65929/0.71420. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.66179/0.71292. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65944/0.71208. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.65875/0.71191. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.65537/0.71237. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.65380/0.71173. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.65432/0.71131. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.65328/0.71108. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65430/0.71035. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.65011/0.71179. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65309/0.71100. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.65067/0.71064. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.64995/0.71026. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64342/0.71228. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64236/0.70872. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.64603/0.70848. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64654/0.70715. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.64194/0.70840. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64040/0.70927. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.64109/0.70812. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.63750/0.71063. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63950/0.70925. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63673/0.70888. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.63381/0.71261. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63602/0.70939. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.63168/0.70999. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.63034/0.70945. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62755/0.70945. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.62661/0.70831. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62627/0.71117. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62635/0.71325. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62649/0.71338. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62409/0.71258. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.62252/0.71620. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.62077/0.71552. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61597/0.71479. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.62166/0.71208. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61986/0.71524. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61980/0.71527. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61525/0.71516. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61152/0.71818. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61051/0.71920. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.61515/0.71547. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61321/0.71525. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.61307/0.71884. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61318/0.71743. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.60842/0.71565. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60868/0.71915. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.60760/0.71816. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60751/0.72037. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60301/0.71978. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60424/0.72020. Took 0.20 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69351/0.69689. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69308/0.69631. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69188/0.69644. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69148/0.69710. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69067/0.69772. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69115/0.69908. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69071/0.69971. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68986/0.70009. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68915/0.70098. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.69123/0.70166. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68929/0.70235. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68803/0.70369. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68856/0.70411. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68835/0.70484. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68801/0.70543. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68780/0.70564. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68806/0.70585. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68776/0.70641. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68699/0.70731. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68722/0.70763. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68621/0.70748. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68623/0.70675. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68707/0.70794. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68464/0.70762. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68553/0.70862. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68525/0.70831. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68492/0.70861. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68433/0.70947. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68365/0.70924. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68450/0.70934. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68394/0.70955. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68237/0.70921. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68220/0.70931. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68262/0.70933. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68178/0.70941. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68240/0.70902. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68267/0.70917. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68187/0.70957. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68101/0.70967. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68119/0.70904. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68092/0.70886. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67946/0.71065. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68047/0.70984. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68021/0.71017. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67855/0.71032. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67791/0.71020. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67668/0.71019. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67735/0.71063. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67570/0.71054. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67675/0.71135. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67562/0.70999. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67515/0.71034. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67443/0.71173. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67200/0.71153. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67339/0.71136. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67418/0.71124. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67302/0.71258. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67334/0.71312. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67167/0.71479. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67381/0.71381. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66980/0.71300. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66937/0.71342. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67110/0.71108. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67026/0.71111. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66885/0.70947. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66976/0.70967. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67061/0.71136. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67117/0.71134. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66855/0.71056. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66677/0.71070. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66758/0.71217. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66654/0.71071. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66643/0.71113. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66435/0.71302. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66679/0.71239. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66510/0.71057. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66386/0.71173. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66107/0.71302. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66251/0.71436. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66164/0.71230. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66321/0.71333. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66216/0.71397. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66214/0.71281. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66175/0.71328. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66415/0.71612. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66087/0.71590. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65997/0.71274. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65833/0.71516. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66210/0.71250. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65863/0.71384. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66061/0.71230. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66137/0.71450. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65631/0.71474. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65816/0.71324. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65476/0.71534. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65324/0.71340. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65272/0.71525. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65462/0.71440. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65273/0.71988. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.65329/0.71900. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69565/0.69374. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69393/0.69229. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69206/0.69168. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69101/0.69133. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69117/0.69110. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69074/0.69082. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69061/0.69061. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69003/0.69043. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68934/0.69014. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68909/0.68988. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68885/0.68957. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68776/0.68930. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68655/0.68913. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68696/0.68888. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68615/0.68872. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68519/0.68866. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68440/0.68875. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68506/0.68888. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68379/0.68895. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68483/0.68897. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68342/0.68902. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68317/0.68930. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68387/0.68956. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68271/0.68962. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68285/0.68977. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68207/0.69003. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68089/0.69019. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68007/0.69047. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68073/0.69056. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67989/0.69120. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67915/0.69192. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67845/0.69265. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67810/0.69321. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67787/0.69372. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67704/0.69475. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67627/0.69486. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67733/0.69535. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67644/0.69614. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67571/0.69729. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67511/0.69815. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67552/0.69877. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67261/0.69909. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67284/0.69969. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67221/0.70118. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67012/0.70192. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67032/0.70255. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66740/0.70361. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66885/0.70484. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66799/0.70546. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66783/0.70635. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66901/0.70715. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66903/0.70794. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66625/0.70822. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66623/0.70935. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66384/0.71067. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66322/0.71155. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66326/0.71273. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66228/0.71409. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66146/0.71475. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66000/0.71628. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65936/0.71729. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65820/0.71706. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66001/0.71890. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65647/0.71797. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65695/0.71930. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65572/0.72174. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65737/0.72286. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65386/0.72225. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65161/0.72223. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65485/0.72441. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65247/0.72481. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64917/0.72502. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64814/0.72518. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64753/0.72616. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64885/0.72712. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64744/0.72816. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64393/0.72910. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64579/0.73113. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64077/0.73164. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.64076/0.73262. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63872/0.73350. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63888/0.73607. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63739/0.73558. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63736/0.73623. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63585/0.73838. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63309/0.73861. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63217/0.74075. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63062/0.74112. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.62959/0.74516. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.62866/0.74668. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.62816/0.74636. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62448/0.74936. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62672/0.74747. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62508/0.75066. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.62456/0.74806. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62329/0.74975. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62231/0.75425. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.62000/0.75541. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61552/0.75550. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.61865/0.75308. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69237/0.69248. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69069/0.69177. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68944/0.69164. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68864/0.69151. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68915/0.69170. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68742/0.69181. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68696/0.69194. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68596/0.69213. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68551/0.69231. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68574/0.69233. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68522/0.69258. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68506/0.69293. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68431/0.69329. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68346/0.69382. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68248/0.69444. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68271/0.69461. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68333/0.69495. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68188/0.69538. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68188/0.69567. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68152/0.69596. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68094/0.69641. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.67970/0.69701. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68129/0.69750. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67898/0.69771. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67966/0.69768. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67860/0.69815. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67807/0.69866. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67636/0.69915. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67768/0.69917. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67552/0.69988. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67695/0.70002. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67581/0.70022. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67532/0.70020. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67550/0.70043. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67340/0.70065. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 0.67322/0.70052. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67297/0.70109. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67268/0.70157. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67296/0.70148. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67081/0.70173. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67120/0.70217. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67235/0.70223. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67010/0.70243. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.66835/0.70304. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66726/0.70327. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66684/0.70415. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66626/0.70433. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66540/0.70485. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66713/0.70446. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66518/0.70508. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66438/0.70571. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66356/0.70594. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66194/0.70711. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66098/0.70704. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65927/0.70801. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65993/0.70764. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65808/0.70834. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65842/0.70697. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65828/0.70717. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65713/0.70764. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65445/0.70900. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.65342/0.71101. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65234/0.71182. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65237/0.71208. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65205/0.71226. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64700/0.71157. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64628/0.71248. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.64604/0.71497. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64526/0.71765. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.64208/0.71815. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.64320/0.71942. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.64214/0.72184. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63762/0.72330. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.63717/0.72466. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.63864/0.72480. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63368/0.72910. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63579/0.72962. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.63032/0.73002. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.62950/0.73091. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62688/0.73221. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62724/0.73653. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.62472/0.73779. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.62501/0.74143. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62507/0.74316. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.62041/0.74462. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61906/0.74544. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.61717/0.74944. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62100/0.75169. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61821/0.75541. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61580/0.75727. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.61405/0.75772. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61370/0.75955. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.61747/0.76474. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61104/0.76807. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 0.60967/0.77164. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60782/0.77163. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.60629/0.77116. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60584/0.77304. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60189/0.77395. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60225/0.77906. Took 0.19 sec\n",
      "ACC: 0.3125\n",
      "Epoch 0, Loss(train/val) 0.69593/0.69518. Took 0.33 sec\n",
      "Epoch 1, Loss(train/val) 0.69194/0.69639. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.69079/0.69743. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68904/0.69867. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68883/0.70001. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68771/0.70141. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68688/0.70304. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68642/0.70465. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68594/0.70615. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68474/0.70783. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68546/0.70913. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68470/0.71045. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68568/0.71156. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68479/0.71252. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68380/0.71366. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68305/0.71438. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68300/0.71553. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68155/0.71682. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68219/0.71784. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68156/0.71857. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68114/0.71925. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68071/0.71999. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68003/0.72066. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67900/0.72111. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67809/0.72213. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67993/0.72224. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67868/0.72259. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67850/0.72290. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67717/0.72312. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67792/0.72297. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67721/0.72313. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67619/0.72367. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67713/0.72363. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67598/0.72353. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67561/0.72379. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67432/0.72410. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67539/0.72441. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67335/0.72444. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67324/0.72470. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67323/0.72454. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67279/0.72476. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67194/0.72569. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67287/0.72562. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67016/0.72574. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66941/0.72636. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66863/0.72668. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67058/0.72657. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67013/0.72655. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66887/0.72655. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66671/0.72710. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66946/0.72705. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66604/0.72743. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66706/0.72824. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66602/0.72814. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66522/0.72834. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66476/0.72739. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66381/0.72770. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66190/0.72772. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66053/0.72853. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66069/0.72929. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66026/0.73033. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66046/0.73033. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65852/0.72923. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65729/0.72937. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65645/0.72930. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65592/0.73014. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.65438/0.73081. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65303/0.73108. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65154/0.73074. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65060/0.73027. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65153/0.73257. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64985/0.73238. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64643/0.73445. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64335/0.73362. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64263/0.73213. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.64517/0.73293. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64145/0.73209. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.63879/0.73450. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63680/0.73514. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63635/0.73544. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63588/0.73694. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63411/0.73742. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63212/0.73846. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63018/0.73981. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63021/0.74199. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62767/0.74075. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62579/0.74320. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.62691/0.74486. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62515/0.74152. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62401/0.74597. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61598/0.74696. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61900/0.74726. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61924/0.74668. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.61206/0.74570. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61374/0.74670. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60967/0.74984. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61347/0.75225. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61087/0.75005. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.60769/0.74753. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60942/0.74984. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69329/0.68350. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69182/0.68325. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69054/0.68375. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68963/0.68413. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68908/0.68437. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68859/0.68514. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68774/0.68569. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68707/0.68635. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68765/0.68693. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68693/0.68735. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68630/0.68814. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68650/0.68925. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68532/0.68972. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68596/0.69004. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68464/0.69016. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68483/0.69121. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68443/0.69099. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68456/0.69107. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68380/0.69151. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68305/0.69179. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68336/0.69199. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68314/0.69209. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68294/0.69261. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68108/0.69254. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68125/0.69287. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68191/0.69343. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68099/0.69298. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68090/0.69367. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68076/0.69359. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67962/0.69407. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67819/0.69408. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67839/0.69508. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67961/0.69485. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.67795/0.69521. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67759/0.69568. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67758/0.69542. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67627/0.69515. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67594/0.69547. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67525/0.69581. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67364/0.69577. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67698/0.69610. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67338/0.69582. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67126/0.69661. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67210/0.69529. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67141/0.69671. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67091/0.69534. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66909/0.69563. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66842/0.69556. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67043/0.69577. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66721/0.69586. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66735/0.69559. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66573/0.69630. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66533/0.69638. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66403/0.69582. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66273/0.69651. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66058/0.69676. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66309/0.69641. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66213/0.69637. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66264/0.69904. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65944/0.69569. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65676/0.69771. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65628/0.69737. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65692/0.69806. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65606/0.70119. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65614/0.69992. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.65522/0.69824. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65366/0.70139. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.65312/0.70101. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65292/0.69884. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65098/0.70275. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65119/0.69881. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65100/0.70239. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65380/0.70232. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.64944/0.70364. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64781/0.69877. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64690/0.69877. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64628/0.69862. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64315/0.69927. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64326/0.70035. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64421/0.70372. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64459/0.70224. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.64020/0.70245. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64403/0.70121. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64104/0.70329. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63880/0.70351. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.63870/0.70151. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64119/0.70569. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63943/0.70404. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.63908/0.70243. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.63410/0.70782. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.63608/0.70542. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.63565/0.70876. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63380/0.70548. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.63393/0.70623. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63513/0.70420. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63214/0.70882. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62748/0.70532. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62916/0.70883. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63342/0.70885. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62955/0.70795. Took 0.18 sec\n",
      "ACC: 0.5625\n",
      "Epoch 0, Loss(train/val) 0.69385/0.69322. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69236/0.69096. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69103/0.69089. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69094/0.69034. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69029/0.68988. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68911/0.68983. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68926/0.68933. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68872/0.68956. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68815/0.68942. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68829/0.68911. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 0.68772/0.68907. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68766/0.68971. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68749/0.68951. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68719/0.68928. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 0.68659/0.68935. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68658/0.68963. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68610/0.69049. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68637/0.69105. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68518/0.69168. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68593/0.69189. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68513/0.69210. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68493/0.69275. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 0.68523/0.69239. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68517/0.69286. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 0.68434/0.69376. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68428/0.69346. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68335/0.69432. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68400/0.69431. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.68278/0.69475. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68286/0.69519. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.68264/0.69576. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68179/0.69587. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68185/0.69582. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68292/0.69599. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68113/0.69588. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68280/0.69643. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 0.68080/0.69737. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68112/0.69736. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 0.68091/0.69720. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67988/0.69804. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68095/0.69918. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.67832/0.69915. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67878/0.69950. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67949/0.69922. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67921/0.69956. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.67826/0.70013. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67739/0.70096. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.67712/0.70091. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67776/0.70097. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67692/0.70180. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67564/0.70348. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.67416/0.70304. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.67484/0.70374. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67454/0.70389. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67337/0.70289. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67344/0.70369. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67434/0.70431. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.67334/0.70495. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67313/0.70617. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.67101/0.70575. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67162/0.70573. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.67061/0.70733. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 0.67091/0.70831. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66874/0.70704. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67042/0.70841. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66785/0.71012. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66665/0.71082. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66616/0.71175. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66550/0.71244. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66537/0.71232. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66476/0.71391. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66634/0.71281. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66386/0.71430. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66485/0.71489. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66338/0.71474. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66119/0.71500. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66209/0.71609. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66159/0.71417. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66144/0.71702. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65959/0.71752. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66007/0.71905. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65902/0.71875. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65742/0.71906. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65843/0.71961. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.65716/0.72149. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65860/0.72448. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65634/0.72374. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65575/0.72398. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65543/0.72590. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65497/0.72777. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.65061/0.72845. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65091/0.72949. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64945/0.72995. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.64841/0.73205. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65164/0.73298. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.65057/0.73201. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64817/0.73323. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64934/0.73582. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64786/0.73635. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.64739/0.73730. Took 0.21 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69222/0.69145. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68811/0.68943. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68740/0.68929. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68731/0.68947. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68693/0.68972. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68589/0.68997. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68605/0.69038. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68527/0.69068. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68602/0.69096. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68473/0.69123. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68436/0.69158. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68405/0.69195. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68327/0.69225. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68298/0.69239. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68321/0.69258. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68281/0.69260. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68243/0.69235. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68188/0.69246. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68090/0.69271. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68074/0.69317. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68065/0.69261. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67934/0.69235. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68023/0.69268. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67924/0.69246. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67675/0.69208. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67744/0.69259. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67521/0.69214. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67439/0.69232. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67433/0.69273. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67151/0.69328. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67103/0.69278. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67039/0.69413. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66785/0.69490. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66969/0.69437. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66694/0.69368. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66415/0.69536. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66406/0.69707. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66125/0.69794. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.65918/0.69903. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.65784/0.69977. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.65866/0.69616. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.65683/0.70110. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.65591/0.70294. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.65297/0.69931. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65211/0.70172. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65161/0.70505. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.64757/0.70287. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.64631/0.70902. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.64414/0.70632. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.64258/0.71202. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.64456/0.70794. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.64586/0.71286. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.63949/0.71060. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64388/0.71441. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.63826/0.71405. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.63789/0.71523. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.63638/0.71744. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.63360/0.71817. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.63147/0.71837. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.63465/0.71634. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.63459/0.71554. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.63133/0.72078. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.62991/0.72437. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.63416/0.72540. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.62618/0.72070. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.62740/0.72307. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.62833/0.72613. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.62414/0.72904. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.62576/0.72801. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.62691/0.72526. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.62184/0.72788. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.62254/0.73079. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62098/0.73225. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62134/0.73397. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62253/0.73177. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.61774/0.73315. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.61514/0.73213. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.61271/0.73455. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.61434/0.73579. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61203/0.73599. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.61298/0.73567. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61032/0.73644. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.60895/0.74044. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.60976/0.74155. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.60845/0.73890. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.61029/0.73860. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.60306/0.73894. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.60645/0.73919. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60457/0.73751. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60565/0.74139. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60261/0.74379. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.60021/0.74715. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.59844/0.74430. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.60404/0.74607. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.59843/0.74704. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.59249/0.74932. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.59086/0.75122. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.59400/0.75246. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.58915/0.75121. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.59394/0.75277. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.68969/0.69415. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68758/0.69576. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68677/0.69506. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68605/0.69421. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68531/0.69331. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68487/0.69228. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68485/0.69132. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68442/0.69098. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68398/0.69020. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68361/0.68935. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68314/0.68914. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68315/0.68895. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68343/0.68823. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68259/0.68805. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68266/0.68766. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68217/0.68741. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68228/0.68681. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68190/0.68648. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68168/0.68637. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68075/0.68627. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68097/0.68584. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68062/0.68595. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68057/0.68620. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68057/0.68593. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67968/0.68600. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67942/0.68549. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67903/0.68527. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67942/0.68555. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68018/0.68527. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67996/0.68517. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67842/0.68518. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67715/0.68467. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67800/0.68517. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67837/0.68433. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67716/0.68436. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67753/0.68406. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67787/0.68457. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67577/0.68445. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67654/0.68523. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67650/0.68482. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67708/0.68486. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67550/0.68455. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67545/0.68421. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67584/0.68387. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67461/0.68412. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67432/0.68337. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.67410/0.68334. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67446/0.68401. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.67344/0.68364. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.67509/0.68445. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.67269/0.68378. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67278/0.68380. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.67262/0.68428. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.67198/0.68349. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67065/0.68372. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.67163/0.68357. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67014/0.68329. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67003/0.68395. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67075/0.68427. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.66968/0.68340. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.67057/0.68315. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66897/0.68400. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66975/0.68369. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66911/0.68432. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66745/0.68479. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66656/0.68386. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66873/0.68483. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66625/0.68467. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66681/0.68434. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.66683/0.68382. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66521/0.68444. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.66501/0.68446. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 0.66321/0.68440. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66495/0.68370. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.66374/0.68541. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.66287/0.68478. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66253/0.68595. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.66144/0.68424. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.66137/0.68572. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.65978/0.68490. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66001/0.68508. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.66015/0.68357. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65972/0.68493. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.65649/0.68652. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65782/0.68567. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.65592/0.68562. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.65659/0.68534. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.65597/0.68391. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.65420/0.68614. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.65515/0.68449. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.65821/0.68538. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.65236/0.68557. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.65289/0.68770. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.65062/0.68774. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65281/0.68501. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.64927/0.68583. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.64900/0.68519. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.64808/0.68506. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.64727/0.68643. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.65115/0.68747. Took 0.19 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69018/0.70530. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68793/0.70709. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.68659/0.70739. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68657/0.70750. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68633/0.70809. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 0.68531/0.70881. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68520/0.70868. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68550/0.70944. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68372/0.71042. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.68380/0.71087. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68356/0.71128. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68294/0.71145. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68314/0.71197. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68276/0.71232. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 0.68186/0.71257. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68154/0.71269. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68167/0.71274. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.68092/0.71357. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68096/0.71327. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68137/0.71292. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68093/0.71329. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67986/0.71324. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68070/0.71331. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67924/0.71326. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 0.67871/0.71331. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67895/0.71337. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 0.67811/0.71246. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67761/0.71297. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 0.67727/0.71281. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67555/0.71306. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 0.67435/0.71250. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67479/0.71235. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67419/0.71233. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67349/0.71235. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67212/0.71178. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67112/0.71188. Took 0.22 sec\n",
      "Epoch 36, Loss(train/val) 0.67187/0.71196. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.66945/0.71251. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.67043/0.71173. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66819/0.71202. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.66832/0.71048. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66550/0.71149. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 0.66678/0.71061. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66545/0.70937. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.66529/0.70937. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66252/0.70535. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 0.66152/0.70615. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65971/0.70643. Took 0.22 sec\n",
      "Epoch 48, Loss(train/val) 0.65939/0.70432. Took 0.22 sec\n",
      "Epoch 49, Loss(train/val) 0.65636/0.70448. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.65540/0.70359. Took 0.22 sec\n",
      "Epoch 51, Loss(train/val) 0.65513/0.70229. Took 0.22 sec\n",
      "Epoch 52, Loss(train/val) 0.65331/0.70198. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.65287/0.70173. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65226/0.70158. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64930/0.69970. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.64882/0.70034. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.64875/0.69743. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 0.64719/0.69757. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.64642/0.69525. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.64572/0.69648. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.64612/0.69587. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64375/0.69641. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.64153/0.69822. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64448/0.69513. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64030/0.69382. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 0.63799/0.69415. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.63768/0.69339. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.63784/0.69305. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 0.63416/0.69494. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63220/0.69734. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.63660/0.69805. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.63427/0.69520. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.63285/0.69499. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62939/0.69679. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.63249/0.69780. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62711/0.69813. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62962/0.69993. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62827/0.70029. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62672/0.69702. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62431/0.69901. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62547/0.69971. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.62368/0.70134. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62411/0.69717. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.61884/0.69768. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.62009/0.70008. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.62023/0.70093. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61905/0.70077. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62059/0.70146. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61892/0.70079. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 0.61798/0.70137. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61833/0.70318. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.61335/0.70430. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.61887/0.70463. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.61023/0.70343. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.61079/0.70494. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.61096/0.70474. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60672/0.70546. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.60653/0.70813. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.60462/0.70752. Took 0.19 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69177/0.68451. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68953/0.68347. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68951/0.68285. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68788/0.68248. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68740/0.68230. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68730/0.68219. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.68613/0.68189. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68552/0.68157. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68631/0.68165. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68476/0.68190. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68344/0.68201. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 0.68290/0.68194. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68271/0.68198. Took 0.18 sec\n",
      "Epoch 13, Loss(train/val) 0.68071/0.68228. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68121/0.68292. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68070/0.68298. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.67924/0.68348. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 0.67881/0.68333. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.67770/0.68358. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.67801/0.68457. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.67751/0.68425. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.67581/0.68409. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67560/0.68532. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 0.67506/0.68506. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67419/0.68509. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67241/0.68504. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67176/0.68537. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.67196/0.68526. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67134/0.68608. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.66991/0.68701. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.66629/0.68740. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.66714/0.68643. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.66420/0.68554. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.66390/0.68526. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.66312/0.68639. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.66150/0.68590. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.65871/0.68632. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.65619/0.68755. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.65504/0.68734. Took 0.18 sec\n",
      "Epoch 39, Loss(train/val) 0.65350/0.68956. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.65279/0.68907. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.65020/0.68721. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.64715/0.68922. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.64748/0.69013. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.64663/0.68976. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 0.64497/0.68995. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.64180/0.69058. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 0.64425/0.69335. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.64083/0.69453. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 0.63713/0.69444. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 0.63771/0.69466. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 0.63720/0.69518. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.63193/0.69571. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 0.63724/0.69310. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.63510/0.69495. Took 0.18 sec\n",
      "Epoch 55, Loss(train/val) 0.63351/0.69776. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.63507/0.69785. Took 0.18 sec\n",
      "Epoch 57, Loss(train/val) 0.62825/0.70203. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.63031/0.69898. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 0.62867/0.69987. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.62885/0.70180. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.62347/0.70257. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.62538/0.70248. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.62320/0.70529. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.62546/0.70236. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.62236/0.70239. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.62195/0.70565. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.62183/0.71032. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.61923/0.70444. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.62129/0.70694. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.62012/0.70794. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.61423/0.71235. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.61635/0.70928. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.61669/0.71196. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.61261/0.70485. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.61611/0.71214. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.61253/0.70857. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.61365/0.71119. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.61052/0.71168. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.60973/0.71106. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.61122/0.71084. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61033/0.71337. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.60493/0.71462. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.60589/0.71694. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.60522/0.71966. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.60439/0.71617. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.60469/0.71595. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.60233/0.72037. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.59516/0.71414. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.59241/0.72143. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.59854/0.71920. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.59906/0.71674. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.59626/0.72037. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.59634/0.72545. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.59468/0.72415. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.59257/0.72107. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59408/0.72606. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.59125/0.71837. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.59477/0.72882. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59410/0.73020. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69266/0.69257. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69151/0.69231. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69117/0.69212. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69029/0.69203. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68994/0.69189. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68948/0.69186. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68829/0.69186. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68804/0.69187. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68798/0.69166. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68768/0.69130. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68562/0.69090. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68659/0.69050. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68572/0.69000. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68557/0.68957. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68490/0.68899. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68402/0.68846. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68481/0.68775. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68291/0.68718. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68368/0.68654. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68200/0.68617. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68240/0.68556. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68132/0.68498. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68161/0.68458. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68061/0.68390. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68006/0.68298. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.67848/0.68233. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67872/0.68166. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67727/0.68114. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67724/0.68025. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67605/0.67957. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67530/0.67909. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67513/0.67880. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67336/0.67792. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.67145/0.67727. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67116/0.67637. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.66930/0.67581. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.66960/0.67481. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.66636/0.67341. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66590/0.67232. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66271/0.67119. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66419/0.66983. Took 0.22 sec\n",
      "Epoch 41, Loss(train/val) 0.66223/0.66735. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66171/0.66612. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.65828/0.66365. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.65806/0.66328. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.65752/0.66265. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.65610/0.66084. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65472/0.65972. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.65208/0.65951. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.65269/0.65776. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65109/0.65639. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.64818/0.65500. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.64923/0.65216. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 0.64789/0.65106. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.64799/0.64987. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.64538/0.65084. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64236/0.64857. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.64031/0.64678. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.64224/0.64483. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.63999/0.64626. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.64412/0.64608. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.63990/0.64393. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.63806/0.64260. Took 0.22 sec\n",
      "Epoch 63, Loss(train/val) 0.64065/0.64290. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.63694/0.64174. Took 0.22 sec\n",
      "Epoch 65, Loss(train/val) 0.63366/0.63981. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.63539/0.63862. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.63496/0.63885. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63016/0.63934. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.63157/0.63850. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.62918/0.63677. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.63291/0.63555. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.62841/0.63512. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.62978/0.63528. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62506/0.63560. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62669/0.63489. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62599/0.63418. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.62295/0.63454. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62366/0.63551. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62280/0.63636. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62562/0.63646. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.62283/0.63392. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61979/0.63342. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.62014/0.63107. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.61575/0.63087. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.61508/0.63108. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.61557/0.63112. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61719/0.62935. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61674/0.63093. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60672/0.63090. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.61476/0.63193. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61225/0.63199. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.60883/0.63198. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.60994/0.63155. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.60674/0.63112. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60930/0.63181. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.60372/0.62986. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60946/0.62957. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60399/0.62704. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60213/0.62717. Took 0.20 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69672/0.69809. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69096/0.70085. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69085/0.70163. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69054/0.70135. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68992/0.70145. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68912/0.70131. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68878/0.70086. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68729/0.70085. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68780/0.70097. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68720/0.70097. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68703/0.70094. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68489/0.70142. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.68482/0.70148. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68467/0.70158. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68449/0.70213. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68433/0.70249. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68243/0.70303. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68373/0.70315. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68150/0.70364. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68172/0.70401. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68248/0.70384. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68061/0.70396. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67914/0.70482. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67860/0.70530. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67906/0.70607. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67756/0.70714. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67814/0.70756. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 0.67659/0.70847. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67663/0.70903. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67541/0.70970. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67675/0.70995. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67430/0.71120. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.67135/0.71222. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67357/0.71221. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.67375/0.71341. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67070/0.71315. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67134/0.71226. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66899/0.71319. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67053/0.71479. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66900/0.71461. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67018/0.71583. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66791/0.71628. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66558/0.71708. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66656/0.71825. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66621/0.71891. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66440/0.71974. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66370/0.72014. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66352/0.72059. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66504/0.72103. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66254/0.72229. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66147/0.72351. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66239/0.72301. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66095/0.72453. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65788/0.72585. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.65810/0.72744. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65649/0.72739. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65786/0.72829. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65609/0.72773. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.65675/0.72858. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65497/0.72900. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65389/0.73093. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65509/0.73159. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65122/0.73317. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65000/0.73380. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65104/0.73497. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64909/0.73474. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64798/0.73673. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64556/0.73872. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64728/0.73924. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64492/0.73962. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64357/0.74154. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64484/0.73972. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64155/0.74031. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.63934/0.74319. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63944/0.74309. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63968/0.74542. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63978/0.74568. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63490/0.74764. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63483/0.74661. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63503/0.75004. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63293/0.75007. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63379/0.75065. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63136/0.75040. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63020/0.74930. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62726/0.74899. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.62592/0.75246. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62753/0.75024. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62328/0.75463. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62490/0.75496. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62260/0.75830. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62410/0.75978. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61994/0.75914. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62403/0.75767. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.62521/0.75784. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61993/0.76175. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62030/0.76080. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61805/0.75945. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60990/0.76436. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61606/0.76693. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61460/0.76556. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69202/0.69227. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68950/0.69217. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68932/0.69227. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68942/0.69235. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68849/0.69238. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68755/0.69258. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68816/0.69261. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68710/0.69273. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68609/0.69278. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68579/0.69273. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68627/0.69287. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68562/0.69297. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68422/0.69301. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68550/0.69313. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68337/0.69318. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68427/0.69305. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68213/0.69315. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68247/0.69320. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68131/0.69314. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68059/0.69314. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68101/0.69382. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68053/0.69349. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67742/0.69391. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67735/0.69433. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67618/0.69430. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67432/0.69440. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67522/0.69397. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67444/0.69442. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67383/0.69460. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67035/0.69381. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.66932/0.69404. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67219/0.69502. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66841/0.69476. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66690/0.69566. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.66460/0.69471. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66376/0.69507. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66158/0.69518. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66131/0.69571. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.65827/0.69594. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.65914/0.69606. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.65634/0.69614. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.65355/0.69812. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.65358/0.69744. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.64967/0.70032. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65122/0.69939. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.64596/0.69935. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.64808/0.69873. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.64344/0.70149. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.64113/0.70265. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.64239/0.70356. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.63927/0.70412. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.64212/0.70492. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.63609/0.70441. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.64149/0.70561. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.63922/0.70773. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.63591/0.70928. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.63343/0.70782. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.63105/0.70749. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.62966/0.71085. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.63418/0.70933. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.62908/0.70956. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.63023/0.71031. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.62449/0.71227. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.62445/0.71062. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.62685/0.71094. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.62357/0.71347. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.62458/0.71363. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.62412/0.71371. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.62170/0.71330. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 0.61703/0.71584. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.62161/0.71954. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.62066/0.71723. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.61446/0.71942. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.61745/0.71513. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.61703/0.71992. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.61317/0.72028. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.61568/0.72271. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.61649/0.72011. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.61125/0.72405. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.61383/0.72455. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.61025/0.72111. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.60924/0.72524. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.60938/0.72468. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.60688/0.72373. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 0.60457/0.72168. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.60174/0.72739. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.60781/0.72492. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.60300/0.72816. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.60559/0.73355. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.59781/0.72968. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60172/0.73117. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.60481/0.73021. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.60243/0.72975. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.59564/0.73432. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.59659/0.73649. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.59877/0.73297. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59628/0.73208. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.59340/0.73636. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.59885/0.73379. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59157/0.73463. Took 0.20 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69178/0.69195. Took 0.36 sec\n",
      "Epoch 1, Loss(train/val) 0.69237/0.69233. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69199/0.69287. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69196/0.69341. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69038/0.69390. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69118/0.69450. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69020/0.69537. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69014/0.69602. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69000/0.69677. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68956/0.69801. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68812/0.69940. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68904/0.70040. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68795/0.70177. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68845/0.70331. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68735/0.70506. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68705/0.70642. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68648/0.70737. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68563/0.70892. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68612/0.70991. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68584/0.71069. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68450/0.71195. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68539/0.71259. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68363/0.71336. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68425/0.71453. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68336/0.71570. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68258/0.71616. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68214/0.71735. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68137/0.71831. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68106/0.71957. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68039/0.72054. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68005/0.72148. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67932/0.72161. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67781/0.72258. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67860/0.72401. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67813/0.72448. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67726/0.72615. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67603/0.72669. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67531/0.72657. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67668/0.72824. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67590/0.72855. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67532/0.72868. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67563/0.72904. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67570/0.73006. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67477/0.73044. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67495/0.73087. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67345/0.73246. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67152/0.73345. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67137/0.73436. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67000/0.73469. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67089/0.73478. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67000/0.73509. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66923/0.73601. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66918/0.73661. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66873/0.73795. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66907/0.73862. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66877/0.73840. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66788/0.73804. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66632/0.73740. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66632/0.73876. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66658/0.73848. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66392/0.74011. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66597/0.74005. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66378/0.74144. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66270/0.74107. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66153/0.74066. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66287/0.73939. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66075/0.74106. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66309/0.74237. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65933/0.74288. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65854/0.74293. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65757/0.74410. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65778/0.74433. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65819/0.74420. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65724/0.74436. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65692/0.74316. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.65874/0.74286. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65487/0.74427. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.65554/0.74569. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65530/0.74719. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65368/0.74543. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65104/0.74581. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65341/0.74597. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65373/0.74625. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.64980/0.74581. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64930/0.74538. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65005/0.74489. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65063/0.74539. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64620/0.74356. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64353/0.74536. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.64696/0.74581. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64547/0.74297. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64412/0.74299. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64433/0.74451. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64279/0.74581. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64352/0.74463. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64170/0.74379. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64234/0.74653. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63894/0.74563. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63659/0.74491. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63931/0.74535. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69388/0.69662. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69213/0.69547. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69188/0.69554. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69146/0.69595. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69119/0.69631. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69013/0.69605. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69029/0.69645. Took 0.18 sec\n",
      "Epoch 7, Loss(train/val) 0.69014/0.69660. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69039/0.69642. Took 0.18 sec\n",
      "Epoch 9, Loss(train/val) 0.69013/0.69663. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68862/0.69662. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68911/0.69685. Took 0.22 sec\n",
      "Epoch 12, Loss(train/val) 0.68888/0.69667. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68861/0.69716. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68892/0.69695. Took 0.18 sec\n",
      "Epoch 15, Loss(train/val) 0.68828/0.69699. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68879/0.69708. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68826/0.69692. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68840/0.69672. Took 0.18 sec\n",
      "Epoch 19, Loss(train/val) 0.68800/0.69733. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 0.68785/0.69713. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 0.68700/0.69700. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.68657/0.69731. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68642/0.69705. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68746/0.69679. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.68701/0.69680. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68673/0.69742. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 0.68667/0.69656. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68572/0.69703. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68691/0.69723. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68616/0.69681. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.68622/0.69735. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68529/0.69745. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 0.68524/0.69697. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68454/0.69691. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.68470/0.69629. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 0.68476/0.69716. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68432/0.69713. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 0.68295/0.69679. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68474/0.69716. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.68418/0.69664. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68308/0.69693. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.68392/0.69699. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68338/0.69685. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 0.68310/0.69613. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68331/0.69664. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.68223/0.69628. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68156/0.69589. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.68115/0.69576. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68053/0.69650. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68095/0.69605. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.68139/0.69585. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.68036/0.69548. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.68054/0.69548. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67967/0.69577. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.68026/0.69563. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67885/0.69502. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.67869/0.69563. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67946/0.69592. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67835/0.69465. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.67805/0.69526. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 0.67837/0.69487. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67783/0.69477. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67809/0.69454. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.67700/0.69546. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.67448/0.69476. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.67552/0.69441. Took 0.22 sec\n",
      "Epoch 67, Loss(train/val) 0.67369/0.69580. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67666/0.69458. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67411/0.69405. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.67428/0.69451. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67436/0.69495. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67066/0.69462. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67399/0.69440. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67241/0.69399. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.67107/0.69447. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67108/0.69483. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67073/0.69488. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66960/0.69380. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.66983/0.69372. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.66726/0.69515. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66932/0.69459. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66752/0.69467. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66664/0.69577. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66525/0.69560. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66585/0.69510. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66670/0.69672. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66194/0.69794. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66359/0.69677. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66358/0.69825. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66094/0.69860. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66264/0.69895. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66082/0.69943. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65962/0.70005. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66057/0.70067. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.66077/0.70059. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65905/0.70215. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65892/0.70130. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65508/0.70084. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65404/0.70290. Took 0.18 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69367/0.69466. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69196/0.69473. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69216/0.69476. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69150/0.69482. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69168/0.69454. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69153/0.69427. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69113/0.69430. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69017/0.69409. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68953/0.69370. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69040/0.69363. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68929/0.69357. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68852/0.69305. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68776/0.69299. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68717/0.69297. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68738/0.69319. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68689/0.69306. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68632/0.69304. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68508/0.69296. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68566/0.69221. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68492/0.69209. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68367/0.69149. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68537/0.69124. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68136/0.69043. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68383/0.69064. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68279/0.69010. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68172/0.68967. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68274/0.68926. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68091/0.68879. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67860/0.68921. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67899/0.68873. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 0.67918/0.68854. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67797/0.68902. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67866/0.68868. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67605/0.68863. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67494/0.68818. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67527/0.68862. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67555/0.68769. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67429/0.68739. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67498/0.68634. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67431/0.68592. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67437/0.68582. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.67278/0.68587. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67174/0.68611. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67083/0.68541. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67093/0.68588. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66917/0.68454. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67043/0.68470. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66688/0.68444. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66684/0.68425. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.66693/0.68435. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66487/0.68430. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66574/0.68414. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66287/0.68398. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66282/0.68378. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.66086/0.68320. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 0.66006/0.68392. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65870/0.68445. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65980/0.68266. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.65719/0.68376. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65685/0.68269. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65790/0.68398. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.65364/0.68222. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 0.65698/0.68220. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65449/0.68160. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.64891/0.68259. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65097/0.68079. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64862/0.68036. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.64574/0.68015. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64294/0.68051. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.64593/0.67955. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64328/0.67906. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64363/0.67746. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.64391/0.67787. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64243/0.67893. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64218/0.67807. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64055/0.67547. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.63931/0.67916. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.63967/0.67589. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63748/0.67747. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63258/0.67672. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63207/0.67495. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63424/0.67761. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63532/0.67549. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62834/0.67606. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63140/0.67601. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.62880/0.67512. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62392/0.67676. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.62570/0.67697. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62372/0.67564. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62129/0.67545. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.61717/0.67503. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.61816/0.67788. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61779/0.67541. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.62404/0.67591. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61305/0.67808. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61384/0.67647. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61413/0.67417. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61261/0.67734. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61351/0.67778. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60964/0.67983. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69262/0.69312. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69251/0.69296. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69239/0.69281. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69210/0.69264. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69245/0.69239. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69133/0.69213. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69145/0.69186. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69060/0.69160. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69076/0.69129. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68911/0.69098. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69038/0.69070. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68893/0.69030. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68841/0.68975. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68833/0.68939. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68804/0.68901. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68835/0.68877. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68700/0.68830. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68685/0.68799. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68602/0.68784. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68476/0.68785. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68343/0.68725. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68423/0.68713. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68345/0.68722. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 0.68294/0.68712. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68290/0.68697. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68056/0.68639. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.68071/0.68690. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67834/0.68714. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68034/0.68688. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67911/0.68558. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67726/0.68711. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67794/0.68733. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67669/0.68751. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67510/0.68899. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67412/0.68922. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67362/0.68891. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67327/0.68990. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67023/0.69136. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66751/0.69192. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66929/0.69288. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66878/0.69413. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66641/0.69521. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66641/0.69595. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 0.66664/0.69668. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65970/0.69736. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.66222/0.69955. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65867/0.70076. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65555/0.70242. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65434/0.70427. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65458/0.70313. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65552/0.70367. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65188/0.70797. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65318/0.70868. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.65105/0.71124. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65290/0.70888. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64881/0.70824. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64736/0.70972. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64326/0.71457. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64589/0.71503. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.64353/0.71368. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.63906/0.71755. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.64098/0.71594. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.63937/0.71817. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64209/0.72056. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63686/0.72272. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63641/0.72004. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63425/0.72300. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.64048/0.72226. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.63636/0.72251. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.63789/0.72395. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63138/0.72634. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62892/0.72640. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62967/0.72819. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62834/0.72761. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62657/0.72841. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62892/0.72753. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62375/0.73333. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.62424/0.73757. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.62142/0.73580. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61938/0.73855. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.62193/0.74056. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61591/0.73616. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.62069/0.73984. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.61711/0.73980. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.61647/0.73961. Took 0.22 sec\n",
      "Epoch 85, Loss(train/val) 0.61495/0.74083. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61622/0.75061. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.61586/0.73905. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.61193/0.74786. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.61459/0.75154. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60827/0.74737. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.60306/0.75249. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.60775/0.75389. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.60559/0.75511. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.60175/0.75358. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60196/0.75587. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.60324/0.76059. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.60332/0.75227. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.60295/0.75883. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60022/0.75851. Took 0.18 sec\n",
      "ACC: 0.5520833333333334\n",
      "Epoch 0, Loss(train/val) 0.69352/0.69157. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69282/0.69189. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69193/0.69234. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69123/0.69251. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69080/0.69256. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68987/0.69284. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68837/0.69289. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68818/0.69310. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68785/0.69316. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68592/0.69350. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68545/0.69377. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68440/0.69400. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68431/0.69420. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68345/0.69441. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68129/0.69458. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68253/0.69469. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.67979/0.69474. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67889/0.69536. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68054/0.69550. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68061/0.69572. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67822/0.69596. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67721/0.69683. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 0.67841/0.69746. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67533/0.69854. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67404/0.69912. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 0.67542/0.69968. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 0.67218/0.70043. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.67298/0.70074. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.67216/0.70073. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 0.67049/0.70112. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67072/0.70173. Took 0.18 sec\n",
      "Epoch 31, Loss(train/val) 0.66961/0.70213. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67033/0.70327. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66784/0.70393. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.66831/0.70389. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 0.66759/0.70354. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.66635/0.70392. Took 0.18 sec\n",
      "Epoch 37, Loss(train/val) 0.66389/0.70465. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.66327/0.70506. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66543/0.70505. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.66392/0.70568. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 0.66145/0.70654. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 0.65970/0.70611. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66098/0.70693. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.66007/0.70778. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65805/0.70884. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 0.65687/0.70969. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.65693/0.70962. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 0.65556/0.70942. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65612/0.70955. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 0.65164/0.71029. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65302/0.71065. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 0.65123/0.71059. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65386/0.71108. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 0.65398/0.71105. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64926/0.71219. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.64854/0.71362. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64721/0.71521. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 0.64438/0.71466. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65055/0.71386. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.64476/0.71404. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64354/0.71539. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 0.64172/0.71589. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64223/0.71716. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.64141/0.71723. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64215/0.71758. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 0.63712/0.71738. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63948/0.71771. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.63826/0.72003. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63377/0.71983. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 0.63583/0.72330. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.63356/0.72319. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.63279/0.72402. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.63061/0.72599. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.63069/0.72455. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.62607/0.72985. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62765/0.72997. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62351/0.72972. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.62566/0.73156. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 0.62341/0.73132. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.62530/0.73145. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61585/0.73587. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.61868/0.73690. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.62013/0.73683. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.61702/0.73961. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61640/0.74040. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 0.61142/0.73999. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.61596/0.73865. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.61429/0.74118. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.61087/0.74335. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60740/0.74531. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.61029/0.74732. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.60455/0.74956. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.60657/0.74892. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.60615/0.75245. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.60171/0.75480. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.59936/0.75910. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.60299/0.76044. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.59698/0.75901. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59983/0.75899. Took 0.19 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69367/0.69680. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69277/0.69300. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69194/0.69170. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69192/0.69124. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69185/0.69143. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69072/0.69157. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69083/0.69152. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69045/0.69093. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69008/0.69127. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69027/0.69151. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68996/0.69083. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68941/0.69153. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68922/0.69067. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68872/0.69018. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68870/0.69121. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68841/0.69045. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68768/0.69102. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68742/0.68968. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68699/0.69029. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68695/0.69072. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68608/0.69006. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68800/0.69048. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68686/0.68936. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68777/0.68890. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68614/0.68907. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68599/0.68897. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68560/0.68949. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68572/0.68948. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68521/0.68924. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68487/0.68906. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68595/0.68777. Took 0.22 sec\n",
      "Epoch 31, Loss(train/val) 0.68508/0.68855. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68503/0.68827. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68490/0.68888. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68380/0.68792. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68397/0.68820. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68442/0.68816. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68358/0.68780. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68246/0.68846. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68363/0.68877. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68134/0.68771. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68168/0.68804. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68194/0.68754. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.68190/0.68707. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68265/0.68825. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68093/0.68718. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68011/0.68745. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68043/0.68583. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67870/0.68647. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67859/0.68689. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67914/0.68672. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67742/0.68686. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67723/0.68730. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67754/0.68891. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67807/0.68768. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67641/0.68719. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67770/0.68709. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67614/0.68579. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67455/0.68719. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67418/0.68689. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67311/0.68697. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67192/0.68596. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67358/0.68826. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67188/0.68656. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67033/0.68670. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66940/0.68565. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66870/0.68724. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66696/0.68771. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67009/0.68583. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66885/0.68523. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66796/0.68620. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66730/0.68415. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.66130/0.68423. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66445/0.68647. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66316/0.68399. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66389/0.68260. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66088/0.68336. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65944/0.68048. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65974/0.68392. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65683/0.68284. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65611/0.68302. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65498/0.67951. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65379/0.67883. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65476/0.67791. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65537/0.68160. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65163/0.67722. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65298/0.67403. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64970/0.67446. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64931/0.67676. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64737/0.67625. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64485/0.67613. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64737/0.67817. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64350/0.67382. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.64515/0.67253. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64169/0.67515. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64160/0.67258. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64059/0.67327. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.63836/0.66964. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.63761/0.67560. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63567/0.67284. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69469/0.69386. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69186/0.69128. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69100/0.69099. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69014/0.69041. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68996/0.69017. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68897/0.68986. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68856/0.68967. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68864/0.68963. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68769/0.68966. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68826/0.68961. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68721/0.68961. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68699/0.68972. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68730/0.68986. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68759/0.68993. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68735/0.69011. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68613/0.69003. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68566/0.69019. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68579/0.69026. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68619/0.69039. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68592/0.69053. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68541/0.69053. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68384/0.69044. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68385/0.69059. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68529/0.69073. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68464/0.69026. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68400/0.69024. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68270/0.69049. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68356/0.69047. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68222/0.69022. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68320/0.68999. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68247/0.69046. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68249/0.69013. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68156/0.69007. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68206/0.69048. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68158/0.68983. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68126/0.68975. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67944/0.69056. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67957/0.68954. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68053/0.68964. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68017/0.68950. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 0.67880/0.68942. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67968/0.68899. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67884/0.68892. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67902/0.68869. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67727/0.68876. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67653/0.68746. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67622/0.68765. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67643/0.68705. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67663/0.68670. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.67442/0.68689. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67450/0.68762. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67437/0.68837. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67385/0.68717. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67411/0.68569. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.67356/0.68629. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67160/0.68547. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.67062/0.68529. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67166/0.68397. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.67089/0.68369. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66877/0.68274. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66844/0.68346. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66760/0.68275. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66679/0.68347. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66719/0.68247. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66781/0.68405. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66236/0.68261. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66525/0.68248. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66255/0.68189. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66129/0.68248. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66115/0.68275. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66158/0.68336. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.65755/0.68192. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66184/0.68304. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65633/0.68109. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65827/0.68165. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65582/0.68007. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65672/0.68231. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65385/0.68259. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65544/0.68199. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65474/0.68174. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65585/0.68100. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65051/0.68277. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64940/0.68143. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65186/0.68227. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65126/0.68091. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64841/0.68352. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.64434/0.68254. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.64864/0.68364. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.64520/0.68017. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64318/0.68430. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64399/0.68019. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.64620/0.68475. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63908/0.68528. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64094/0.68296. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64047/0.68199. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.63844/0.68565. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.63874/0.68445. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.63646/0.68565. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.63903/0.68349. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63374/0.68489. Took 0.18 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69064/0.70094. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68892/0.70317. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68794/0.70466. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68721/0.70628. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68688/0.70736. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68677/0.70873. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68620/0.70986. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68592/0.71068. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68520/0.71138. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68476/0.71229. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68509/0.71290. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68570/0.71304. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68323/0.71362. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68427/0.71389. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68433/0.71424. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68309/0.71435. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68321/0.71484. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68359/0.71492. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68235/0.71508. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68333/0.71521. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68328/0.71537. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68226/0.71522. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68319/0.71527. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68197/0.71551. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68190/0.71561. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68132/0.71566. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68135/0.71600. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68134/0.71627. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68121/0.71698. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68015/0.71700. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68108/0.71714. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67950/0.71760. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67910/0.71796. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67842/0.71863. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.67886/0.71892. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67681/0.71946. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.67666/0.71953. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67516/0.72019. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67729/0.72061. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67247/0.72176. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.67466/0.72210. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67268/0.72343. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67176/0.72429. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67164/0.72585. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67092/0.72633. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67308/0.72684. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67164/0.72737. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67061/0.72871. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66876/0.72930. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66687/0.73078. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66642/0.73173. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66857/0.73267. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66770/0.73314. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66566/0.73344. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66635/0.73273. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66662/0.73385. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66482/0.73593. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66602/0.73594. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66420/0.73741. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66282/0.73678. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66190/0.73763. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66474/0.73720. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.65900/0.73673. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66022/0.73866. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66176/0.73749. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.65982/0.73862. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66045/0.74007. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65975/0.74030. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.66026/0.74160. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.65989/0.74221. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65750/0.74060. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65670/0.74190. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65830/0.74418. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.65643/0.74372. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65658/0.74451. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65442/0.74406. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65850/0.74437. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65391/0.74389. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65185/0.74638. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.65239/0.74675. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.65224/0.74521. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 0.65258/0.74391. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65134/0.74487. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64902/0.74756. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64997/0.74709. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65136/0.74964. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65158/0.74683. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65079/0.74909. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.65032/0.74962. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.65224/0.75027. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.64937/0.75073. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64698/0.75056. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64733/0.75020. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64722/0.75237. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64529/0.75426. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64664/0.75138. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64526/0.75040. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.64474/0.75427. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64690/0.75320. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64193/0.75282. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69168/0.69188. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68920/0.69188. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68898/0.69158. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68847/0.69107. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68694/0.69098. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68720/0.69053. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68667/0.69017. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68647/0.68998. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68548/0.68980. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68560/0.68983. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68561/0.68965. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68519/0.68961. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68455/0.68955. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68452/0.68951. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68443/0.68938. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68392/0.68941. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68384/0.68960. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68358/0.68959. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68358/0.68991. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68317/0.68947. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68338/0.68960. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68261/0.68993. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68129/0.69017. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68288/0.68994. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68176/0.68999. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68092/0.69009. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68149/0.68984. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68082/0.69002. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68072/0.69021. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68079/0.68992. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68015/0.68988. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68099/0.68975. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67972/0.69030. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67922/0.69066. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67946/0.69079. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67930/0.69112. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67868/0.69131. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67787/0.69113. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67864/0.69142. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67825/0.69192. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67754/0.69260. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67679/0.69270. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67657/0.69258. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67669/0.69325. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67604/0.69353. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67646/0.69346. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67666/0.69368. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67348/0.69426. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67549/0.69549. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67340/0.69532. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67365/0.69488. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.67427/0.69559. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67309/0.69570. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.67274/0.69689. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66979/0.69721. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67031/0.69862. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.66926/0.69930. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66945/0.69941. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67052/0.69952. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66892/0.70027. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66828/0.70076. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66710/0.70135. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.66820/0.70272. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66730/0.70197. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66709/0.70127. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66623/0.70229. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66434/0.70349. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66587/0.70289. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66436/0.70431. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66461/0.70413. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66373/0.70375. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66383/0.70380. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66163/0.70416. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.66478/0.70531. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66264/0.70359. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66059/0.70436. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66275/0.70457. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.66151/0.70462. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66108/0.70376. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65998/0.70527. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65713/0.70687. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65621/0.70612. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65622/0.70606. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65768/0.70548. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65612/0.70425. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65682/0.70556. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65652/0.70641. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65440/0.70477. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65323/0.70675. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65437/0.70746. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65026/0.70783. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65215/0.71049. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65069/0.70958. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.65254/0.70901. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64986/0.70859. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64687/0.70815. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64761/0.71045. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64916/0.70877. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64829/0.71105. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65040/0.71099. Took 0.18 sec\n",
      "ACC: 0.4583333333333333\n",
      "Epoch 0, Loss(train/val) 0.69259/0.69594. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69115/0.69814. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68986/0.70044. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68851/0.70265. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68859/0.70430. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68749/0.70514. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68759/0.70576. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68769/0.70617. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68612/0.70700. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68517/0.70747. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68520/0.70769. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68405/0.70864. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68371/0.70918. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68360/0.70977. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68209/0.71032. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68227/0.71073. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68100/0.71165. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.67966/0.71189. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67912/0.71290. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67716/0.71349. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67713/0.71399. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.67556/0.71465. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67585/0.71580. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67567/0.71722. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67376/0.71836. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67345/0.71799. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67398/0.71776. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67231/0.71828. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67109/0.71991. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.66844/0.72035. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67054/0.72049. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.66883/0.72091. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.66845/0.72219. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.66691/0.72241. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.66580/0.72302. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.66646/0.72271. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.66249/0.72386. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66416/0.72386. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.66334/0.72395. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.66242/0.72435. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66033/0.72395. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.66075/0.72512. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.66013/0.72656. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.65949/0.72577. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.65848/0.72547. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.65702/0.72563. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.65549/0.72585. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.65482/0.72497. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.65344/0.72504. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.65511/0.72472. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.65270/0.72513. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.65412/0.72381. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65039/0.72443. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.64878/0.72499. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.64857/0.72421. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.64576/0.72460. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.64821/0.72463. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.64598/0.72542. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.64201/0.72501. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 0.64359/0.72505. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.64152/0.72645. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64147/0.72862. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.64193/0.72903. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.63715/0.73020. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.63736/0.73061. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.63663/0.73174. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.63477/0.73058. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.63571/0.73174. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63287/0.73368. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63200/0.73300. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.62751/0.73657. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.62788/0.73712. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.62446/0.73824. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.62275/0.73754. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.62298/0.74168. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.62454/0.74187. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.62177/0.74189. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.62177/0.74431. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.62137/0.74718. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.61658/0.74851. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.61889/0.74861. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.61607/0.75132. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.60976/0.75327. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.61053/0.75166. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.60787/0.75451. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.61043/0.75710. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.60801/0.75794. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.60902/0.76161. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.60031/0.76536. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.60712/0.76079. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.60426/0.76085. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.60075/0.76712. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.59912/0.77042. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.59657/0.76793. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.59811/0.76760. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.58997/0.77149. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.59431/0.77501. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.59421/0.77856. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.59345/0.78331. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 0.59550/0.77868. Took 0.19 sec\n",
      "ACC: 0.5104166666666666\n",
      "Epoch 0, Loss(train/val) 0.69792/0.69992. Took 0.36 sec\n",
      "Epoch 1, Loss(train/val) 0.69376/0.69761. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69406/0.69642. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69293/0.69521. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69314/0.69455. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69313/0.69385. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69124/0.69335. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69231/0.69308. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69140/0.69287. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69088/0.69268. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.69104/0.69266. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69090/0.69248. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.69114/0.69256. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69036/0.69267. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69066/0.69279. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68996/0.69300. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68910/0.69332. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68932/0.69399. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68896/0.69445. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68926/0.69470. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68859/0.69531. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68805/0.69590. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68786/0.69625. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68776/0.69687. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68765/0.69773. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68717/0.69844. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68678/0.69904. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68637/0.69954. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68633/0.70015. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68631/0.70072. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68496/0.70121. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 0.68491/0.70231. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68353/0.70295. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68296/0.70398. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68358/0.70496. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68289/0.70612. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68375/0.70659. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68256/0.70713. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68210/0.70793. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68189/0.70945. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68012/0.71077. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68079/0.71142. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68115/0.71205. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67933/0.71286. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67853/0.71368. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67812/0.71402. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67931/0.71509. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 0.67633/0.71616. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67651/0.71698. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67535/0.71860. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67451/0.71782. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67437/0.71782. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67348/0.71933. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67411/0.72084. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67247/0.72066. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67160/0.72149. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67070/0.72351. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66985/0.72347. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 0.66834/0.72612. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66722/0.72689. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66639/0.72697. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.66557/0.72924. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66664/0.73050. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66598/0.72967. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.66505/0.73013. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 0.66542/0.73084. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66396/0.73500. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.66219/0.73626. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 0.66210/0.73568. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66361/0.73614. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.66009/0.73807. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65928/0.73716. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65928/0.73877. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 0.65822/0.74213. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65625/0.74256. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65877/0.74177. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.65880/0.74464. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.65550/0.74637. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65651/0.74688. Took 0.22 sec\n",
      "Epoch 79, Loss(train/val) 0.65572/0.74525. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65575/0.74550. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.65180/0.74506. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.65418/0.74945. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65031/0.74908. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65219/0.75184. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.64994/0.75273. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64848/0.75231. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.64849/0.75172. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64703/0.75332. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64809/0.75422. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.64634/0.75750. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64410/0.75849. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.64204/0.75798. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64181/0.75940. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63843/0.76549. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.64134/0.76810. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64045/0.76515. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64197/0.76421. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64514/0.76432. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.63821/0.76471. Took 0.18 sec\n",
      "ACC: 0.4791666666666667\n",
      "Epoch 0, Loss(train/val) 0.69441/0.68663. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69228/0.68585. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69301/0.68621. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69147/0.68660. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69091/0.68696. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69102/0.68728. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68996/0.68751. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69015/0.68769. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68953/0.68813. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68958/0.68850. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68822/0.68897. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68847/0.68914. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68815/0.68946. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68837/0.69027. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68820/0.69065. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68596/0.69104. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68725/0.69124. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68659/0.69156. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68612/0.69236. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68608/0.69252. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68491/0.69267. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68516/0.69317. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68350/0.69366. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68369/0.69406. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68231/0.69441. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68281/0.69515. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68198/0.69564. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68114/0.69664. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68076/0.69636. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67923/0.69742. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67933/0.69760. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67921/0.69897. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67661/0.69963. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67875/0.70041. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67822/0.70158. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.67641/0.70321. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67499/0.70436. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67361/0.70486. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67430/0.70658. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67459/0.70726. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67251/0.70802. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67245/0.70769. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67264/0.70944. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67092/0.71133. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67260/0.71158. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66981/0.71282. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66747/0.71392. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67022/0.71338. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66956/0.71482. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66803/0.71479. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66516/0.71588. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66730/0.71821. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66667/0.71935. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66428/0.71936. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66214/0.71973. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66326/0.72175. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66286/0.72282. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66114/0.72324. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66361/0.72428. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65943/0.72513. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65747/0.72389. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65669/0.72586. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65830/0.72513. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65714/0.72856. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65559/0.72909. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65537/0.73159. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65504/0.73244. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.65390/0.73130. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65344/0.73377. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65173/0.73258. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.65149/0.73332. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65255/0.73427. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.65151/0.73636. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64921/0.73783. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.65041/0.73795. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64807/0.73946. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64893/0.74030. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64394/0.74271. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64607/0.74432. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64245/0.74428. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64104/0.74547. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64395/0.74873. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.64081/0.74657. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63834/0.74661. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.64021/0.75052. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63978/0.75355. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63571/0.75450. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63940/0.75536. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.63387/0.75403. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.63242/0.75250. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.63394/0.75641. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.63509/0.75768. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.63218/0.75940. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63421/0.75819. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.63015/0.76270. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63595/0.76601. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62755/0.76126. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.62401/0.76277. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.62347/0.76390. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.62629/0.76929. Took 0.18 sec\n",
      "ACC: 0.59375\n",
      "Epoch 0, Loss(train/val) 0.69258/0.68902. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 0.69145/0.68589. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69100/0.68596. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69054/0.68635. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69066/0.68710. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68878/0.68768. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68908/0.68834. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68925/0.68906. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68882/0.68915. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68824/0.68958. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68759/0.69028. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68778/0.69087. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68767/0.69126. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68653/0.69149. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68718/0.69190. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68587/0.69158. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68538/0.69187. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68483/0.69236. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68515/0.69290. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68469/0.69299. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68367/0.69276. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68440/0.69327. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68220/0.69357. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68185/0.69352. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68210/0.69391. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68087/0.69363. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68036/0.69322. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67908/0.69284. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67871/0.69390. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67768/0.69373. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67614/0.69404. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67619/0.69405. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67534/0.69483. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67412/0.69503. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67276/0.69595. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67269/0.69643. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67084/0.69754. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.66942/0.69859. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67067/0.69952. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.66830/0.69905. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66896/0.69985. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.66795/0.70055. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66745/0.70194. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66570/0.70271. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66472/0.70343. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66268/0.70417. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.66087/0.70640. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66218/0.70707. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66170/0.70889. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.65934/0.71001. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66006/0.71130. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.65893/0.71172. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.65929/0.71293. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.65788/0.71484. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65590/0.71527. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65408/0.71747. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65433/0.71806. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.65318/0.71967. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65278/0.71878. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65107/0.72143. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65249/0.72172. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64855/0.72518. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64651/0.72532. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.64748/0.72910. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64610/0.72622. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.64823/0.72953. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64612/0.72898. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64362/0.73020. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64363/0.73221. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64406/0.73339. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63915/0.73473. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64201/0.73471. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63744/0.73789. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.64167/0.73897. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.63871/0.73819. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.63803/0.73900. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63823/0.73989. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63749/0.74137. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63527/0.74019. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63266/0.74088. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.63589/0.74241. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63034/0.74185. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 0.62925/0.74425. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.62728/0.74437. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.62913/0.74289. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63151/0.74577. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62524/0.74571. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62490/0.74836. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62304/0.75050. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62058/0.74935. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62307/0.75020. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.61831/0.75022. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.61848/0.74955. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.61743/0.75262. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.61669/0.74942. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.61470/0.75266. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61466/0.74847. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61136/0.74948. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61077/0.75027. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.60484/0.75238. Took 0.18 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69482/0.69549. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69117/0.69906. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69052/0.69980. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68960/0.70045. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68865/0.70177. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68808/0.70265. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68697/0.70322. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68672/0.70391. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68622/0.70452. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68634/0.70494. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68493/0.70571. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68506/0.70656. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68328/0.70705. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68373/0.70765. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68301/0.70827. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68244/0.70903. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68126/0.71068. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68181/0.71102. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68076/0.71164. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68063/0.71256. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68029/0.71429. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67955/0.71567. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.67980/0.71665. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.67892/0.71668. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67745/0.71802. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.67723/0.71937. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67734/0.72034. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67666/0.72200. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67734/0.72267. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67678/0.72316. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67726/0.72348. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67533/0.72435. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67415/0.72472. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67583/0.72520. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67432/0.72494. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67222/0.72769. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67322/0.72817. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67348/0.72762. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67148/0.72822. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67253/0.72912. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67156/0.73042. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67245/0.73046. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67042/0.73068. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67162/0.73174. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 0.67193/0.73210. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 0.66994/0.73282. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67069/0.73254. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66964/0.73353. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67009/0.73345. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.66940/0.73407. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66643/0.73465. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66798/0.73545. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66843/0.73651. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66734/0.73690. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66741/0.73829. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.66709/0.73706. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66605/0.73895. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66467/0.73861. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66464/0.73797. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66324/0.73772. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66463/0.73743. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66254/0.73843. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66169/0.73883. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.66181/0.74184. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66183/0.74382. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66106/0.74216. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66053/0.74278. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65982/0.74271. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.65747/0.74453. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65875/0.74352. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 0.65849/0.74380. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.65723/0.74703. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.65603/0.74566. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.65846/0.74790. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 0.65193/0.74741. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.65242/0.75101. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.65419/0.74839. Took 0.22 sec\n",
      "Epoch 77, Loss(train/val) 0.65042/0.75098. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.65005/0.75224. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 0.64801/0.75173. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.64798/0.75106. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.64710/0.75500. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.64895/0.75530. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.64479/0.75646. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.64445/0.75748. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 0.64199/0.75824. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.64442/0.75905. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 0.63922/0.75995. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 0.64054/0.76009. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 0.63852/0.76297. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.63916/0.76317. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.63695/0.76576. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.63764/0.76824. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.63633/0.76930. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.63297/0.76770. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.63214/0.76974. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.62806/0.77288. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 0.63306/0.77123. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.63116/0.77192. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.62675/0.77266. Took 0.19 sec\n",
      "ACC: 0.5208333333333334\n",
      "Epoch 0, Loss(train/val) 0.69493/0.69204. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.68941/0.69591. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68889/0.69756. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68796/0.69822. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68747/0.69853. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.68782/0.69853. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68573/0.69875. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68549/0.69903. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68479/0.69923. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.68466/0.69964. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68401/0.69938. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68397/0.69954. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68250/0.69989. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68242/0.70009. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68125/0.70016. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68014/0.70049. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.67885/0.70046. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.67888/0.70018. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.67796/0.70102. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.67832/0.70114. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.67626/0.70003. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.67545/0.70005. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67293/0.69938. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67222/0.69935. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67123/0.69891. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.66868/0.69684. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.66846/0.69726. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.66648/0.69614. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.66166/0.69474. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.66100/0.69626. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.65854/0.69485. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.65740/0.69577. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.65768/0.69580. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.65054/0.69810. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.65113/0.69796. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.64984/0.69968. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.65063/0.69927. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.65005/0.70073. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.64623/0.70049. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.64538/0.70232. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.64272/0.70249. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.63983/0.70456. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.64115/0.70587. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.64173/0.70577. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.63865/0.70518. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.63969/0.70492. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.63566/0.70716. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.63242/0.70734. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.63197/0.70748. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 0.63469/0.70837. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.62895/0.71026. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.63131/0.71165. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.62684/0.71242. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.62760/0.71389. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.62552/0.71310. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.62201/0.71553. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.62541/0.71735. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.62273/0.71734. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.61887/0.71759. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.62052/0.71658. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.61721/0.71577. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.62209/0.71611. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.61689/0.71679. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.61497/0.71881. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.61186/0.71692. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.61480/0.71787. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.61413/0.71920. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.61117/0.72080. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.61247/0.72165. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.60842/0.72179. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.60649/0.72508. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.60539/0.72263. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.60585/0.72343. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.60481/0.72276. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.60197/0.72483. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.60558/0.72630. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.59813/0.72689. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.60143/0.73330. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.60167/0.72953. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.59699/0.73175. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.60101/0.73235. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.59323/0.73206. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.59654/0.73076. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.59771/0.73329. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.59597/0.73086. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.59625/0.73479. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.59211/0.73250. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.59112/0.72908. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.58544/0.72921. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.58700/0.72624. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.58651/0.72972. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.58487/0.72821. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.58402/0.72825. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.58406/0.73288. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.58166/0.73147. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.58098/0.73195. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.58052/0.73062. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.57600/0.73013. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.57527/0.73233. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.57861/0.72648. Took 0.18 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69354/0.68581. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69059/0.68352. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.68944/0.68250. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.68912/0.68194. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 0.68892/0.68121. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68873/0.68063. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68796/0.67993. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.68753/0.67921. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.68708/0.67876. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68688/0.67811. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68580/0.67738. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68567/0.67659. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68562/0.67632. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68359/0.67562. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68404/0.67560. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68291/0.67547. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68220/0.67569. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68270/0.67586. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68274/0.67594. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68154/0.67614. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68133/0.67644. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68005/0.67648. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.67903/0.67716. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.67891/0.67756. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.67891/0.67787. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.67671/0.67805. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.67779/0.67841. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67737/0.67904. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.67576/0.67888. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.67642/0.67888. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67638/0.67930. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67628/0.67973. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67388/0.68076. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67387/0.68070. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67411/0.68088. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67398/0.68156. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67290/0.68203. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67183/0.68252. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67107/0.68357. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67051/0.68416. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.66963/0.68486. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.66928/0.68556. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.66805/0.68576. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.66688/0.68663. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.66635/0.68702. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66579/0.68735. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66546/0.68879. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66533/0.68988. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66201/0.69056. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 0.66254/0.69100. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.66017/0.69136. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66002/0.69215. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66002/0.69241. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.65890/0.69416. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.65672/0.69404. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.65767/0.69447. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65736/0.69539. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65394/0.69747. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65281/0.69791. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65508/0.69961. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65085/0.69950. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.64749/0.70210. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.64604/0.70183. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.64545/0.70243. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.64724/0.70757. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.64261/0.70770. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64179/0.70553. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.63816/0.71007. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.63850/0.71159. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.63457/0.71424. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.63319/0.71759. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.63057/0.71705. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.63044/0.72066. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.62840/0.72203. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.62763/0.72385. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.62596/0.72204. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.62513/0.72601. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.62461/0.72721. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.62464/0.73036. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.62241/0.73373. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.61757/0.73620. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.61594/0.73375. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.61746/0.73582. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.61271/0.74137. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.60966/0.74781. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.61275/0.74938. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.61011/0.74785. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.60646/0.74934. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.60580/0.75288. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.60543/0.75298. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.60400/0.75506. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.60311/0.75231. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.59903/0.76183. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.60047/0.75994. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.59565/0.75943. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.60188/0.75745. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.59525/0.76202. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.59149/0.76737. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.59171/0.76602. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.59027/0.76791. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69144/0.69478. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68997/0.69667. Took 0.18 sec\n",
      "Epoch 2, Loss(train/val) 0.68871/0.69724. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68865/0.69713. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68825/0.69716. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68816/0.69686. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68716/0.69644. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68646/0.69666. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68715/0.69647. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68774/0.69659. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68666/0.69597. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68577/0.69616. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68569/0.69579. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68615/0.69571. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68530/0.69618. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68503/0.69656. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68503/0.69640. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68435/0.69669. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68424/0.69737. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68311/0.69723. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68385/0.69762. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68268/0.69722. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68416/0.69710. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68335/0.69734. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68340/0.69730. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68226/0.69726. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68212/0.69803. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68310/0.69824. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68166/0.69789. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68140/0.69797. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68031/0.69803. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68024/0.69909. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67765/0.69874. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67847/0.69943. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67936/0.69912. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67926/0.69901. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67714/0.69955. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67748/0.69936. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67638/0.69864. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67515/0.69893. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67653/0.69942. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67387/0.69877. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67242/0.69883. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67261/0.69897. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67213/0.69889. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67269/0.69936. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67100/0.69862. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67141/0.69954. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66778/0.69809. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66830/0.69732. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67079/0.69852. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 0.66859/0.69917. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66864/0.69974. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.66675/0.69967. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66667/0.69977. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66507/0.69998. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 0.66622/0.69951. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.66809/0.69883. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 0.66468/0.69925. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.66745/0.69833. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 0.66320/0.69941. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 0.66275/0.69932. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66479/0.69893. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 0.66716/0.69944. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.66050/0.69964. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 0.66240/0.69999. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.66118/0.70023. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 0.66207/0.69975. Took 0.22 sec\n",
      "Epoch 68, Loss(train/val) 0.66029/0.69955. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65987/0.70067. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.65902/0.70039. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 0.65868/0.70038. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 0.65892/0.70063. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 0.66014/0.69912. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.65791/0.69880. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 0.65789/0.69790. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 0.66220/0.69944. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 0.65630/0.69878. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65533/0.70102. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65438/0.70038. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.65213/0.70047. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 0.65331/0.70040. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 0.65253/0.69934. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65016/0.69817. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.65099/0.70038. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65298/0.69908. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.65293/0.70198. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.64846/0.70098. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.64861/0.70138. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.64964/0.70115. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 0.64401/0.69871. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.64930/0.69846. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.64612/0.69966. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64595/0.70025. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.64300/0.70001. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64300/0.70011. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 0.64352/0.70117. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64780/0.70197. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.64286/0.70067. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64379/0.69922. Took 0.19 sec\n",
      "ACC: 0.4895833333333333\n",
      "Epoch 0, Loss(train/val) 0.69252/0.69637. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69061/0.69807. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69007/0.69925. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68944/0.70013. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68863/0.70103. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68812/0.70170. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68834/0.70270. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68788/0.70375. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.68766/0.70424. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68753/0.70517. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68663/0.70625. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68613/0.70745. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68591/0.70848. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68475/0.70975. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68428/0.71108. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68468/0.71207. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68319/0.71308. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68269/0.71430. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68301/0.71540. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68276/0.71660. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68255/0.71709. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68061/0.71806. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68120/0.71890. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68091/0.71844. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.67949/0.72019. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68042/0.72031. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.67807/0.72113. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.67834/0.72182. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67783/0.72231. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.67782/0.72349. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67650/0.72394. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.67626/0.72406. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67530/0.72434. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.67425/0.72590. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67450/0.72717. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67351/0.72681. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67378/0.72700. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67313/0.72741. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67175/0.72834. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67311/0.72899. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67065/0.73018. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67072/0.73170. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67058/0.73172. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.66778/0.73292. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.66881/0.73449. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.66819/0.73461. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66474/0.73659. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.66568/0.73854. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.66428/0.73976. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66332/0.74142. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66380/0.74202. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66325/0.74279. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 0.66365/0.74392. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66018/0.74524. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66200/0.74519. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65970/0.74704. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.65836/0.74763. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65946/0.74928. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65804/0.75134. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65714/0.75314. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65591/0.75547. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65762/0.75688. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65033/0.75885. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65411/0.75981. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65189/0.76111. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.65284/0.76259. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.64991/0.76416. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65173/0.76446. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.64960/0.76591. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64714/0.76748. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64782/0.76789. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64566/0.76868. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64474/0.77152. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64354/0.77326. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64408/0.77299. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64531/0.77410. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.63994/0.77685. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.64055/0.77733. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63975/0.77869. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.64066/0.78127. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63942/0.78276. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63596/0.78461. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63380/0.78504. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63611/0.78459. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63385/0.78683. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63329/0.78704. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63242/0.78913. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62744/0.79111. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62699/0.79338. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62646/0.79314. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62867/0.79693. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.62845/0.79697. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62512/0.79815. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62369/0.79768. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62247/0.80015. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62049/0.80043. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.62088/0.80119. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61674/0.80467. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.61587/0.80413. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61463/0.80441. Took 0.19 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69606/0.71092. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69174/0.70834. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69038/0.70922. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.68907/0.70920. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68824/0.70992. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68754/0.71014. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68698/0.71046. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68615/0.71100. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68608/0.71126. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68584/0.71128. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68503/0.71164. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68516/0.71187. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68513/0.71195. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68440/0.71168. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68414/0.71163. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68386/0.71149. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68334/0.71142. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68398/0.71112. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68333/0.71023. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68315/0.71048. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68216/0.71019. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68181/0.71012. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68219/0.70980. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68117/0.70963. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68111/0.70934. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68040/0.70847. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68066/0.70914. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68100/0.70913. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.67996/0.70895. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68040/0.70804. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.67984/0.70731. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.67853/0.70714. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.67904/0.70695. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67899/0.70734. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67830/0.70685. Took 0.23 sec\n",
      "Epoch 35, Loss(train/val) 0.67792/0.70724. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67667/0.70613. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.67685/0.70700. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67694/0.70671. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67661/0.70684. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67507/0.70656. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67532/0.70629. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67493/0.70667. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.67543/0.70619. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.67336/0.70725. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67400/0.70581. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67304/0.70622. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.67343/0.70662. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.67362/0.70740. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67369/0.70626. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67365/0.70596. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67177/0.70669. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67193/0.70696. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67201/0.70692. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66949/0.70781. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67058/0.70684. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67147/0.70691. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66775/0.70800. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.66916/0.70796. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66780/0.70829. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.66836/0.70749. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66760/0.70880. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.66661/0.70816. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66737/0.70859. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66609/0.71029. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.66508/0.71067. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66520/0.71111. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66305/0.71118. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66200/0.71050. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.66327/0.71124. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66270/0.71176. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66148/0.71272. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66280/0.71429. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.65943/0.71454. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66033/0.71445. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.65862/0.71207. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.65726/0.71468. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66009/0.71562. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.65623/0.71590. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.65827/0.71583. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65697/0.71618. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.65779/0.71495. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65771/0.71542. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.65409/0.71716. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65605/0.71674. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.65320/0.71923. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65258/0.72227. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65262/0.72126. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65092/0.72234. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65125/0.72331. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65219/0.72042. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65142/0.72045. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65146/0.72459. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64741/0.72111. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.64723/0.72254. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64841/0.72381. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.64951/0.72335. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.64793/0.72181. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64525/0.72406. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64610/0.72370. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69353/0.69787. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.68969/0.69827. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.68934/0.69884. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68869/0.69929. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.68765/0.69987. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68849/0.70034. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.68774/0.70081. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68725/0.70106. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68762/0.70134. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68700/0.70183. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68725/0.70218. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68601/0.70249. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68589/0.70275. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68565/0.70306. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68563/0.70342. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68589/0.70360. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68615/0.70382. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68589/0.70416. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68549/0.70438. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68583/0.70459. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68521/0.70481. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68476/0.70531. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68377/0.70561. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68376/0.70611. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68349/0.70615. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68482/0.70636. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68363/0.70684. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68418/0.70708. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68357/0.70750. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68281/0.70791. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68358/0.70818. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68269/0.70856. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68210/0.70928. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68250/0.70979. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68246/0.71029. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68132/0.71102. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68199/0.71139. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68205/0.71197. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68097/0.71256. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68050/0.71308. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68200/0.71323. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.68169/0.71344. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68054/0.71374. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68022/0.71427. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 0.68042/0.71505. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68072/0.71554. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 0.67935/0.71595. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67872/0.71604. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67989/0.71622. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67871/0.71686. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67859/0.71772. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67794/0.71804. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67832/0.71908. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67783/0.71927. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67683/0.71997. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67661/0.72034. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.67709/0.72090. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67763/0.72096. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67646/0.72163. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67698/0.72161. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67551/0.72224. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67626/0.72334. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67575/0.72354. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67559/0.72517. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67453/0.72610. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.67538/0.72621. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67339/0.72685. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67458/0.72775. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67301/0.72797. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67233/0.72889. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67137/0.72984. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 0.67360/0.73072. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 0.67039/0.73235. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67090/0.73240. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67196/0.73290. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67126/0.73413. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 0.66952/0.73489. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66885/0.73593. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.66853/0.73695. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66792/0.73715. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66618/0.73854. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.66716/0.74020. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66844/0.74113. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66517/0.74189. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66474/0.74298. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66574/0.74413. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66551/0.74444. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66404/0.74556. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66392/0.74642. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66337/0.74753. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66447/0.74850. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66016/0.75027. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 0.66178/0.75075. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66174/0.75194. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65920/0.75314. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65936/0.75442. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.65874/0.75542. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65772/0.75700. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.65821/0.75778. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.65792/0.75898. Took 0.19 sec\n",
      "ACC: 0.5416666666666666\n",
      "Epoch 0, Loss(train/val) 0.69668/0.69775. Took 0.37 sec\n",
      "Epoch 1, Loss(train/val) 0.69365/0.69516. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 0.69226/0.69353. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69148/0.69226. Took 0.22 sec\n",
      "Epoch 4, Loss(train/val) 0.69022/0.69125. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68984/0.69062. Took 0.22 sec\n",
      "Epoch 6, Loss(train/val) 0.68981/0.68986. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68909/0.68923. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 0.68934/0.68868. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.68850/0.68841. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 0.68824/0.68831. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68776/0.68809. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68763/0.68780. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68768/0.68779. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68694/0.68757. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68733/0.68746. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68662/0.68745. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68712/0.68746. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68682/0.68757. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68525/0.68750. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68643/0.68765. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68587/0.68758. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68482/0.68765. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68455/0.68759. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68393/0.68792. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68467/0.68796. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68470/0.68812. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68474/0.68805. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68441/0.68814. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68339/0.68830. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68408/0.68859. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68414/0.68888. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68257/0.68945. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68274/0.68949. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68241/0.69002. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68140/0.69024. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68191/0.69039. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68125/0.69062. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67960/0.69089. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.67939/0.69144. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67906/0.69223. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67934/0.69274. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68065/0.69337. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67878/0.69414. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67815/0.69471. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67798/0.69541. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67677/0.69604. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67619/0.69656. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67804/0.69781. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67560/0.69805. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67595/0.69933. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.67367/0.70029. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67424/0.70049. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67521/0.70155. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67481/0.70231. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67486/0.70232. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67236/0.70300. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67168/0.70421. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67153/0.70534. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67256/0.70552. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67081/0.70674. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67082/0.70780. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67127/0.70879. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.66727/0.71002. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.66837/0.71122. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.66885/0.71208. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.66512/0.71326. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.66817/0.71411. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.66770/0.71398. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.66872/0.71541. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.66589/0.71555. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.66503/0.71632. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66498/0.71722. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.66332/0.71790. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66093/0.71933. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66516/0.71969. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66313/0.72015. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66103/0.72123. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.65930/0.72289. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66203/0.72316. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.65986/0.72362. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66040/0.72413. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.65880/0.72483. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.65863/0.72657. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.65531/0.72851. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.65569/0.72955. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65546/0.72984. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.65414/0.73193. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65567/0.73150. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.65404/0.73076. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65562/0.73251. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.65241/0.73305. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65233/0.73386. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.64757/0.73390. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.65221/0.73509. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.64871/0.73657. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65036/0.73849. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65060/0.73824. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.64947/0.73760. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.64400/0.73897. Took 0.18 sec\n",
      "ACC: 0.53125\n",
      "Epoch 0, Loss(train/val) 0.69568/0.70285. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69447/0.70056. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69435/0.69887. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69345/0.69762. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69264/0.69681. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69349/0.69559. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69234/0.69464. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69244/0.69430. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69122/0.69386. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69093/0.69341. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69097/0.69301. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69152/0.69254. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 0.69000/0.69216. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.69121/0.69154. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.69016/0.69126. Took 0.22 sec\n",
      "Epoch 15, Loss(train/val) 0.69037/0.69093. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68980/0.69061. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68880/0.69020. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 0.68891/0.68974. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68920/0.68946. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68882/0.68908. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68947/0.68898. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68785/0.68865. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68702/0.68799. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68772/0.68777. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68669/0.68780. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68512/0.68782. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68435/0.68738. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68528/0.68700. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68409/0.68709. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68371/0.68677. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68260/0.68689. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68401/0.68630. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68124/0.68552. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68365/0.68678. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68208/0.68662. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67947/0.68641. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68055/0.68673. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.67988/0.68634. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68093/0.68658. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67594/0.68669. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.67787/0.68696. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67574/0.68675. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67673/0.68653. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67845/0.68647. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67476/0.68712. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67409/0.68804. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67354/0.68935. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67312/0.69020. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66834/0.69035. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67146/0.69027. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 0.67018/0.69063. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66595/0.69172. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66835/0.69143. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66445/0.69124. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66563/0.69335. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.66481/0.69174. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.66201/0.69207. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 0.66188/0.69144. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.65994/0.69316. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.66131/0.69457. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.66201/0.69618. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65910/0.69405. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.65550/0.69548. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.65545/0.69491. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65434/0.69575. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65503/0.69437. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.65418/0.69732. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64750/0.69637. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.65107/0.69967. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64786/0.70132. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64594/0.69935. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64389/0.69780. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64511/0.70107. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 0.64541/0.70195. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 0.64334/0.70028. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64274/0.70344. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.63989/0.70231. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 0.63975/0.70384. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.63912/0.69842. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 0.63610/0.70034. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.63715/0.69898. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63190/0.70447. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.63339/0.70197. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.63873/0.70304. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63582/0.70137. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.63126/0.70024. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.62632/0.70430. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.62463/0.70473. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62341/0.70604. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62313/0.70326. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62481/0.70325. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.62117/0.70397. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.61420/0.70336. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.61914/0.70352. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.61684/0.70805. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61995/0.70652. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.61438/0.70339. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.61413/0.70845. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.61112/0.70436. Took 0.18 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69437/0.69398. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69317/0.69451. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69381/0.69489. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69304/0.69535. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69235/0.69587. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 0.69308/0.69636. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69257/0.69695. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69159/0.69771. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.69117/0.69848. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 0.69164/0.69918. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69134/0.69989. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.69068/0.70078. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.69046/0.70178. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.69000/0.70278. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.69009/0.70377. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68989/0.70459. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68936/0.70527. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.69001/0.70586. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68944/0.70658. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68838/0.70741. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68837/0.70789. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68897/0.70835. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68813/0.70899. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68650/0.70946. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68809/0.71031. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68622/0.71078. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68666/0.71119. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68556/0.71195. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68541/0.71264. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68532/0.71305. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68495/0.71347. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68422/0.71359. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68356/0.71407. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68392/0.71406. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 0.68109/0.71490. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68204/0.71495. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68241/0.71532. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68156/0.71576. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68003/0.71621. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67919/0.71729. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67931/0.71754. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67716/0.71769. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.67982/0.71781. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67813/0.71832. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67504/0.71861. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.67353/0.71909. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67408/0.71990. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67135/0.72007. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67233/0.72037. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66877/0.72161. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66874/0.72273. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66756/0.72343. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66619/0.72331. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66370/0.72386. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66362/0.72428. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.65864/0.72531. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65804/0.72684. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65822/0.72771. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65946/0.72706. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.65755/0.72779. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.65601/0.72851. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65587/0.72926. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65552/0.73142. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.64949/0.73135. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65255/0.73634. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.64726/0.73894. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 0.64958/0.74001. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 0.65050/0.73542. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.64738/0.73812. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64664/0.73805. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 0.64270/0.74013. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.64535/0.73901. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64556/0.74041. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64416/0.74255. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64110/0.74176. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.64426/0.73904. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64257/0.73933. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.64045/0.73853. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.63728/0.74261. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63558/0.74272. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.63738/0.74216. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63707/0.73889. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63489/0.74003. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63517/0.74098. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63296/0.73898. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63333/0.74422. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.63387/0.74235. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.63019/0.74002. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62904/0.73929. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.62886/0.74147. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62317/0.74132. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62968/0.74261. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 0.62620/0.74409. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.62782/0.74494. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.62793/0.74180. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.62430/0.74363. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.62589/0.74139. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61816/0.74578. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.62272/0.74537. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.61592/0.74486. Took 0.19 sec\n",
      "ACC: 0.5\n",
      "Epoch 0, Loss(train/val) 0.69417/0.69251. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 0.69368/0.69223. Took 0.22 sec\n",
      "Epoch 2, Loss(train/val) 0.69366/0.69210. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 0.69253/0.69206. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 0.69189/0.69213. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69254/0.69220. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 0.69178/0.69234. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69139/0.69251. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 0.69187/0.69262. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69127/0.69284. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69080/0.69302. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68962/0.69325. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68941/0.69330. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68803/0.69344. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68872/0.69386. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68882/0.69456. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68761/0.69480. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 0.68791/0.69537. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68675/0.69575. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68648/0.69645. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68650/0.69659. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 0.68649/0.69745. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68424/0.69826. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68416/0.69842. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68429/0.69907. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68462/0.69995. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68342/0.70077. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68209/0.70023. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68257/0.70131. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68163/0.70192. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.67999/0.70224. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68125/0.70282. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.67906/0.70314. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.67950/0.70456. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.67889/0.70550. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.67668/0.70631. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.67714/0.70779. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.67494/0.70943. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.67635/0.70941. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.67456/0.70884. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.67408/0.71058. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 0.67341/0.71130. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.67424/0.71101. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.67361/0.71158. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67129/0.71191. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.67108/0.71123. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.66796/0.71197. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.66779/0.71161. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.66751/0.71204. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.66616/0.71419. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.66713/0.71737. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.66354/0.71568. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.66302/0.71804. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.66512/0.71624. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.66255/0.71914. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.66399/0.71820. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 0.65926/0.71914. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.65806/0.72073. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.65847/0.71987. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.66035/0.72048. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 0.65557/0.72023. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.65522/0.72417. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.65644/0.72115. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.65505/0.72281. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.65506/0.72024. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 0.65446/0.72269. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.65390/0.72430. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.64698/0.72274. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.65280/0.72552. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.64938/0.72351. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.64938/0.72392. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.64608/0.72357. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.64688/0.72509. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.64418/0.72654. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.64355/0.72710. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.64324/0.72811. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.64343/0.72695. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 0.63809/0.72810. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.64307/0.72904. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.63981/0.72785. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.64012/0.72939. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.63451/0.73043. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.63997/0.72996. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.63178/0.73429. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.63652/0.73272. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.63458/0.73195. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.62921/0.73197. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.63294/0.73311. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.62982/0.73229. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.62946/0.73154. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.62430/0.73493. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.62260/0.73761. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.62621/0.73450. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.62782/0.73629. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.62290/0.73522. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.62122/0.73578. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.61877/0.73977. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.61497/0.73733. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 0.61767/0.74283. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.61493/0.74280. Took 0.18 sec\n",
      "ACC: 0.4166666666666667\n",
      "Epoch 0, Loss(train/val) 0.69432/0.69865. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69365/0.69711. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69249/0.69588. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69265/0.69533. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69255/0.69473. Took 0.23 sec\n",
      "Epoch 5, Loss(train/val) 0.69186/0.69416. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69175/0.69373. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 0.69115/0.69308. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69030/0.69251. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69048/0.69181. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69056/0.69099. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 0.68960/0.69064. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.68942/0.69054. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68971/0.68955. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68932/0.68894. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68867/0.68798. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68890/0.68761. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68884/0.68730. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68786/0.68661. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68757/0.68608. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68822/0.68602. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68687/0.68671. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68742/0.68662. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68756/0.68642. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68689/0.68550. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 0.68666/0.68534. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68661/0.68530. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68715/0.68508. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68523/0.68444. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 0.68667/0.68509. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68588/0.68460. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68610/0.68360. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 0.68527/0.68481. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68616/0.68391. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68494/0.68338. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68454/0.68316. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68508/0.68325. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68458/0.68403. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68403/0.68352. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68470/0.68377. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68434/0.68243. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.68271/0.68301. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68333/0.68334. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68342/0.68326. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68352/0.68329. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68339/0.68307. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68278/0.68247. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 0.68121/0.68151. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 0.68203/0.68163. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68170/0.68202. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 0.68164/0.68210. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.68139/0.68295. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68255/0.68239. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.68035/0.68330. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.68071/0.68160. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68052/0.68196. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67998/0.68243. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67967/0.68361. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67954/0.68191. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 0.67725/0.68321. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67971/0.68244. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67749/0.68153. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67816/0.68187. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67558/0.68330. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 0.67824/0.68260. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67524/0.68250. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67767/0.68377. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67639/0.68127. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 0.67505/0.68299. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67516/0.68158. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67320/0.68293. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 0.67342/0.68145. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67280/0.68026. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67092/0.68199. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67205/0.68122. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67146/0.68139. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67137/0.68203. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 0.67021/0.68408. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67062/0.68480. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67072/0.68207. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66794/0.68272. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66856/0.68263. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66791/0.68379. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66616/0.68435. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.66588/0.68236. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66530/0.68384. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66406/0.68415. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66218/0.68398. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66265/0.68292. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.66435/0.68128. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66227/0.68398. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66257/0.68600. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66134/0.68644. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65875/0.68579. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.65411/0.68640. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 0.65295/0.68673. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65681/0.68600. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65705/0.68759. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65284/0.68578. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65354/0.68468. Took 0.18 sec\n",
      "ACC: 0.5729166666666666\n",
      "Epoch 0, Loss(train/val) 0.69576/0.69256. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69361/0.69275. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69259/0.69319. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69170/0.69369. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69176/0.69422. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.69184/0.69473. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.69122/0.69528. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.69093/0.69585. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68988/0.69649. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68942/0.69717. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.69004/0.69787. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68954/0.69852. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68866/0.69917. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 0.68841/0.69978. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68850/0.70043. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68906/0.70083. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68804/0.70142. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68727/0.70187. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68754/0.70260. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68743/0.70310. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68728/0.70340. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68693/0.70383. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68721/0.70427. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68686/0.70443. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68726/0.70469. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68641/0.70490. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68588/0.70536. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68529/0.70563. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68640/0.70584. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68555/0.70575. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68482/0.70586. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68427/0.70600. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68565/0.70589. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 0.68571/0.70611. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68458/0.70616. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68511/0.70595. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68418/0.70592. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68427/0.70616. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68349/0.70661. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68351/0.70684. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68301/0.70711. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68388/0.70696. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68409/0.70673. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68287/0.70678. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68346/0.70666. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68403/0.70620. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68250/0.70644. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 0.68177/0.70647. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68201/0.70691. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68286/0.70688. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68157/0.70728. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.68085/0.70719. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68137/0.70693. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68273/0.70697. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 0.68028/0.70698. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67904/0.70664. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68058/0.70698. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.68188/0.70667. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67894/0.70647. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.68007/0.70661. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.68111/0.70570. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67902/0.70556. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67849/0.70535. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67878/0.70569. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67825/0.70592. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67837/0.70645. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67652/0.70594. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67803/0.70518. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67684/0.70575. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67895/0.70565. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67441/0.70621. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67567/0.70670. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67485/0.70704. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67392/0.70729. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67579/0.70721. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67604/0.70704. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67435/0.70640. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67610/0.70584. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 0.67451/0.70666. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.67340/0.70690. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 0.67511/0.70704. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.67409/0.70786. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 0.67222/0.70772. Took 0.18 sec\n",
      "Epoch 83, Loss(train/val) 0.67265/0.70827. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 0.67243/0.70800. Took 0.18 sec\n",
      "Epoch 85, Loss(train/val) 0.67074/0.70812. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 0.66888/0.70844. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 0.67026/0.70852. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66808/0.70809. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 0.66933/0.70898. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 0.66835/0.70987. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 0.66853/0.71000. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 0.66618/0.70996. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 0.66730/0.71090. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.66664/0.70992. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 0.66768/0.71023. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 0.66576/0.71111. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 0.66507/0.71103. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 0.66454/0.71136. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.66324/0.71138. Took 0.19 sec\n",
      "ACC: 0.46875\n",
      "Epoch 0, Loss(train/val) 0.69570/0.69619. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 0.69399/0.69639. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69321/0.69655. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 0.69225/0.69682. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69222/0.69690. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.69182/0.69697. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69175/0.69712. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.69130/0.69742. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 0.69126/0.69779. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 0.69081/0.69805. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.69072/0.69855. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.69057/0.69905. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 0.69020/0.69957. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.69043/0.70010. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68985/0.70080. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68886/0.70127. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68985/0.70195. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68901/0.70237. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68918/0.70288. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 0.68886/0.70322. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68828/0.70376. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68854/0.70424. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68851/0.70418. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68804/0.70495. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68797/0.70543. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 0.68793/0.70567. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68767/0.70622. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68716/0.70660. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68698/0.70674. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 0.68782/0.70738. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68717/0.70690. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 0.68668/0.70784. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68673/0.70825. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68700/0.70817. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68608/0.70878. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 0.68539/0.70942. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68696/0.70955. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68627/0.70989. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68601/0.70967. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68461/0.71025. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68502/0.71085. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68536/0.71100. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68584/0.71120. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68415/0.71162. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68467/0.71234. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68416/0.71225. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68465/0.71280. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68327/0.71271. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68341/0.71297. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68186/0.71258. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68317/0.71329. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.68237/0.71409. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68206/0.71461. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.68203/0.71555. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68283/0.71587. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68178/0.71592. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68164/0.71622. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.68090/0.71696. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68071/0.71682. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.68078/0.71732. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.68072/0.71749. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.68108/0.71803. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67910/0.71876. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67980/0.71858. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67866/0.71893. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67828/0.71959. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67767/0.71958. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67885/0.71968. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67668/0.72032. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67735/0.72025. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67617/0.72123. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67658/0.72156. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67713/0.72127. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67403/0.72149. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67534/0.72258. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.67456/0.72265. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67468/0.72277. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67521/0.72315. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67254/0.72386. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67307/0.72474. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67190/0.72424. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.67209/0.72413. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67267/0.72404. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67075/0.72395. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66842/0.72399. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.67036/0.72529. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.67020/0.72544. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66862/0.72549. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66905/0.72576. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66756/0.72565. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66786/0.72612. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 0.66772/0.72771. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66513/0.72713. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.66501/0.72865. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 0.66558/0.72827. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66642/0.72899. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66743/0.73006. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66227/0.73043. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66505/0.73250. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.66262/0.73140. Took 0.18 sec\n",
      "ACC: 0.4375\n",
      "Epoch 0, Loss(train/val) 0.69467/0.70650. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69142/0.70892. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69009/0.71327. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.68947/0.71511. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.68949/0.71718. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 0.68906/0.71878. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68841/0.72020. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68804/0.72193. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68766/0.72236. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68840/0.72338. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68718/0.72373. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 0.68733/0.72423. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68665/0.72457. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68657/0.72550. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68680/0.72589. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 0.68649/0.72626. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 0.68703/0.72624. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68634/0.72623. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68674/0.72592. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68572/0.72556. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68583/0.72634. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 0.68525/0.72628. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68386/0.72658. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68509/0.72643. Took 0.18 sec\n",
      "Epoch 24, Loss(train/val) 0.68468/0.72620. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68420/0.72642. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68403/0.72633. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 0.68425/0.72579. Took 0.18 sec\n",
      "Epoch 28, Loss(train/val) 0.68399/0.72609. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68357/0.72617. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68321/0.72553. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68292/0.72477. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68281/0.72523. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68234/0.72487. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68254/0.72528. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68222/0.72458. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68243/0.72475. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68172/0.72422. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68155/0.72375. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 0.68118/0.72417. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68086/0.72455. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68128/0.72463. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68134/0.72422. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 0.68071/0.72349. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.67956/0.72365. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68009/0.72422. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67942/0.72335. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67966/0.72351. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.67754/0.72235. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67823/0.72183. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.67878/0.72143. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67829/0.72311. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67739/0.72276. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67691/0.72265. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67531/0.72284. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 0.67660/0.72175. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67741/0.72297. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 0.67681/0.72284. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67609/0.72262. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67530/0.72140. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67556/0.72089. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67550/0.72003. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.67379/0.72138. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67260/0.72122. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 0.67417/0.72103. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67339/0.72111. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67247/0.72094. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 0.67192/0.72199. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67362/0.72060. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 0.67195/0.71919. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67225/0.71946. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67094/0.71945. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.66960/0.71920. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 0.67063/0.72108. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.66748/0.72021. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 0.66906/0.72035. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66906/0.72102. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66597/0.71930. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66787/0.71961. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 0.66692/0.72013. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66736/0.71907. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66775/0.72004. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66493/0.72057. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.66438/0.72129. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66478/0.72047. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 0.66373/0.72315. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66389/0.72191. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 0.66016/0.72423. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 0.66240/0.72275. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66106/0.72313. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66012/0.72284. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 0.66022/0.72324. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65942/0.72386. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 0.65785/0.72387. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.66009/0.72430. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65807/0.72407. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65542/0.72523. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 0.65719/0.72640. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65550/0.72715. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 0.65359/0.72773. Took 0.18 sec\n",
      "ACC: 0.4270833333333333\n",
      "Epoch 0, Loss(train/val) 0.69361/0.69019. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69322/0.68971. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 0.69186/0.68960. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 0.69175/0.68954. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 0.69113/0.68952. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 0.68972/0.68957. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 0.68991/0.68963. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 0.68885/0.68964. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68811/0.68970. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68832/0.68997. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 0.68788/0.69016. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68736/0.69037. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68696/0.69077. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 0.68744/0.69113. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 0.68621/0.69165. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 0.68609/0.69202. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 0.68584/0.69210. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 0.68641/0.69224. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 0.68544/0.69262. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 0.68538/0.69286. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 0.68541/0.69321. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 0.68503/0.69335. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 0.68553/0.69360. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 0.68467/0.69378. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68470/0.69405. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68447/0.69424. Took 0.18 sec\n",
      "Epoch 26, Loss(train/val) 0.68405/0.69461. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68467/0.69470. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 0.68449/0.69463. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 0.68459/0.69469. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 0.68320/0.69502. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68409/0.69513. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 0.68415/0.69519. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 0.68219/0.69537. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 0.68399/0.69544. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 0.68173/0.69590. Took 0.18 sec\n",
      "Epoch 36, Loss(train/val) 0.68269/0.69602. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 0.68349/0.69611. Took 0.18 sec\n",
      "Epoch 38, Loss(train/val) 0.68215/0.69667. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68174/0.69711. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 0.68266/0.69713. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 0.68163/0.69720. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 0.68120/0.69713. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 0.68109/0.69760. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68107/0.69807. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 0.68135/0.69849. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.67951/0.69889. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.67888/0.69891. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68026/0.69882. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.67964/0.69942. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68023/0.69974. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.67969/0.69995. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.67823/0.70072. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 0.67836/0.70087. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.67899/0.70149. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.67581/0.70176. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.67680/0.70224. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 0.67819/0.70259. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.67750/0.70291. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67662/0.70290. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67523/0.70345. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 0.67632/0.70394. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 0.67453/0.70443. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 0.67390/0.70500. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67418/0.70567. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67546/0.70541. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67317/0.70557. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67559/0.70564. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67167/0.70576. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67111/0.70593. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67213/0.70656. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67202/0.70640. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67024/0.70681. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67078/0.70625. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67049/0.70583. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.66679/0.70635. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.66832/0.70612. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.66857/0.70705. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.66663/0.70757. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.66567/0.70707. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.66621/0.70686. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 0.66260/0.70687. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.66365/0.70680. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 0.66226/0.70741. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66185/0.70708. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66300/0.70696. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.65722/0.70643. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.65910/0.70782. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.65924/0.70814. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 0.65884/0.70673. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.65895/0.70526. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.65410/0.70602. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.65422/0.70567. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 0.65423/0.70626. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 0.65199/0.70463. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.65188/0.70461. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.65315/0.70397. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.65154/0.70364. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.65007/0.70375. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 0.64837/0.70252. Took 0.18 sec\n",
      "ACC: 0.40625\n",
      "Epoch 0, Loss(train/val) 0.69743/0.69683. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 0.69245/0.69608. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 0.69120/0.69636. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 0.69129/0.69611. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 0.69060/0.69599. Took 0.26 sec\n",
      "Epoch 5, Loss(train/val) 0.69030/0.69618. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 0.69051/0.69612. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 0.68997/0.69606. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 0.68891/0.69620. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 0.68936/0.69616. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 0.68934/0.69690. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 0.68840/0.69631. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 0.68876/0.69667. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 0.68806/0.69673. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 0.68775/0.69672. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 0.68881/0.69678. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 0.68739/0.69694. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 0.68758/0.69632. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 0.68709/0.69662. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 0.68717/0.69601. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 0.68674/0.69581. Took 0.22 sec\n",
      "Epoch 21, Loss(train/val) 0.68729/0.69611. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 0.68593/0.69543. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 0.68572/0.69584. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 0.68623/0.69563. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 0.68647/0.69610. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 0.68593/0.69570. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 0.68700/0.69525. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 0.68654/0.69517. Took 0.22 sec\n",
      "Epoch 29, Loss(train/val) 0.68493/0.69518. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 0.68559/0.69547. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 0.68559/0.69552. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 0.68569/0.69515. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 0.68453/0.69475. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 0.68559/0.69449. Took 0.22 sec\n",
      "Epoch 35, Loss(train/val) 0.68560/0.69462. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 0.68535/0.69479. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 0.68449/0.69491. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 0.68358/0.69495. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 0.68485/0.69505. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 0.68336/0.69455. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 0.68384/0.69496. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 0.68403/0.69538. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 0.68378/0.69501. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 0.68250/0.69486. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 0.68260/0.69579. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 0.68207/0.69610. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 0.68117/0.69589. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 0.68258/0.69586. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 0.68228/0.69603. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 0.68126/0.69586. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 0.68175/0.69616. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 0.68157/0.69702. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 0.68195/0.69666. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 0.68062/0.69696. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 0.68257/0.69719. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 0.68098/0.69695. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 0.68130/0.69644. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 0.68031/0.69735. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 0.67815/0.69752. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 0.67869/0.69823. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 0.67865/0.69805. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 0.68022/0.69909. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 0.67547/0.69959. Took 0.18 sec\n",
      "Epoch 64, Loss(train/val) 0.67778/0.69911. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 0.67787/0.69987. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 0.67720/0.70010. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 0.67618/0.70040. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 0.67706/0.69989. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 0.67696/0.70010. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 0.67642/0.70146. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 0.67522/0.70165. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 0.67438/0.70289. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 0.67581/0.70221. Took 0.18 sec\n",
      "Epoch 74, Loss(train/val) 0.67465/0.70318. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 0.67339/0.70308. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 0.67376/0.70361. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 0.67332/0.70532. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 0.67445/0.70477. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 0.67052/0.70617. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 0.67113/0.70616. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 0.67172/0.70530. Took 0.18 sec\n",
      "Epoch 82, Loss(train/val) 0.67121/0.70686. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 0.67259/0.70653. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 0.66985/0.70646. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 0.66883/0.70788. Took 0.18 sec\n",
      "Epoch 86, Loss(train/val) 0.66957/0.70793. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 0.66835/0.70900. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 0.66852/0.70989. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 0.66971/0.70952. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 0.66626/0.71097. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 0.66768/0.71016. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 0.66663/0.71071. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 0.66860/0.71106. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 0.66554/0.71245. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 0.66467/0.71375. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 0.66649/0.71430. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 0.66360/0.71446. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 0.66305/0.71408. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 0.66374/0.71544. Took 0.18 sec\n",
      "ACC: 0.5\n"
     ]
    }
   ],
   "source": [
    "## 실행파일\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from Stock_dataloader_csv_ti import stock_csv_read\n",
    "from Stock_Dataset import StockDataset\n",
    "\n",
    "\n",
    "args.data_list = os.listdir(r\"C:\\Users\\lab\\Desktop\\Multimodal_Transformer\\data\\kdd17\\price_long_50\")\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'Multimodal_transformer_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        est = time.time()\n",
    "\n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"Multimodal_transformer_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        csv_read = stock_csv_read(data, args.ts_len,args.target_len)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "\n",
    "        with open(args.new_file_path + '\\\\'+ str(args.symbol)+'test_acc_list' +'.csv', 'w',newline='') as alist:\n",
    "            www = csv.writer(alist)\n",
    "            www.writerow([\"acc_list\"])\n",
    "        \n",
    "            ACC_cv = []\n",
    "            for i, data in enumerate(split_data_list):\n",
    "                \n",
    "                args.sp_ith = i\n",
    "\n",
    "                args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "                os.makedirs(args.split_file_path)\n",
    "                ## Model\n",
    "                Transformer = args.Transformer(args.m1_input_feature_size,args.m2_input_feature_size,args.m3_input_feature_size,\n",
    "                                            args.Transformer_feature_size, args.nhead, args,\n",
    "                                            args.nlayer, args.dropout, args.ts_len)\n",
    "                Transformer.to(args.device)\n",
    "\n",
    "                ##Optimizer\n",
    "                Transformer_optimizer = optim.Adam(Transformer.parameters(), lr=args.lr, weight_decay=args.L2)\n",
    "\n",
    "                ## training\n",
    "                Train_losses = []\n",
    "                Validation_losses = []\n",
    "                for epoch in range(args.epoch):\n",
    "                    ts = time.time()\n",
    "\n",
    "                    trainset = StockDataset(data[0])\n",
    "                    valset = StockDataset(data[1])\n",
    "                    testset = StockDataset(data[2])\n",
    "\n",
    "\n",
    "                    partition = {'train': trainset, 'val': valset, 'test': testset}     \n",
    "\n",
    "                    Transformer, train_loss = train(Transformer,Transformer_optimizer, args, partition)\n",
    "                    Transformer, validation_loss = validation(Transformer, args, partition)\n",
    "\n",
    "\n",
    "                    ## .state_dict() : model의 parameter(W)만을 저장하는것임 => 다시 불러올 때 모델의 파라미터를 알고있어야함\n",
    "                    if len(Validation_losses) == 0:\n",
    "                        torch.save(Transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Transformer' +'.pt')\n",
    "                    elif min(Validation_losses) > validation_loss:\n",
    "                        torch.save(Transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_Transformer' +'.pt')\n",
    "                    \n",
    "                    Train_losses.append(train_loss)\n",
    "                    Validation_losses.append(validation_loss)\n",
    "                    \n",
    "                    te = time.time()\n",
    "\n",
    "                    print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "                    .format(epoch, train_loss, validation_loss, te - ts))\n",
    "\n",
    "                ## Test\n",
    "                # state_dict로 저장했기 때문에 model의 hyperparameter를 불러와야함\n",
    "                Transformer = args.Transformer(args.m1_input_feature_size,args.m2_input_feature_size,args.m3_input_feature_size,\n",
    "                                            args.Transformer_feature_size, args.nhead, args,\n",
    "                                            args.nlayer, args.dropout,args.ts_len)\n",
    "                Transformer.to(args.device)\n",
    "\n",
    "                # Model_selection\n",
    "                min_val_losses = Validation_losses.index(min(Validation_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "\n",
    "                Transformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(min_val_losses) +'_transformer' + '.pt'))\n",
    "\n",
    "                ACC = test(Transformer, args, partition)\n",
    "                print('ACC: {}'.format(ACC))\n",
    "                www.writerow([ACC])\n",
    "\n",
    "                with open(args.split_file_path + '\\\\'+ str(min_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "                    print('ACC: {}'.format(ACC), file=fd)\n",
    "\n",
    "                result = {}\n",
    "\n",
    "                result['train_losses'] = Train_losses\n",
    "                result['val_losses'] = Validation_losses\n",
    "                result['ACC'] = ACC\n",
    "\n",
    "                eet = time.time()\n",
    "                entire_exp_time = eet - est\n",
    "\n",
    "                ## draw loss curve\n",
    "                fig = plt.figure()\n",
    "                plt.plot(result['train_losses'])\n",
    "                plt.plot(result['val_losses'])\n",
    "                plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "                plt.xlabel('epoch', fontsize=15)\n",
    "                plt.ylabel('loss', fontsize=15)\n",
    "                plt.grid()\n",
    "                plt.savefig(args.split_file_path + '\\\\' + 'fig' + '.png')\n",
    "                plt.close(fig)\n",
    "                ACC_cv.append(result['ACC'])\n",
    "\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"MM_Transformer\", args.symbol, entire_exp_time, acc_avg, acc_std])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('taewon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "013403e7ebf8f35ee0411721c7e4b108aa3c3f8cb903b89610d110413a68ec3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
