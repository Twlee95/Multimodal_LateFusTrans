{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db823349-554f-4e1c-8b9f-47a4503d8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "importing Jupyter notebook from AttLSTM.ipynb\n",
      "importing Jupyter notebook from attention.ipynb\n",
      "importing Jupyter notebook from Transformer_Encoder.ipynb\n",
      "importing Jupyter notebook from metric.ipynb\n",
      "importing Jupyter notebook from loss_fn.ipynb\n",
      "importing Jupyter notebook from Stock_datasets_csv.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('C:\\\\taewon_project\\\\DTML_selfatt')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Stock_Dataset import StockDataset\n",
    "import argparse\n",
    "from AttLSTM import att_LSTM\n",
    "from Transformer_Encoder import Transformer\n",
    "import numpy as np\n",
    "import time\n",
    "from metric import metric_acc as ACC\n",
    "from metric import metric_mcc as MCC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from loss_fn import Selective_Regularization\n",
    "from Stock_datasets_csv import stock_csv_read\n",
    "\n",
    "def train(att_LSTM,transformer,                                  ## Model\n",
    "          att_LSTM_optimizer, transformer_optimizer,   ## Optimizer\n",
    "          Partition, args):                                      ## Data, loss function, argument\n",
    "    trainloader = DataLoader(Partition['train'],\n",
    "                             batch_size = args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "\n",
    "    att_LSTM.train()\n",
    "    transformer.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for j, data in enumerate(trainloader):\n",
    "        data_out_list = []\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            att_LSTM_optimizer.zero_grad()\n",
    "            transformer_optimizer.zero_grad()\n",
    "            \n",
    "            x_input = data[i][0].to(args.device)\n",
    "            \n",
    "            if i == 1:\n",
    "                true_y = data[i][1].squeeze().float().to(args.device)\n",
    "            \n",
    "            att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "            \n",
    "            # 'list' object has no attribute 'float'\n",
    "\n",
    "            hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "            data_out_list.append(hidden_context)\n",
    "\n",
    "        index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "        stock_output = data_out_list[1] # torch.Size([128, 10])\n",
    "        \n",
    "        # torch.Size([64, 10, 10]) batch seq fea\n",
    "        Transformer_input = index_output* args.market_beta + stock_output\n",
    "        \n",
    "        \n",
    "        output1,wp,bp = transformer(Transformer_input.transpose(0,1))\n",
    "\n",
    "\n",
    "        # output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "        # output_.requires_grad=True\n",
    "        \n",
    "        loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "        loss.backward()\n",
    "\n",
    "        att_LSTM_optimizer.step() ## parameter 갱신\n",
    "        transformer_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return att_LSTM, transformer, train_loss\n",
    "\n",
    "\n",
    "def validation(att_LSTM,transformer,\n",
    "               partition, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(valloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(data)):\n",
    "                \n",
    "                x_input = data[i][0].to(args.device)\n",
    "\n",
    "                if i == 1:\n",
    "                    true_y = data[i][1].squeeze().float().to(args.device)\n",
    "                    \n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "\n",
    "            Transformer_input = index_output* args.market_beta  + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1))\n",
    "\n",
    "            loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        return att_LSTM, transformer, val_loss\n",
    "\n",
    "\n",
    "def test(att_LSTM, transformer,\n",
    "               partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "\n",
    "    ACC_metric = 0.0\n",
    "    MCC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(testloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(data)):\n",
    "                \n",
    "                x_input = data[i][0].to(args.device)\n",
    "\n",
    "                if i == 1:\n",
    "                    true_y = data[i][1].squeeze().float().to(args.device)\n",
    "\n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "\n",
    "            Transformer_input = index_output * args.market_beta + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1))\n",
    "\n",
    "            output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            ACC_metric += ACC(output_, true_y)\n",
    "            MCC_metric += MCC(output_, true_y)\n",
    "\n",
    "        ACC_metric = ACC_metric / len(testloader)\n",
    "        MCC_metric = MCC_metric / len(testloader)\n",
    "\n",
    "        return ACC_metric, MCC_metric\n",
    "\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "    att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size, args.dropout, args.use_bn, args.attention_head, args.attn_size, args.activation)\n",
    "    transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers, args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "    \n",
    "    att_LSTM.to(args.device)\n",
    "    transformer.to(args.device)\n",
    "\n",
    "    if args.optim == 'SGD':\n",
    "        att_LSTM_optimizer = optim.SGD(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.SGD(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        att_LSTM_optimizer = optim.RMSprop(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.RMSprop(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        att_LSTM_optimizer = optim.Adam(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.Adam(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # ===================================== #\n",
    "    for epoch in range(args.epoch):\n",
    "        ts = time.time()\n",
    "        att_LSTM, transformer, train_loss = train(att_LSTM, transformer,\n",
    "                                                  att_LSTM_optimizer, transformer_optimizer,\n",
    "                                                  partition, args)\n",
    "\n",
    "        att_LSTM, transformer, val_loss = validation(att_LSTM, transformer, partition, args)\n",
    "\n",
    "        te = time.time()\n",
    "\n",
    "        ## 각 에폭마다 모델을 저장하기 위한 코드\n",
    "        torch.save(att_LSTM.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_epoch' +'_att_LSTM' +'.pt')\n",
    "        torch.save(transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_epoch' +'_transformer' +'.pt')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "              .format(epoch, train_loss, val_loss, te - ts))\n",
    "\n",
    "    ## val_losses에서 가장 값이 최소인 위치를 저장함\n",
    "    site_val_losses = val_losses.index(min(val_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "    att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size, args.dropout, args.use_bn, args.attention_head, args.attn_size, args.activation)\n",
    "    transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers,args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "    att_LSTM.to(args.device)\n",
    "    transformer.to(args.device)\n",
    "\n",
    "    att_LSTM.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) +'_epoch' +'_att_LSTM'+ '.pt'))\n",
    "    transformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) + '_epoch' + '_transformer' + '.pt'))\n",
    "\n",
    "    ACC,MCC = test(att_LSTM, transformer, partition, args)\n",
    "    print('ACC: {}, MCC: {}'.format(ACC, MCC))\n",
    "\n",
    "    with open(args.split_file_path + '\\\\'+ str(site_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "        print('ACC: {}, MCC: {}'.format(ACC, MCC), file=fd)\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['ACC'] = ACC\n",
    "    result['MCC'] = MCC\n",
    "\n",
    "    return vars(args), result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd115972-567d-475b-ac08-15dedfc7d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ========= experiment setting ========== #\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.save_file_path = \"c:\\\\taewon_project\\\\DTML_selfatt\\\\results\"\n",
    "\n",
    "\n",
    "# ====== hyperparameter ======= #\n",
    "args.batch_size = 64\n",
    "args.x_frames = 10\n",
    "args.y_frames = 1\n",
    "args.input_dim = 10\n",
    "args.output_dim = 1\n",
    "args.dropout = 0.2\n",
    "args.use_bn = True\n",
    "args.loss_fn = Selective_Regularization  ## loss function for classification : cross entropy\n",
    "args.optim = 'Adam'\n",
    "args.lr = 0.0005\n",
    "args.l2 = 0.00001 #?\n",
    "args.epoch = 100\n",
    "args.activation=\"ReLU\"\n",
    "\n",
    "# ============= model ================== #\n",
    "args.att_LSTM = att_LSTM\n",
    "args.transformer = Transformer\n",
    "\n",
    "# ====== att_lstm hyperparameter ======= #\n",
    "args.hid_dim = 10\n",
    "args.attention_head = 1\n",
    "args.attn_size = 10\n",
    "args.num_layers = 1\n",
    "args.decoder_x_frames = 1\n",
    "\n",
    "# ====== transformer hyperparameter ======= #\n",
    "args.trans_feature_size = 40\n",
    "args.trans_num_laysers = 1\n",
    "args.trans_nhead = 10\n",
    "args.market_beta = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fdb0bdf-b811-4e2c-a9a4-1289b194c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss(train/val) 4.72787/4.72836. Took 0.50 sec\n",
      "Epoch 1, Loss(train/val) 4.72654/4.72919. Took 0.54 sec\n",
      "Epoch 2, Loss(train/val) 4.72582/4.72916. Took 0.59 sec\n",
      "Epoch 3, Loss(train/val) 4.72601/4.72952. Took 0.49 sec\n",
      "Epoch 4, Loss(train/val) 4.72573/4.72984. Took 0.48 sec\n",
      "Epoch 5, Loss(train/val) 4.72471/4.73049. Took 0.48 sec\n",
      "Epoch 6, Loss(train/val) 4.72507/4.73069. Took 0.50 sec\n",
      "Epoch 7, Loss(train/val) 4.72482/4.73134. Took 0.48 sec\n",
      "Epoch 8, Loss(train/val) 4.72502/4.73332. Took 0.48 sec\n",
      "Epoch 9, Loss(train/val) 4.72403/4.73541. Took 0.50 sec\n",
      "Epoch 10, Loss(train/val) 4.72368/4.73708. Took 0.47 sec\n",
      "Epoch 11, Loss(train/val) 4.72404/4.73860. Took 0.57 sec\n",
      "Epoch 12, Loss(train/val) 4.72372/4.73717. Took 0.51 sec\n",
      "Epoch 13, Loss(train/val) 4.72425/4.73832. Took 0.50 sec\n",
      "Epoch 14, Loss(train/val) 4.72369/4.73791. Took 0.50 sec\n",
      "Epoch 15, Loss(train/val) 4.72394/4.73984. Took 0.57 sec\n",
      "Epoch 16, Loss(train/val) 4.72297/4.74052. Took 0.56 sec\n",
      "Epoch 17, Loss(train/val) 4.72258/4.74111. Took 0.57 sec\n",
      "Epoch 18, Loss(train/val) 4.72143/4.74089. Took 0.56 sec\n",
      "Epoch 19, Loss(train/val) 4.72283/4.74026. Took 0.50 sec\n",
      "Epoch 20, Loss(train/val) 4.72124/4.74082. Took 0.47 sec\n",
      "Epoch 21, Loss(train/val) 4.72001/4.74237. Took 0.49 sec\n",
      "Epoch 22, Loss(train/val) 4.72041/4.74067. Took 0.50 sec\n",
      "Epoch 23, Loss(train/val) 4.72245/4.73760. Took 0.49 sec\n",
      "Epoch 24, Loss(train/val) 4.72063/4.74107. Took 0.47 sec\n",
      "Epoch 25, Loss(train/val) 4.71764/4.74524. Took 0.52 sec\n",
      "Epoch 26, Loss(train/val) 4.72101/4.74204. Took 0.56 sec\n",
      "Epoch 27, Loss(train/val) 4.71820/4.74381. Took 0.63 sec\n",
      "Epoch 28, Loss(train/val) 4.71811/4.74245. Took 0.61 sec\n",
      "Epoch 29, Loss(train/val) 4.71611/4.74756. Took 0.53 sec\n",
      "Epoch 30, Loss(train/val) 4.71664/4.74291. Took 0.50 sec\n",
      "Epoch 31, Loss(train/val) 4.71663/4.73773. Took 0.58 sec\n",
      "Epoch 32, Loss(train/val) 4.71592/4.73922. Took 0.57 sec\n",
      "Epoch 33, Loss(train/val) 4.71552/4.74064. Took 0.57 sec\n",
      "Epoch 34, Loss(train/val) 4.71670/4.74123. Took 0.59 sec\n",
      "Epoch 35, Loss(train/val) 4.71228/4.74253. Took 0.50 sec\n",
      "Epoch 36, Loss(train/val) 4.71088/4.74383. Took 0.52 sec\n",
      "Epoch 37, Loss(train/val) 4.71113/4.74404. Took 0.52 sec\n",
      "Epoch 38, Loss(train/val) 4.71366/4.74387. Took 0.48 sec\n",
      "Epoch 39, Loss(train/val) 4.71329/4.74221. Took 0.51 sec\n",
      "Epoch 40, Loss(train/val) 4.70997/4.74902. Took 0.51 sec\n",
      "Epoch 41, Loss(train/val) 4.71734/4.74338. Took 0.60 sec\n",
      "Epoch 42, Loss(train/val) 4.71217/4.74213. Took 0.54 sec\n",
      "Epoch 43, Loss(train/val) 4.71041/4.74629. Took 0.54 sec\n",
      "Epoch 44, Loss(train/val) 4.71346/4.75037. Took 0.57 sec\n",
      "Epoch 45, Loss(train/val) 4.71143/4.74671. Took 0.56 sec\n",
      "Epoch 46, Loss(train/val) 4.70759/4.74993. Took 0.64 sec\n",
      "Epoch 47, Loss(train/val) 4.71036/4.74934. Took 0.70 sec\n",
      "Epoch 48, Loss(train/val) 4.70999/4.74977. Took 0.66 sec\n",
      "Epoch 49, Loss(train/val) 4.70699/4.74951. Took 0.68 sec\n",
      "Epoch 50, Loss(train/val) 4.70716/4.75344. Took 0.67 sec\n",
      "Epoch 51, Loss(train/val) 4.71089/4.75187. Took 0.64 sec\n",
      "Epoch 52, Loss(train/val) 4.70797/4.75477. Took 0.70 sec\n",
      "Epoch 53, Loss(train/val) 4.70574/4.75583. Took 0.56 sec\n",
      "Epoch 54, Loss(train/val) 4.70966/4.75062. Took 0.57 sec\n",
      "Epoch 55, Loss(train/val) 4.70509/4.75396. Took 0.57 sec\n",
      "Epoch 56, Loss(train/val) 4.70557/4.75632. Took 0.60 sec\n",
      "Epoch 57, Loss(train/val) 4.70466/4.75985. Took 0.64 sec\n",
      "Epoch 58, Loss(train/val) 4.70895/4.75442. Took 0.54 sec\n",
      "Epoch 59, Loss(train/val) 4.70429/4.75843. Took 0.57 sec\n",
      "Epoch 60, Loss(train/val) 4.70482/4.75728. Took 0.58 sec\n",
      "Epoch 61, Loss(train/val) 4.70723/4.76262. Took 0.62 sec\n",
      "Epoch 62, Loss(train/val) 4.70442/4.75733. Took 0.53 sec\n",
      "Epoch 63, Loss(train/val) 4.70120/4.76383. Took 0.56 sec\n",
      "Epoch 64, Loss(train/val) 4.70538/4.76551. Took 0.57 sec\n",
      "Epoch 65, Loss(train/val) 4.70621/4.76427. Took 0.60 sec\n",
      "Epoch 66, Loss(train/val) 4.70788/4.75509. Took 0.68 sec\n",
      "Epoch 67, Loss(train/val) 4.70420/4.76238. Took 0.87 sec\n",
      "Epoch 68, Loss(train/val) 4.70271/4.75969. Took 0.83 sec\n",
      "Epoch 69, Loss(train/val) 4.70639/4.76380. Took 0.69 sec\n",
      "Epoch 70, Loss(train/val) 4.70319/4.76162. Took 0.65 sec\n",
      "Epoch 71, Loss(train/val) 4.70614/4.76322. Took 0.68 sec\n",
      "Epoch 72, Loss(train/val) 4.70690/4.76438. Took 0.78 sec\n",
      "Epoch 73, Loss(train/val) 4.70486/4.75964. Took 0.71 sec\n",
      "Epoch 74, Loss(train/val) 4.70258/4.76916. Took 0.70 sec\n",
      "Epoch 75, Loss(train/val) 4.70218/4.77248. Took 0.59 sec\n",
      "Epoch 76, Loss(train/val) 4.70699/4.76316. Took 0.64 sec\n",
      "Epoch 77, Loss(train/val) 4.70256/4.76663. Took 0.67 sec\n",
      "Epoch 78, Loss(train/val) 4.70319/4.76019. Took 0.76 sec\n",
      "Epoch 79, Loss(train/val) 4.70151/4.77342. Took 0.68 sec\n",
      "Epoch 80, Loss(train/val) 4.69763/4.76846. Took 0.64 sec\n",
      "Epoch 81, Loss(train/val) 4.69921/4.77867. Took 0.66 sec\n",
      "Epoch 82, Loss(train/val) 4.70095/4.77858. Took 0.63 sec\n",
      "Epoch 83, Loss(train/val) 4.69951/4.77409. Took 0.68 sec\n",
      "Epoch 84, Loss(train/val) 4.69994/4.77394. Took 0.63 sec\n",
      "Epoch 85, Loss(train/val) 4.70016/4.78567. Took 0.64 sec\n",
      "Epoch 86, Loss(train/val) 4.70384/4.76677. Took 0.70 sec\n",
      "Epoch 87, Loss(train/val) 4.69967/4.76812. Took 0.71 sec\n",
      "Epoch 88, Loss(train/val) 4.69916/4.78270. Took 0.67 sec\n",
      "Epoch 89, Loss(train/val) 4.70198/4.76655. Took 0.81 sec\n",
      "Epoch 90, Loss(train/val) 4.69784/4.78298. Took 0.79 sec\n",
      "Epoch 91, Loss(train/val) 4.69983/4.77669. Took 0.71 sec\n",
      "Epoch 92, Loss(train/val) 4.69906/4.78181. Took 0.70 sec\n",
      "Epoch 93, Loss(train/val) 4.69525/4.78502. Took 0.67 sec\n",
      "Epoch 94, Loss(train/val) 4.70145/4.77706. Took 0.73 sec\n",
      "Epoch 95, Loss(train/val) 4.69876/4.78273. Took 0.66 sec\n",
      "Epoch 96, Loss(train/val) 4.69694/4.77587. Took 0.66 sec\n",
      "Epoch 97, Loss(train/val) 4.69877/4.78357. Took 0.66 sec\n",
      "Epoch 98, Loss(train/val) 4.69768/4.77909. Took 0.67 sec\n",
      "Epoch 99, Loss(train/val) 4.70023/4.78888. Took 0.69 sec\n",
      "ACC: 0.4375, MCC: -0.10421364855284106\n",
      "Epoch 0, Loss(train/val) 4.94375/4.96426. Took 0.65 sec\n",
      "Epoch 1, Loss(train/val) 4.93780/4.96890. Took 0.80 sec\n",
      "Epoch 2, Loss(train/val) 4.93511/4.97254. Took 0.93 sec\n",
      "Epoch 3, Loss(train/val) 4.93680/4.97327. Took 0.87 sec\n",
      "Epoch 4, Loss(train/val) 4.93664/4.97214. Took 0.82 sec\n",
      "Epoch 5, Loss(train/val) 4.93524/4.97348. Took 0.67 sec\n",
      "Epoch 6, Loss(train/val) 4.93389/4.97571. Took 0.74 sec\n",
      "Epoch 7, Loss(train/val) 4.93587/4.97588. Took 0.73 sec\n",
      "Epoch 8, Loss(train/val) 4.93355/4.97729. Took 0.68 sec\n",
      "Epoch 9, Loss(train/val) 4.93426/4.97868. Took 0.72 sec\n",
      "Epoch 10, Loss(train/val) 4.93388/4.98061. Took 0.65 sec\n",
      "Epoch 11, Loss(train/val) 4.93481/4.97977. Took 0.69 sec\n",
      "Epoch 12, Loss(train/val) 4.93401/4.97893. Took 0.74 sec\n",
      "Epoch 13, Loss(train/val) 4.93442/4.97816. Took 0.62 sec\n",
      "Epoch 14, Loss(train/val) 4.93343/4.98158. Took 0.62 sec\n",
      "Epoch 15, Loss(train/val) 4.93350/4.98314. Took 0.74 sec\n",
      "Epoch 16, Loss(train/val) 4.93387/4.98236. Took 0.66 sec\n",
      "Epoch 17, Loss(train/val) 4.93389/4.98209. Took 0.66 sec\n",
      "Epoch 18, Loss(train/val) 4.93183/4.98640. Took 0.66 sec\n",
      "Epoch 19, Loss(train/val) 4.93381/4.98624. Took 0.67 sec\n",
      "Epoch 20, Loss(train/val) 4.93280/4.98619. Took 0.67 sec\n",
      "Epoch 21, Loss(train/val) 4.93107/4.99109. Took 0.67 sec\n",
      "Epoch 22, Loss(train/val) 4.93340/4.98937. Took 0.72 sec\n",
      "Epoch 23, Loss(train/val) 4.93254/4.98669. Took 0.75 sec\n",
      "Epoch 24, Loss(train/val) 4.93109/4.99019. Took 0.72 sec\n",
      "Epoch 25, Loss(train/val) 4.93069/4.98902. Took 0.67 sec\n",
      "Epoch 26, Loss(train/val) 4.92952/4.99010. Took 0.65 sec\n",
      "Epoch 27, Loss(train/val) 4.93111/4.98824. Took 0.62 sec\n",
      "Epoch 28, Loss(train/val) 4.92966/4.98408. Took 0.63 sec\n",
      "Epoch 29, Loss(train/val) 4.92772/4.98888. Took 0.60 sec\n",
      "Epoch 30, Loss(train/val) 4.93026/4.99460. Took 0.62 sec\n",
      "Epoch 31, Loss(train/val) 4.92940/4.99530. Took 0.67 sec\n",
      "Epoch 32, Loss(train/val) 4.92767/4.99412. Took 0.65 sec\n",
      "Epoch 33, Loss(train/val) 4.92835/5.00308. Took 0.63 sec\n",
      "Epoch 34, Loss(train/val) 4.92670/4.99885. Took 0.62 sec\n",
      "Epoch 35, Loss(train/val) 4.92672/4.99448. Took 0.62 sec\n",
      "Epoch 36, Loss(train/val) 4.92754/4.99820. Took 0.66 sec\n",
      "Epoch 37, Loss(train/val) 4.92548/5.00279. Took 0.67 sec\n",
      "Epoch 38, Loss(train/val) 4.92713/4.99823. Took 0.65 sec\n",
      "Epoch 39, Loss(train/val) 4.92539/4.99969. Took 0.71 sec\n",
      "Epoch 40, Loss(train/val) 4.92679/5.00056. Took 0.68 sec\n",
      "Epoch 41, Loss(train/val) 4.92523/5.00161. Took 0.66 sec\n",
      "Epoch 42, Loss(train/val) 4.92706/4.99758. Took 0.66 sec\n",
      "Epoch 43, Loss(train/val) 4.92358/4.99660. Took 0.66 sec\n",
      "Epoch 44, Loss(train/val) 4.92472/5.00044. Took 0.71 sec\n",
      "Epoch 45, Loss(train/val) 4.92490/4.99786. Took 0.75 sec\n",
      "Epoch 46, Loss(train/val) 4.92529/4.99117. Took 0.68 sec\n",
      "Epoch 47, Loss(train/val) 4.92171/4.99226. Took 0.65 sec\n",
      "Epoch 48, Loss(train/val) 4.92250/4.99725. Took 0.59 sec\n",
      "Epoch 49, Loss(train/val) 4.92253/4.99099. Took 0.59 sec\n",
      "Epoch 50, Loss(train/val) 4.92403/4.99993. Took 0.59 sec\n",
      "Epoch 51, Loss(train/val) 4.92074/5.00366. Took 0.60 sec\n",
      "Epoch 52, Loss(train/val) 4.92230/5.00149. Took 0.66 sec\n",
      "Epoch 53, Loss(train/val) 4.92376/4.99890. Took 0.60 sec\n",
      "Epoch 54, Loss(train/val) 4.91890/4.99463. Took 0.59 sec\n",
      "Epoch 55, Loss(train/val) 4.92269/5.00558. Took 0.60 sec\n",
      "Epoch 56, Loss(train/val) 4.92119/5.00206. Took 0.65 sec\n",
      "Epoch 57, Loss(train/val) 4.92113/4.98897. Took 0.57 sec\n",
      "Epoch 58, Loss(train/val) 4.92250/4.99671. Took 0.60 sec\n",
      "Epoch 59, Loss(train/val) 4.91917/5.00033. Took 0.62 sec\n",
      "Epoch 60, Loss(train/val) 4.92014/4.99722. Took 0.63 sec\n",
      "Epoch 61, Loss(train/val) 4.92198/5.00354. Took 0.66 sec\n",
      "Epoch 62, Loss(train/val) 4.92010/5.00233. Took 0.57 sec\n",
      "Epoch 63, Loss(train/val) 4.92004/4.99322. Took 0.59 sec\n",
      "Epoch 64, Loss(train/val) 4.91671/5.00693. Took 0.60 sec\n",
      "Epoch 65, Loss(train/val) 4.91940/5.00079. Took 0.63 sec\n",
      "Epoch 66, Loss(train/val) 4.91921/4.99746. Took 0.62 sec\n",
      "Epoch 67, Loss(train/val) 4.91966/5.00813. Took 0.57 sec\n",
      "Epoch 68, Loss(train/val) 4.91697/5.00424. Took 0.61 sec\n",
      "Epoch 69, Loss(train/val) 4.91833/5.00917. Took 0.61 sec\n",
      "Epoch 70, Loss(train/val) 4.91872/4.99884. Took 0.61 sec\n",
      "Epoch 71, Loss(train/val) 4.91804/5.00013. Took 0.66 sec\n",
      "Epoch 72, Loss(train/val) 4.91687/5.00566. Took 0.59 sec\n",
      "Epoch 73, Loss(train/val) 4.91761/5.00933. Took 0.59 sec\n",
      "Epoch 74, Loss(train/val) 4.91689/4.99758. Took 0.64 sec\n",
      "Epoch 75, Loss(train/val) 4.91793/5.00200. Took 0.62 sec\n",
      "Epoch 76, Loss(train/val) 4.91949/5.01074. Took 0.71 sec\n",
      "Epoch 77, Loss(train/val) 4.91555/5.00571. Took 0.64 sec\n",
      "Epoch 78, Loss(train/val) 4.91577/5.00172. Took 0.60 sec\n",
      "Epoch 79, Loss(train/val) 4.91456/5.01890. Took 0.63 sec\n",
      "Epoch 80, Loss(train/val) 4.91704/4.99704. Took 0.60 sec\n",
      "Epoch 81, Loss(train/val) 4.91709/4.99430. Took 0.59 sec\n",
      "Epoch 82, Loss(train/val) 4.91290/5.00622. Took 0.56 sec\n",
      "Epoch 83, Loss(train/val) 4.91563/5.00398. Took 0.61 sec\n",
      "Epoch 84, Loss(train/val) 4.91694/5.01108. Took 0.63 sec\n",
      "Epoch 85, Loss(train/val) 4.91352/5.01004. Took 0.59 sec\n",
      "Epoch 86, Loss(train/val) 4.91404/5.00204. Took 0.61 sec\n",
      "Epoch 87, Loss(train/val) 4.91155/5.00770. Took 0.60 sec\n",
      "Epoch 88, Loss(train/val) 4.91382/5.00796. Took 0.63 sec\n",
      "Epoch 89, Loss(train/val) 4.91390/5.00810. Took 0.63 sec\n",
      "Epoch 90, Loss(train/val) 4.91773/5.00221. Took 0.62 sec\n",
      "Epoch 91, Loss(train/val) 4.91077/4.99793. Took 0.65 sec\n",
      "Epoch 92, Loss(train/val) 4.91758/5.01547. Took 0.61 sec\n",
      "Epoch 93, Loss(train/val) 4.91945/5.01575. Took 0.71 sec\n",
      "Epoch 94, Loss(train/val) 4.91549/5.00404. Took 0.64 sec\n",
      "Epoch 95, Loss(train/val) 4.91355/5.00751. Took 0.60 sec\n",
      "Epoch 96, Loss(train/val) 4.91070/5.01443. Took 0.69 sec\n",
      "Epoch 97, Loss(train/val) 4.91335/5.00111. Took 0.64 sec\n",
      "Epoch 98, Loss(train/val) 4.91575/5.00765. Took 0.68 sec\n",
      "Epoch 99, Loss(train/val) 4.90838/5.00282. Took 0.60 sec\n",
      "ACC: 0.46875, MCC: -0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 4.83232/4.82624. Took 0.65 sec\n",
      "Epoch 1, Loss(train/val) 4.82749/4.82969. Took 0.76 sec\n",
      "Epoch 2, Loss(train/val) 4.82814/4.82608. Took 0.69 sec\n",
      "Epoch 3, Loss(train/val) 4.82709/4.82849. Took 0.58 sec\n",
      "Epoch 4, Loss(train/val) 4.82677/4.82806. Took 0.59 sec\n",
      "Epoch 5, Loss(train/val) 4.82674/4.82796. Took 0.61 sec\n",
      "Epoch 6, Loss(train/val) 4.82577/4.83105. Took 0.67 sec\n",
      "Epoch 7, Loss(train/val) 4.82631/4.83043. Took 0.84 sec\n",
      "Epoch 8, Loss(train/val) 4.82411/4.83065. Took 0.85 sec\n",
      "Epoch 9, Loss(train/val) 4.82623/4.83127. Took 0.70 sec\n",
      "Epoch 10, Loss(train/val) 4.82581/4.82989. Took 0.70 sec\n",
      "Epoch 11, Loss(train/val) 4.82369/4.83197. Took 0.65 sec\n",
      "Epoch 12, Loss(train/val) 4.82426/4.83032. Took 0.68 sec\n",
      "Epoch 13, Loss(train/val) 4.82166/4.83215. Took 0.82 sec\n",
      "Epoch 14, Loss(train/val) 4.82235/4.83476. Took 0.72 sec\n",
      "Epoch 15, Loss(train/val) 4.82410/4.83259. Took 0.67 sec\n",
      "Epoch 16, Loss(train/val) 4.82228/4.83484. Took 0.71 sec\n",
      "Epoch 17, Loss(train/val) 4.82088/4.83671. Took 0.67 sec\n",
      "Epoch 18, Loss(train/val) 4.81992/4.84073. Took 0.82 sec\n",
      "Epoch 19, Loss(train/val) 4.82297/4.83788. Took 0.78 sec\n",
      "Epoch 20, Loss(train/val) 4.82402/4.83553. Took 0.65 sec\n",
      "Epoch 21, Loss(train/val) 4.82145/4.83640. Took 0.69 sec\n",
      "Epoch 22, Loss(train/val) 4.82161/4.83727. Took 0.79 sec\n",
      "Epoch 23, Loss(train/val) 4.81915/4.84037. Took 0.63 sec\n",
      "Epoch 24, Loss(train/val) 4.81992/4.84124. Took 0.83 sec\n",
      "Epoch 25, Loss(train/val) 4.81999/4.84259. Took 0.85 sec\n",
      "Epoch 26, Loss(train/val) 4.82323/4.83655. Took 0.87 sec\n",
      "Epoch 27, Loss(train/val) 4.82051/4.83789. Took 1.11 sec\n",
      "Epoch 28, Loss(train/val) 4.82039/4.83691. Took 0.80 sec\n",
      "Epoch 29, Loss(train/val) 4.82049/4.83715. Took 0.90 sec\n",
      "Epoch 30, Loss(train/val) 4.82059/4.84082. Took 0.97 sec\n",
      "Epoch 31, Loss(train/val) 4.81907/4.84442. Took 0.97 sec\n",
      "Epoch 32, Loss(train/val) 4.81807/4.84840. Took 0.83 sec\n",
      "Epoch 33, Loss(train/val) 4.81884/4.84672. Took 0.56 sec\n",
      "Epoch 34, Loss(train/val) 4.81825/4.84461. Took 0.57 sec\n",
      "Epoch 35, Loss(train/val) 4.82146/4.84246. Took 0.63 sec\n",
      "Epoch 36, Loss(train/val) 4.81843/4.84503. Took 0.57 sec\n",
      "Epoch 37, Loss(train/val) 4.81943/4.84634. Took 0.61 sec\n",
      "Epoch 38, Loss(train/val) 4.81702/4.84874. Took 0.63 sec\n",
      "Epoch 39, Loss(train/val) 4.81848/4.84928. Took 0.56 sec\n",
      "Epoch 40, Loss(train/val) 4.81873/4.84437. Took 0.62 sec\n",
      "Epoch 41, Loss(train/val) 4.81884/4.84644. Took 0.62 sec\n",
      "Epoch 42, Loss(train/val) 4.81897/4.84637. Took 0.58 sec\n",
      "Epoch 43, Loss(train/val) 4.81931/4.84553. Took 0.58 sec\n",
      "Epoch 44, Loss(train/val) 4.81712/4.84640. Took 0.63 sec\n",
      "Epoch 45, Loss(train/val) 4.81891/4.84745. Took 0.54 sec\n",
      "Epoch 46, Loss(train/val) 4.81620/4.84843. Took 0.55 sec\n",
      "Epoch 47, Loss(train/val) 4.81877/4.84911. Took 0.57 sec\n",
      "Epoch 48, Loss(train/val) 4.81644/4.85114. Took 0.57 sec\n",
      "Epoch 49, Loss(train/val) 4.81824/4.84905. Took 0.63 sec\n",
      "Epoch 50, Loss(train/val) 4.81596/4.84776. Took 0.64 sec\n",
      "Epoch 51, Loss(train/val) 4.81846/4.84928. Took 0.68 sec\n",
      "Epoch 52, Loss(train/val) 4.81878/4.84922. Took 0.70 sec\n",
      "Epoch 53, Loss(train/val) 4.81515/4.84855. Took 0.68 sec\n",
      "Epoch 54, Loss(train/val) 4.81616/4.84883. Took 0.66 sec\n",
      "Epoch 55, Loss(train/val) 4.81754/4.84932. Took 0.63 sec\n",
      "Epoch 56, Loss(train/val) 4.81937/4.84901. Took 0.65 sec\n",
      "Epoch 57, Loss(train/val) 4.81761/4.84745. Took 0.68 sec\n",
      "Epoch 58, Loss(train/val) 4.81587/4.84797. Took 0.67 sec\n",
      "Epoch 59, Loss(train/val) 4.81595/4.84800. Took 0.65 sec\n",
      "Epoch 60, Loss(train/val) 4.81453/4.84845. Took 0.69 sec\n",
      "Epoch 61, Loss(train/val) 4.81850/4.84718. Took 0.64 sec\n",
      "Epoch 62, Loss(train/val) 4.81538/4.84893. Took 0.65 sec\n",
      "Epoch 63, Loss(train/val) 4.81787/4.85151. Took 0.64 sec\n",
      "Epoch 64, Loss(train/val) 4.81521/4.85246. Took 0.62 sec\n",
      "Epoch 65, Loss(train/val) 4.81580/4.85370. Took 0.65 sec\n",
      "Epoch 66, Loss(train/val) 4.81510/4.85587. Took 0.65 sec\n",
      "Epoch 67, Loss(train/val) 4.81486/4.85612. Took 0.72 sec\n",
      "Epoch 68, Loss(train/val) 4.81533/4.85552. Took 0.66 sec\n",
      "Epoch 69, Loss(train/val) 4.81438/4.85404. Took 0.66 sec\n",
      "Epoch 70, Loss(train/val) 4.81531/4.85442. Took 0.64 sec\n",
      "Epoch 71, Loss(train/val) 4.81431/4.85288. Took 0.68 sec\n",
      "Epoch 72, Loss(train/val) 4.81258/4.85806. Took 0.68 sec\n",
      "Epoch 73, Loss(train/val) 4.81218/4.85786. Took 0.71 sec\n",
      "Epoch 74, Loss(train/val) 4.81418/4.85807. Took 0.71 sec\n",
      "Epoch 75, Loss(train/val) 4.81644/4.86036. Took 0.80 sec\n",
      "Epoch 76, Loss(train/val) 4.80972/4.85973. Took 0.73 sec\n",
      "Epoch 77, Loss(train/val) 4.81123/4.85783. Took 0.72 sec\n",
      "Epoch 78, Loss(train/val) 4.81512/4.85865. Took 0.64 sec\n",
      "Epoch 79, Loss(train/val) 4.81337/4.85770. Took 0.67 sec\n",
      "Epoch 80, Loss(train/val) 4.81318/4.85519. Took 0.73 sec\n",
      "Epoch 81, Loss(train/val) 4.81086/4.86018. Took 0.64 sec\n",
      "Epoch 82, Loss(train/val) 4.81374/4.86323. Took 0.64 sec\n",
      "Epoch 83, Loss(train/val) 4.81115/4.86355. Took 0.74 sec\n",
      "Epoch 84, Loss(train/val) 4.81402/4.86143. Took 0.68 sec\n",
      "Epoch 85, Loss(train/val) 4.81262/4.86023. Took 0.64 sec\n",
      "Epoch 86, Loss(train/val) 4.81331/4.85730. Took 0.65 sec\n",
      "Epoch 87, Loss(train/val) 4.81199/4.85928. Took 0.70 sec\n",
      "Epoch 88, Loss(train/val) 4.81246/4.86025. Took 0.68 sec\n",
      "Epoch 89, Loss(train/val) 4.80944/4.85982. Took 0.77 sec\n",
      "Epoch 90, Loss(train/val) 4.81044/4.85925. Took 0.74 sec\n",
      "Epoch 91, Loss(train/val) 4.80957/4.86000. Took 0.70 sec\n",
      "Epoch 92, Loss(train/val) 4.81295/4.86202. Took 0.64 sec\n",
      "Epoch 93, Loss(train/val) 4.81447/4.85729. Took 0.65 sec\n",
      "Epoch 94, Loss(train/val) 4.81545/4.85854. Took 0.70 sec\n",
      "Epoch 95, Loss(train/val) 4.80745/4.86152. Took 0.76 sec\n",
      "Epoch 96, Loss(train/val) 4.80897/4.86323. Took 0.68 sec\n",
      "Epoch 97, Loss(train/val) 4.80752/4.86254. Took 0.72 sec\n",
      "Epoch 98, Loss(train/val) 4.81082/4.86435. Took 0.68 sec\n",
      "Epoch 99, Loss(train/val) 4.80939/4.86441. Took 0.77 sec\n",
      "ACC: 0.484375, MCC: -0.03126526997403612\n",
      "Epoch 0, Loss(train/val) 4.93806/4.96824. Took 0.60 sec\n",
      "Epoch 1, Loss(train/val) 4.93501/4.96620. Took 0.81 sec\n",
      "Epoch 2, Loss(train/val) 4.93468/4.96219. Took 0.88 sec\n",
      "Epoch 3, Loss(train/val) 4.93520/4.96080. Took 1.19 sec\n",
      "Epoch 4, Loss(train/val) 4.93429/4.96163. Took 0.85 sec\n",
      "Epoch 5, Loss(train/val) 4.93416/4.96281. Took 0.70 sec\n",
      "Epoch 6, Loss(train/val) 4.93468/4.96183. Took 0.65 sec\n",
      "Epoch 7, Loss(train/val) 4.93374/4.96113. Took 0.70 sec\n",
      "Epoch 8, Loss(train/val) 4.93417/4.96039. Took 0.69 sec\n",
      "Epoch 9, Loss(train/val) 4.93404/4.96012. Took 0.73 sec\n",
      "Epoch 10, Loss(train/val) 4.93365/4.96013. Took 0.66 sec\n",
      "Epoch 11, Loss(train/val) 4.93307/4.96052. Took 0.66 sec\n",
      "Epoch 12, Loss(train/val) 4.93299/4.95994. Took 0.68 sec\n",
      "Epoch 13, Loss(train/val) 4.93417/4.96173. Took 0.67 sec\n",
      "Epoch 14, Loss(train/val) 4.93272/4.96009. Took 0.66 sec\n",
      "Epoch 15, Loss(train/val) 4.93312/4.96168. Took 0.61 sec\n",
      "Epoch 16, Loss(train/val) 4.93269/4.95942. Took 0.66 sec\n",
      "Epoch 17, Loss(train/val) 4.93307/4.96038. Took 0.59 sec\n",
      "Epoch 18, Loss(train/val) 4.93295/4.95865. Took 0.59 sec\n",
      "Epoch 19, Loss(train/val) 4.93151/4.96199. Took 0.57 sec\n",
      "Epoch 20, Loss(train/val) 4.93111/4.95969. Took 0.64 sec\n",
      "Epoch 21, Loss(train/val) 4.93199/4.96621. Took 0.65 sec\n",
      "Epoch 22, Loss(train/val) 4.93214/4.96166. Took 0.63 sec\n",
      "Epoch 23, Loss(train/val) 4.93289/4.96262. Took 0.64 sec\n",
      "Epoch 24, Loss(train/val) 4.93058/4.96184. Took 0.60 sec\n",
      "Epoch 25, Loss(train/val) 4.93188/4.96404. Took 0.59 sec\n",
      "Epoch 26, Loss(train/val) 4.93106/4.96090. Took 0.56 sec\n",
      "Epoch 27, Loss(train/val) 4.93079/4.96313. Took 0.55 sec\n",
      "Epoch 28, Loss(train/val) 4.92990/4.96467. Took 0.58 sec\n",
      "Epoch 29, Loss(train/val) 4.93298/4.96102. Took 0.54 sec\n",
      "Epoch 30, Loss(train/val) 4.92944/4.95981. Took 0.53 sec\n",
      "Epoch 31, Loss(train/val) 4.92978/4.96562. Took 0.53 sec\n",
      "Epoch 32, Loss(train/val) 4.92802/4.96401. Took 0.55 sec\n",
      "Epoch 33, Loss(train/val) 4.93033/4.96494. Took 0.53 sec\n",
      "Epoch 34, Loss(train/val) 4.92883/4.96201. Took 0.56 sec\n",
      "Epoch 35, Loss(train/val) 4.92900/4.96474. Took 0.55 sec\n",
      "Epoch 36, Loss(train/val) 4.92797/4.96563. Took 0.53 sec\n",
      "Epoch 37, Loss(train/val) 4.92996/4.96428. Took 0.57 sec\n",
      "Epoch 38, Loss(train/val) 4.92942/4.96413. Took 0.56 sec\n",
      "Epoch 39, Loss(train/val) 4.92791/4.96516. Took 0.57 sec\n",
      "Epoch 40, Loss(train/val) 4.92808/4.96626. Took 0.53 sec\n",
      "Epoch 41, Loss(train/val) 4.92997/4.96609. Took 0.57 sec\n",
      "Epoch 42, Loss(train/val) 4.92715/4.96609. Took 0.56 sec\n",
      "Epoch 43, Loss(train/val) 4.92487/4.97109. Took 0.55 sec\n",
      "Epoch 44, Loss(train/val) 4.92509/4.97181. Took 0.53 sec\n",
      "Epoch 45, Loss(train/val) 4.92695/4.97275. Took 0.55 sec\n",
      "Epoch 46, Loss(train/val) 4.92583/4.96945. Took 0.52 sec\n",
      "Epoch 47, Loss(train/val) 4.92578/4.97609. Took 0.60 sec\n",
      "Epoch 48, Loss(train/val) 4.92872/4.96840. Took 0.63 sec\n",
      "Epoch 49, Loss(train/val) 4.92596/4.97089. Took 0.61 sec\n",
      "Epoch 50, Loss(train/val) 4.92428/4.97620. Took 0.57 sec\n",
      "Epoch 51, Loss(train/val) 4.92596/4.97594. Took 0.57 sec\n",
      "Epoch 52, Loss(train/val) 4.92459/4.96831. Took 0.59 sec\n",
      "Epoch 53, Loss(train/val) 4.92555/4.97375. Took 0.70 sec\n",
      "Epoch 54, Loss(train/val) 4.92532/4.97342. Took 0.84 sec\n",
      "Epoch 55, Loss(train/val) 4.92333/4.97345. Took 0.79 sec\n",
      "Epoch 56, Loss(train/val) 4.92278/4.98243. Took 0.70 sec\n",
      "Epoch 57, Loss(train/val) 4.92538/4.97309. Took 0.82 sec\n",
      "Epoch 58, Loss(train/val) 4.92458/4.98733. Took 0.82 sec\n",
      "Epoch 59, Loss(train/val) 4.92343/4.97814. Took 0.65 sec\n",
      "Epoch 60, Loss(train/val) 4.92411/4.97511. Took 0.64 sec\n",
      "Epoch 61, Loss(train/val) 4.92328/4.98013. Took 0.59 sec\n",
      "Epoch 62, Loss(train/val) 4.92151/4.98112. Took 0.62 sec\n",
      "Epoch 63, Loss(train/val) 4.92400/4.98269. Took 0.60 sec\n",
      "Epoch 64, Loss(train/val) 4.91976/4.97948. Took 0.64 sec\n",
      "Epoch 65, Loss(train/val) 4.91985/4.97840. Took 0.64 sec\n",
      "Epoch 66, Loss(train/val) 4.91983/4.98501. Took 0.65 sec\n",
      "Epoch 67, Loss(train/val) 4.92084/4.98453. Took 0.65 sec\n",
      "Epoch 68, Loss(train/val) 4.92259/4.99280. Took 0.60 sec\n",
      "Epoch 69, Loss(train/val) 4.92554/4.98695. Took 0.57 sec\n",
      "Epoch 70, Loss(train/val) 4.92300/4.98335. Took 0.62 sec\n",
      "Epoch 71, Loss(train/val) 4.92025/4.98283. Took 0.69 sec\n",
      "Epoch 72, Loss(train/val) 4.92173/4.99073. Took 0.57 sec\n",
      "Epoch 73, Loss(train/val) 4.92636/4.97973. Took 0.68 sec\n",
      "Epoch 74, Loss(train/val) 4.92298/4.98523. Took 0.67 sec\n",
      "Epoch 75, Loss(train/val) 4.92009/4.98682. Took 0.58 sec\n",
      "Epoch 76, Loss(train/val) 4.92280/4.98062. Took 0.57 sec\n",
      "Epoch 77, Loss(train/val) 4.92289/4.98667. Took 0.60 sec\n",
      "Epoch 78, Loss(train/val) 4.92221/4.98696. Took 0.55 sec\n",
      "Epoch 79, Loss(train/val) 4.91950/4.98566. Took 0.59 sec\n",
      "Epoch 80, Loss(train/val) 4.92108/4.98237. Took 0.55 sec\n",
      "Epoch 81, Loss(train/val) 4.91968/4.99171. Took 0.58 sec\n",
      "Epoch 82, Loss(train/val) 4.92128/4.98565. Took 0.58 sec\n",
      "Epoch 83, Loss(train/val) 4.92079/4.99008. Took 0.54 sec\n",
      "Epoch 84, Loss(train/val) 4.91945/4.98664. Took 0.61 sec\n",
      "Epoch 85, Loss(train/val) 4.91747/4.99214. Took 0.56 sec\n",
      "Epoch 86, Loss(train/val) 4.91783/4.99706. Took 0.56 sec\n",
      "Epoch 87, Loss(train/val) 4.92063/5.00030. Took 0.65 sec\n",
      "Epoch 88, Loss(train/val) 4.91888/4.99330. Took 0.58 sec\n",
      "Epoch 89, Loss(train/val) 4.91817/5.00005. Took 0.59 sec\n",
      "Epoch 90, Loss(train/val) 4.91973/5.00327. Took 0.61 sec\n",
      "Epoch 91, Loss(train/val) 4.91893/4.99580. Took 0.60 sec\n",
      "Epoch 92, Loss(train/val) 4.91877/5.00087. Took 0.61 sec\n",
      "Epoch 93, Loss(train/val) 4.92200/5.00021. Took 0.65 sec\n",
      "Epoch 94, Loss(train/val) 4.92036/4.99369. Took 0.60 sec\n",
      "Epoch 95, Loss(train/val) 4.91728/4.99451. Took 0.57 sec\n",
      "Epoch 96, Loss(train/val) 4.91990/4.99786. Took 0.65 sec\n",
      "Epoch 97, Loss(train/val) 4.91749/5.00005. Took 0.63 sec\n",
      "Epoch 98, Loss(train/val) 4.92076/5.00067. Took 0.62 sec\n",
      "Epoch 99, Loss(train/val) 4.91877/5.00024. Took 0.62 sec\n",
      "ACC: 0.515625, MCC: 0.025861699363244256\n",
      "Epoch 0, Loss(train/val) 5.00577/5.00413. Took 0.62 sec\n",
      "Epoch 1, Loss(train/val) 5.00487/4.99819. Took 0.71 sec\n",
      "Epoch 2, Loss(train/val) 5.00319/4.99835. Took 0.87 sec\n",
      "Epoch 3, Loss(train/val) 5.00164/4.99632. Took 1.05 sec\n",
      "Epoch 4, Loss(train/val) 5.00139/4.99614. Took 0.71 sec\n",
      "Epoch 5, Loss(train/val) 5.00343/4.99703. Took 0.59 sec\n",
      "Epoch 6, Loss(train/val) 5.00238/4.99680. Took 0.57 sec\n",
      "Epoch 7, Loss(train/val) 5.00196/4.99489. Took 0.56 sec\n",
      "Epoch 8, Loss(train/val) 5.00232/4.99374. Took 0.54 sec\n",
      "Epoch 9, Loss(train/val) 5.00171/4.99566. Took 0.55 sec\n",
      "Epoch 10, Loss(train/val) 5.00211/4.99375. Took 0.64 sec\n",
      "Epoch 11, Loss(train/val) 5.00143/4.99287. Took 0.90 sec\n",
      "Epoch 12, Loss(train/val) 5.00142/4.99470. Took 0.62 sec\n",
      "Epoch 13, Loss(train/val) 5.00165/4.99043. Took 0.69 sec\n",
      "Epoch 14, Loss(train/val) 5.00053/4.98849. Took 0.65 sec\n",
      "Epoch 15, Loss(train/val) 5.00107/4.98791. Took 0.66 sec\n",
      "Epoch 16, Loss(train/val) 5.00115/4.98921. Took 0.64 sec\n",
      "Epoch 17, Loss(train/val) 4.99882/4.98561. Took 0.57 sec\n",
      "Epoch 18, Loss(train/val) 4.99883/4.98134. Took 0.60 sec\n",
      "Epoch 19, Loss(train/val) 4.99865/4.97963. Took 0.64 sec\n",
      "Epoch 20, Loss(train/val) 4.99788/4.98224. Took 0.73 sec\n",
      "Epoch 21, Loss(train/val) 4.99809/4.97629. Took 0.73 sec\n",
      "Epoch 22, Loss(train/val) 4.99685/4.97295. Took 0.75 sec\n",
      "Epoch 23, Loss(train/val) 4.99554/4.97627. Took 0.71 sec\n",
      "Epoch 24, Loss(train/val) 4.99871/4.98208. Took 0.63 sec\n",
      "Epoch 25, Loss(train/val) 4.99904/4.97762. Took 0.61 sec\n",
      "Epoch 26, Loss(train/val) 4.99731/4.97513. Took 0.78 sec\n",
      "Epoch 27, Loss(train/val) 4.99664/4.98012. Took 0.77 sec\n",
      "Epoch 28, Loss(train/val) 4.99706/4.98093. Took 0.69 sec\n",
      "Epoch 29, Loss(train/val) 4.99533/4.98216. Took 0.74 sec\n",
      "Epoch 30, Loss(train/val) 4.99446/4.98268. Took 0.70 sec\n",
      "Epoch 31, Loss(train/val) 4.99498/4.97617. Took 0.71 sec\n",
      "Epoch 32, Loss(train/val) 4.99242/4.97366. Took 0.71 sec\n",
      "Epoch 33, Loss(train/val) 4.99405/4.97856. Took 0.69 sec\n",
      "Epoch 34, Loss(train/val) 4.99338/4.97917. Took 0.75 sec\n",
      "Epoch 35, Loss(train/val) 4.99400/4.98511. Took 0.61 sec\n",
      "Epoch 36, Loss(train/val) 4.99478/4.98144. Took 0.66 sec\n",
      "Epoch 37, Loss(train/val) 4.99242/4.98219. Took 0.64 sec\n",
      "Epoch 38, Loss(train/val) 4.99390/4.97558. Took 0.59 sec\n",
      "Epoch 39, Loss(train/val) 4.99362/4.98268. Took 0.66 sec\n",
      "Epoch 40, Loss(train/val) 4.99353/4.98198. Took 0.77 sec\n",
      "Epoch 41, Loss(train/val) 4.99344/4.97938. Took 0.77 sec\n",
      "Epoch 42, Loss(train/val) 4.99345/4.98062. Took 0.73 sec\n",
      "Epoch 43, Loss(train/val) 4.99172/4.97538. Took 0.68 sec\n",
      "Epoch 44, Loss(train/val) 4.99175/4.98190. Took 0.66 sec\n",
      "Epoch 45, Loss(train/val) 4.99419/4.98433. Took 0.64 sec\n",
      "Epoch 46, Loss(train/val) 4.99233/4.98669. Took 0.57 sec\n",
      "Epoch 47, Loss(train/val) 4.99042/4.98788. Took 0.70 sec\n",
      "Epoch 48, Loss(train/val) 4.99210/4.98183. Took 0.75 sec\n",
      "Epoch 49, Loss(train/val) 4.99273/4.98294. Took 0.63 sec\n",
      "Epoch 50, Loss(train/val) 4.99220/4.98292. Took 0.68 sec\n",
      "Epoch 51, Loss(train/val) 4.98949/4.98283. Took 0.69 sec\n",
      "Epoch 52, Loss(train/val) 4.99093/4.98607. Took 0.65 sec\n",
      "Epoch 53, Loss(train/val) 4.99151/4.97977. Took 0.62 sec\n",
      "Epoch 54, Loss(train/val) 4.99135/4.98160. Took 0.58 sec\n",
      "Epoch 55, Loss(train/val) 4.98954/4.99076. Took 0.70 sec\n",
      "Epoch 56, Loss(train/val) 4.98935/4.98904. Took 0.65 sec\n",
      "Epoch 57, Loss(train/val) 4.98785/4.98590. Took 0.63 sec\n",
      "Epoch 58, Loss(train/val) 4.99086/4.98879. Took 0.69 sec\n",
      "Epoch 59, Loss(train/val) 4.99122/4.98812. Took 0.62 sec\n",
      "Epoch 60, Loss(train/val) 4.98917/4.98872. Took 0.68 sec\n",
      "Epoch 61, Loss(train/val) 4.98898/4.98099. Took 0.68 sec\n",
      "Epoch 62, Loss(train/val) 4.99109/4.98448. Took 0.78 sec\n",
      "Epoch 63, Loss(train/val) 4.99053/4.98381. Took 0.76 sec\n",
      "Epoch 64, Loss(train/val) 4.98932/4.98478. Took 0.70 sec\n",
      "Epoch 65, Loss(train/val) 4.98815/4.99156. Took 0.72 sec\n",
      "Epoch 66, Loss(train/val) 4.98915/4.98916. Took 0.64 sec\n",
      "Epoch 67, Loss(train/val) 4.98802/4.99055. Took 0.64 sec\n",
      "Epoch 68, Loss(train/val) 4.98868/4.98967. Took 0.58 sec\n",
      "Epoch 69, Loss(train/val) 4.98807/4.98382. Took 0.76 sec\n",
      "Epoch 70, Loss(train/val) 4.98692/4.98889. Took 0.73 sec\n",
      "Epoch 71, Loss(train/val) 4.98773/4.98020. Took 0.66 sec\n",
      "Epoch 72, Loss(train/val) 4.98684/4.98836. Took 0.70 sec\n",
      "Epoch 73, Loss(train/val) 4.98617/4.98762. Took 0.65 sec\n",
      "Epoch 74, Loss(train/val) 4.98684/4.97569. Took 0.60 sec\n",
      "Epoch 75, Loss(train/val) 4.98935/4.99411. Took 0.57 sec\n",
      "Epoch 76, Loss(train/val) 4.98694/4.98933. Took 0.57 sec\n",
      "Epoch 77, Loss(train/val) 4.98469/5.00144. Took 0.58 sec\n",
      "Epoch 78, Loss(train/val) 4.98672/4.99181. Took 0.59 sec\n",
      "Epoch 79, Loss(train/val) 4.98456/4.98371. Took 0.55 sec\n",
      "Epoch 80, Loss(train/val) 4.98516/4.99220. Took 0.57 sec\n",
      "Epoch 81, Loss(train/val) 4.98652/4.99019. Took 0.61 sec\n",
      "Epoch 82, Loss(train/val) 4.98496/4.98684. Took 0.53 sec\n",
      "Epoch 83, Loss(train/val) 4.98726/4.98422. Took 0.56 sec\n",
      "Epoch 84, Loss(train/val) 4.98336/4.99070. Took 0.57 sec\n",
      "Epoch 85, Loss(train/val) 4.98585/4.98911. Took 0.63 sec\n",
      "Epoch 86, Loss(train/val) 4.98580/4.98073. Took 0.54 sec\n",
      "Epoch 87, Loss(train/val) 4.98604/4.98820. Took 0.58 sec\n",
      "Epoch 88, Loss(train/val) 4.98403/4.99498. Took 0.59 sec\n",
      "Epoch 89, Loss(train/val) 4.98356/4.98502. Took 0.63 sec\n",
      "Epoch 90, Loss(train/val) 4.98390/4.98195. Took 0.68 sec\n",
      "Epoch 91, Loss(train/val) 4.98408/4.98407. Took 0.92 sec\n",
      "Epoch 92, Loss(train/val) 4.98457/4.98669. Took 0.82 sec\n",
      "Epoch 93, Loss(train/val) 4.98452/4.98795. Took 0.77 sec\n",
      "Epoch 94, Loss(train/val) 4.98485/4.98362. Took 0.70 sec\n",
      "Epoch 95, Loss(train/val) 4.98024/4.98054. Took 0.67 sec\n",
      "Epoch 96, Loss(train/val) 4.98253/4.98516. Took 0.71 sec\n",
      "Epoch 97, Loss(train/val) 4.98229/4.99048. Took 0.81 sec\n",
      "Epoch 98, Loss(train/val) 4.98316/4.98392. Took 0.86 sec\n",
      "Epoch 99, Loss(train/val) 4.98076/4.98488. Took 0.67 sec\n",
      "ACC: 0.5, MCC: 0.03253000243161777\n",
      "Epoch 0, Loss(train/val) 4.98039/4.98790. Took 0.64 sec\n",
      "Epoch 1, Loss(train/val) 4.97518/4.98511. Took 0.78 sec\n",
      "Epoch 2, Loss(train/val) 4.97499/4.98629. Took 0.82 sec\n",
      "Epoch 3, Loss(train/val) 4.97511/4.98542. Took 0.88 sec\n",
      "Epoch 4, Loss(train/val) 4.97468/4.98416. Took 0.83 sec\n",
      "Epoch 5, Loss(train/val) 4.97249/4.98677. Took 0.70 sec\n",
      "Epoch 6, Loss(train/val) 4.97290/4.98647. Took 0.76 sec\n",
      "Epoch 7, Loss(train/val) 4.97215/4.98588. Took 0.65 sec\n",
      "Epoch 8, Loss(train/val) 4.97269/4.98705. Took 0.64 sec\n",
      "Epoch 9, Loss(train/val) 4.97396/4.98582. Took 0.78 sec\n",
      "Epoch 10, Loss(train/val) 4.97323/4.98410. Took 0.68 sec\n",
      "Epoch 11, Loss(train/val) 4.97114/4.98715. Took 0.63 sec\n",
      "Epoch 12, Loss(train/val) 4.97231/4.98509. Took 0.68 sec\n",
      "Epoch 13, Loss(train/val) 4.97308/4.98635. Took 0.90 sec\n",
      "Epoch 14, Loss(train/val) 4.97388/4.98605. Took 0.84 sec\n",
      "Epoch 15, Loss(train/val) 4.97348/4.98604. Took 0.96 sec\n",
      "Epoch 16, Loss(train/val) 4.97289/4.98538. Took 1.18 sec\n",
      "Epoch 17, Loss(train/val) 4.97201/4.98701. Took 0.68 sec\n",
      "Epoch 18, Loss(train/val) 4.97225/4.98557. Took 0.57 sec\n",
      "Epoch 19, Loss(train/val) 4.97258/4.98577. Took 0.61 sec\n",
      "Epoch 20, Loss(train/val) 4.97150/4.98722. Took 0.57 sec\n",
      "Epoch 21, Loss(train/val) 4.97202/4.98709. Took 0.60 sec\n",
      "Epoch 22, Loss(train/val) 4.97092/4.98674. Took 0.60 sec\n",
      "Epoch 23, Loss(train/val) 4.97182/4.98773. Took 0.67 sec\n",
      "Epoch 24, Loss(train/val) 4.97038/4.98570. Took 0.59 sec\n",
      "Epoch 25, Loss(train/val) 4.97151/4.98478. Took 0.60 sec\n",
      "Epoch 26, Loss(train/val) 4.97141/4.98560. Took 0.57 sec\n",
      "Epoch 27, Loss(train/val) 4.97127/4.98638. Took 0.54 sec\n",
      "Epoch 28, Loss(train/val) 4.97066/4.98720. Took 0.67 sec\n",
      "Epoch 29, Loss(train/val) 4.96965/4.98612. Took 0.61 sec\n",
      "Epoch 30, Loss(train/val) 4.97011/4.98711. Took 0.61 sec\n",
      "Epoch 31, Loss(train/val) 4.97163/4.98984. Took 0.61 sec\n",
      "Epoch 32, Loss(train/val) 4.97064/4.99073. Took 0.63 sec\n",
      "Epoch 33, Loss(train/val) 4.96849/4.99120. Took 0.69 sec\n",
      "Epoch 34, Loss(train/val) 4.96855/4.99241. Took 0.84 sec\n",
      "Epoch 35, Loss(train/val) 4.96946/4.99127. Took 0.86 sec\n",
      "Epoch 36, Loss(train/val) 4.97325/4.98801. Took 0.82 sec\n",
      "Epoch 37, Loss(train/val) 4.97105/4.99043. Took 0.78 sec\n",
      "Epoch 38, Loss(train/val) 4.96914/4.98992. Took 0.66 sec\n",
      "Epoch 39, Loss(train/val) 4.96833/4.98773. Took 0.67 sec\n",
      "Epoch 40, Loss(train/val) 4.97006/4.99129. Took 0.69 sec\n",
      "Epoch 41, Loss(train/val) 4.96927/4.98909. Took 0.66 sec\n",
      "Epoch 42, Loss(train/val) 4.96974/4.99202. Took 0.72 sec\n",
      "Epoch 43, Loss(train/val) 4.96898/4.99283. Took 0.67 sec\n",
      "Epoch 44, Loss(train/val) 4.97005/4.98965. Took 0.69 sec\n",
      "Epoch 45, Loss(train/val) 4.96919/4.99069. Took 0.68 sec\n",
      "Epoch 46, Loss(train/val) 4.97054/4.98566. Took 0.73 sec\n",
      "Epoch 47, Loss(train/val) 4.96981/4.98960. Took 0.70 sec\n",
      "Epoch 48, Loss(train/val) 4.96932/4.99045. Took 0.70 sec\n",
      "Epoch 49, Loss(train/val) 4.96986/4.98937. Took 0.80 sec\n",
      "Epoch 50, Loss(train/val) 4.96605/4.98687. Took 0.82 sec\n",
      "Epoch 51, Loss(train/val) 4.96770/4.98700. Took 0.79 sec\n",
      "Epoch 52, Loss(train/val) 4.96898/4.98737. Took 0.83 sec\n",
      "Epoch 53, Loss(train/val) 4.96881/4.98916. Took 0.72 sec\n",
      "Epoch 54, Loss(train/val) 4.96530/4.98529. Took 0.76 sec\n",
      "Epoch 55, Loss(train/val) 4.96659/4.98940. Took 0.62 sec\n",
      "Epoch 56, Loss(train/val) 4.96400/4.98291. Took 0.75 sec\n",
      "Epoch 57, Loss(train/val) 4.96439/4.99480. Took 0.70 sec\n",
      "Epoch 58, Loss(train/val) 4.96737/4.98845. Took 0.69 sec\n",
      "Epoch 59, Loss(train/val) 4.96511/4.98755. Took 0.74 sec\n",
      "Epoch 60, Loss(train/val) 4.96643/4.98813. Took 0.63 sec\n",
      "Epoch 61, Loss(train/val) 4.96620/4.98192. Took 0.62 sec\n",
      "Epoch 62, Loss(train/val) 4.96563/4.98987. Took 0.70 sec\n",
      "Epoch 63, Loss(train/val) 4.96430/4.98343. Took 0.66 sec\n",
      "Epoch 64, Loss(train/val) 4.96479/4.98008. Took 0.62 sec\n",
      "Epoch 65, Loss(train/val) 4.96626/4.98056. Took 0.71 sec\n",
      "Epoch 66, Loss(train/val) 4.96432/4.99548. Took 0.69 sec\n",
      "Epoch 67, Loss(train/val) 4.96235/4.98757. Took 0.67 sec\n",
      "Epoch 68, Loss(train/val) 4.96201/4.98742. Took 0.75 sec\n",
      "Epoch 69, Loss(train/val) 4.96418/4.98512. Took 0.64 sec\n",
      "Epoch 70, Loss(train/val) 4.96428/4.98764. Took 0.61 sec\n",
      "Epoch 71, Loss(train/val) 4.96053/4.99135. Took 0.72 sec\n",
      "Epoch 72, Loss(train/val) 4.95977/4.99583. Took 0.85 sec\n",
      "Epoch 73, Loss(train/val) 4.96078/4.98851. Took 0.70 sec\n",
      "Epoch 74, Loss(train/val) 4.96310/4.97962. Took 0.72 sec\n",
      "Epoch 75, Loss(train/val) 4.96051/4.98385. Took 0.69 sec\n",
      "Epoch 76, Loss(train/val) 4.95697/4.98888. Took 0.68 sec\n",
      "Epoch 77, Loss(train/val) 4.95950/4.99195. Took 0.64 sec\n",
      "Epoch 78, Loss(train/val) 4.95954/4.98212. Took 0.72 sec\n",
      "Epoch 79, Loss(train/val) 4.96003/4.98563. Took 0.65 sec\n",
      "Epoch 80, Loss(train/val) 4.95809/4.97786. Took 0.65 sec\n",
      "Epoch 81, Loss(train/val) 4.95504/4.98270. Took 0.67 sec\n",
      "Epoch 82, Loss(train/val) 4.95732/4.99012. Took 0.67 sec\n",
      "Epoch 83, Loss(train/val) 4.95633/4.98596. Took 0.64 sec\n",
      "Epoch 84, Loss(train/val) 4.95647/4.98374. Took 0.68 sec\n",
      "Epoch 85, Loss(train/val) 4.95471/4.98706. Took 0.79 sec\n",
      "Epoch 86, Loss(train/val) 4.95420/5.00306. Took 0.84 sec\n",
      "Epoch 87, Loss(train/val) 4.95509/4.98395. Took 0.74 sec\n",
      "Epoch 88, Loss(train/val) 4.95406/4.97669. Took 0.71 sec\n",
      "Epoch 89, Loss(train/val) 4.95337/4.98170. Took 0.75 sec\n",
      "Epoch 90, Loss(train/val) 4.95319/4.98035. Took 0.76 sec\n",
      "Epoch 91, Loss(train/val) 4.95415/4.98098. Took 0.74 sec\n",
      "Epoch 92, Loss(train/val) 4.95213/4.97364. Took 0.75 sec\n",
      "Epoch 93, Loss(train/val) 4.94933/4.98699. Took 0.81 sec\n",
      "Epoch 94, Loss(train/val) 4.94875/4.98163. Took 0.71 sec\n",
      "Epoch 95, Loss(train/val) 4.95199/4.97985. Took 0.64 sec\n",
      "Epoch 96, Loss(train/val) 4.95183/4.98125. Took 0.65 sec\n",
      "Epoch 97, Loss(train/val) 4.95182/4.97065. Took 0.61 sec\n",
      "Epoch 98, Loss(train/val) 4.94973/4.97498. Took 0.63 sec\n",
      "Epoch 99, Loss(train/val) 4.95482/4.97951. Took 0.70 sec\n",
      "ACC: 0.40625, MCC: -0.17930563858025494\n",
      "Epoch 0, Loss(train/val) 4.95447/4.95366. Took 0.73 sec\n",
      "Epoch 1, Loss(train/val) 4.95138/4.95115. Took 1.03 sec\n",
      "Epoch 2, Loss(train/val) 4.94949/4.95098. Took 0.89 sec\n",
      "Epoch 3, Loss(train/val) 4.95095/4.94941. Took 1.17 sec\n",
      "Epoch 4, Loss(train/val) 4.95081/4.94812. Took 0.77 sec\n",
      "Epoch 5, Loss(train/val) 4.94951/4.94765. Took 0.78 sec\n",
      "Epoch 6, Loss(train/val) 4.95000/4.94607. Took 0.74 sec\n",
      "Epoch 7, Loss(train/val) 4.94920/4.94528. Took 0.75 sec\n",
      "Epoch 8, Loss(train/val) 4.94845/4.94509. Took 0.64 sec\n",
      "Epoch 9, Loss(train/val) 4.94871/4.94625. Took 0.70 sec\n",
      "Epoch 10, Loss(train/val) 4.94561/4.94449. Took 0.65 sec\n",
      "Epoch 11, Loss(train/val) 4.94940/4.94453. Took 0.71 sec\n",
      "Epoch 12, Loss(train/val) 4.94793/4.94448. Took 0.65 sec\n",
      "Epoch 13, Loss(train/val) 4.94903/4.94535. Took 0.68 sec\n",
      "Epoch 14, Loss(train/val) 4.94888/4.94675. Took 0.65 sec\n",
      "Epoch 15, Loss(train/val) 4.94713/4.94650. Took 0.69 sec\n",
      "Epoch 16, Loss(train/val) 4.94618/4.94732. Took 0.68 sec\n",
      "Epoch 17, Loss(train/val) 4.94637/4.94751. Took 0.68 sec\n",
      "Epoch 18, Loss(train/val) 4.94630/4.94760. Took 0.68 sec\n",
      "Epoch 19, Loss(train/val) 4.94413/4.95162. Took 0.70 sec\n",
      "Epoch 20, Loss(train/val) 4.94233/4.95180. Took 0.68 sec\n",
      "Epoch 21, Loss(train/val) 4.94470/4.95440. Took 0.65 sec\n",
      "Epoch 22, Loss(train/val) 4.94590/4.95258. Took 0.72 sec\n",
      "Epoch 23, Loss(train/val) 4.94325/4.95509. Took 0.68 sec\n",
      "Epoch 24, Loss(train/val) 4.94322/4.95750. Took 0.81 sec\n",
      "Epoch 25, Loss(train/val) 4.94303/4.95675. Took 0.68 sec\n",
      "Epoch 26, Loss(train/val) 4.94273/4.95898. Took 0.74 sec\n",
      "Epoch 27, Loss(train/val) 4.94104/4.96730. Took 0.77 sec\n",
      "Epoch 28, Loss(train/val) 4.94154/4.96734. Took 0.88 sec\n",
      "Epoch 29, Loss(train/val) 4.93883/4.96846. Took 0.66 sec\n",
      "Epoch 30, Loss(train/val) 4.93949/4.96825. Took 0.76 sec\n",
      "Epoch 31, Loss(train/val) 4.94114/4.97105. Took 0.61 sec\n",
      "Epoch 32, Loss(train/val) 4.93783/4.97030. Took 0.63 sec\n",
      "Epoch 33, Loss(train/val) 4.93942/4.97195. Took 0.67 sec\n",
      "Epoch 34, Loss(train/val) 4.94050/4.97416. Took 0.64 sec\n",
      "Epoch 35, Loss(train/val) 4.93808/4.97600. Took 0.73 sec\n",
      "Epoch 36, Loss(train/val) 4.93579/4.97439. Took 0.67 sec\n",
      "Epoch 37, Loss(train/val) 4.93688/4.97570. Took 0.60 sec\n",
      "Epoch 38, Loss(train/val) 4.93825/4.98481. Took 0.59 sec\n",
      "Epoch 39, Loss(train/val) 4.93815/4.97645. Took 0.62 sec\n",
      "Epoch 40, Loss(train/val) 4.93840/4.97461. Took 0.59 sec\n",
      "Epoch 41, Loss(train/val) 4.93630/4.97494. Took 0.55 sec\n",
      "Epoch 42, Loss(train/val) 4.93411/4.97928. Took 0.56 sec\n",
      "Epoch 43, Loss(train/val) 4.93093/4.98095. Took 0.56 sec\n",
      "Epoch 44, Loss(train/val) 4.93070/4.98806. Took 0.60 sec\n",
      "Epoch 45, Loss(train/val) 4.93695/4.98733. Took 0.65 sec\n",
      "Epoch 46, Loss(train/val) 4.93528/4.97615. Took 0.76 sec\n",
      "Epoch 47, Loss(train/val) 4.93690/4.97721. Took 0.62 sec\n",
      "Epoch 48, Loss(train/val) 4.93673/4.98136. Took 0.62 sec\n",
      "Epoch 49, Loss(train/val) 4.93157/4.97865. Took 0.79 sec\n",
      "Epoch 50, Loss(train/val) 4.93448/4.97521. Took 0.79 sec\n",
      "Epoch 51, Loss(train/val) 4.93610/4.98028. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 4.93244/4.98105. Took 0.96 sec\n",
      "Epoch 53, Loss(train/val) 4.93159/4.98372. Took 0.80 sec\n",
      "Epoch 54, Loss(train/val) 4.92730/4.98494. Took 0.69 sec\n",
      "Epoch 55, Loss(train/val) 4.93108/4.98249. Took 0.67 sec\n",
      "Epoch 56, Loss(train/val) 4.93415/4.97706. Took 0.57 sec\n",
      "Epoch 57, Loss(train/val) 4.93356/4.98225. Took 0.55 sec\n",
      "Epoch 58, Loss(train/val) 4.93110/4.98003. Took 0.55 sec\n",
      "Epoch 59, Loss(train/val) 4.92584/4.97800. Took 0.53 sec\n",
      "Epoch 60, Loss(train/val) 4.92907/4.98042. Took 0.55 sec\n",
      "Epoch 61, Loss(train/val) 4.92815/4.98253. Took 0.58 sec\n",
      "Epoch 62, Loss(train/val) 4.93100/4.98364. Took 0.55 sec\n",
      "Epoch 63, Loss(train/val) 4.92581/4.98149. Took 0.55 sec\n",
      "Epoch 64, Loss(train/val) 4.92956/4.97814. Took 0.62 sec\n",
      "Epoch 65, Loss(train/val) 4.93140/4.98000. Took 0.57 sec\n",
      "Epoch 66, Loss(train/val) 4.92918/4.97662. Took 0.51 sec\n",
      "Epoch 67, Loss(train/val) 4.92463/4.97864. Took 0.54 sec\n",
      "Epoch 68, Loss(train/val) 4.92850/4.97293. Took 0.53 sec\n",
      "Epoch 69, Loss(train/val) 4.92055/4.97671. Took 0.52 sec\n",
      "Epoch 70, Loss(train/val) 4.93259/4.97821. Took 0.57 sec\n",
      "Epoch 71, Loss(train/val) 4.92477/4.97243. Took 0.69 sec\n",
      "Epoch 72, Loss(train/val) 4.92002/4.97251. Took 0.63 sec\n",
      "Epoch 73, Loss(train/val) 4.92447/4.97329. Took 0.62 sec\n",
      "Epoch 74, Loss(train/val) 4.92778/4.98163. Took 0.58 sec\n",
      "Epoch 75, Loss(train/val) 4.92666/4.97221. Took 0.57 sec\n",
      "Epoch 76, Loss(train/val) 4.92724/4.97581. Took 0.56 sec\n",
      "Epoch 77, Loss(train/val) 4.92292/4.96954. Took 0.59 sec\n",
      "Epoch 78, Loss(train/val) 4.92563/4.96947. Took 0.57 sec\n",
      "Epoch 79, Loss(train/val) 4.92348/4.96721. Took 0.60 sec\n",
      "Epoch 80, Loss(train/val) 4.92478/4.97923. Took 0.54 sec\n",
      "Epoch 81, Loss(train/val) 4.92329/4.97762. Took 0.66 sec\n",
      "Epoch 82, Loss(train/val) 4.92705/4.97610. Took 0.59 sec\n",
      "Epoch 83, Loss(train/val) 4.92272/4.96996. Took 0.60 sec\n",
      "Epoch 84, Loss(train/val) 4.92242/4.97372. Took 0.58 sec\n",
      "Epoch 85, Loss(train/val) 4.92073/4.98198. Took 0.55 sec\n",
      "Epoch 86, Loss(train/val) 4.92509/4.97831. Took 0.54 sec\n",
      "Epoch 87, Loss(train/val) 4.92330/4.97081. Took 0.56 sec\n",
      "Epoch 88, Loss(train/val) 4.92600/4.97181. Took 0.56 sec\n",
      "Epoch 89, Loss(train/val) 4.92090/4.97637. Took 0.67 sec\n",
      "Epoch 90, Loss(train/val) 4.92007/4.98041. Took 0.65 sec\n",
      "Epoch 91, Loss(train/val) 4.92109/4.97373. Took 0.73 sec\n",
      "Epoch 92, Loss(train/val) 4.92096/4.97543. Took 0.66 sec\n",
      "Epoch 93, Loss(train/val) 4.92135/4.97502. Took 0.66 sec\n",
      "Epoch 94, Loss(train/val) 4.92402/4.97358. Took 0.63 sec\n",
      "Epoch 95, Loss(train/val) 4.92161/4.97411. Took 0.68 sec\n",
      "Epoch 96, Loss(train/val) 4.92098/4.97534. Took 0.68 sec\n",
      "Epoch 97, Loss(train/val) 4.91766/4.97308. Took 0.55 sec\n",
      "Epoch 98, Loss(train/val) 4.92225/4.97771. Took 0.65 sec\n",
      "Epoch 99, Loss(train/val) 4.92483/4.97459. Took 0.70 sec\n",
      "ACC: 0.59375, MCC: 0.159079640347213\n",
      "Epoch 0, Loss(train/val) 4.78998/4.78837. Took 0.78 sec\n",
      "Epoch 1, Loss(train/val) 4.78333/4.78653. Took 0.94 sec\n",
      "Epoch 2, Loss(train/val) 4.78401/4.78893. Took 0.85 sec\n",
      "Epoch 3, Loss(train/val) 4.78250/4.78850. Took 0.98 sec\n",
      "Epoch 4, Loss(train/val) 4.78280/4.78968. Took 0.80 sec\n",
      "Epoch 5, Loss(train/val) 4.78300/4.79027. Took 0.71 sec\n",
      "Epoch 6, Loss(train/val) 4.78207/4.79049. Took 0.70 sec\n",
      "Epoch 7, Loss(train/val) 4.78235/4.79150. Took 0.61 sec\n",
      "Epoch 8, Loss(train/val) 4.78319/4.79011. Took 0.65 sec\n",
      "Epoch 9, Loss(train/val) 4.78227/4.79117. Took 0.77 sec\n",
      "Epoch 10, Loss(train/val) 4.78233/4.79152. Took 0.80 sec\n",
      "Epoch 11, Loss(train/val) 4.78050/4.79123. Took 0.77 sec\n",
      "Epoch 12, Loss(train/val) 4.78135/4.79280. Took 0.78 sec\n",
      "Epoch 13, Loss(train/val) 4.78083/4.79281. Took 0.73 sec\n",
      "Epoch 14, Loss(train/val) 4.78087/4.79308. Took 0.68 sec\n",
      "Epoch 15, Loss(train/val) 4.78058/4.79444. Took 0.71 sec\n",
      "Epoch 16, Loss(train/val) 4.78211/4.79426. Took 0.71 sec\n",
      "Epoch 17, Loss(train/val) 4.77875/4.79508. Took 0.82 sec\n",
      "Epoch 18, Loss(train/val) 4.78022/4.79550. Took 0.64 sec\n",
      "Epoch 19, Loss(train/val) 4.77724/4.79776. Took 0.69 sec\n",
      "Epoch 20, Loss(train/val) 4.77852/4.79549. Took 0.67 sec\n",
      "Epoch 21, Loss(train/val) 4.77799/4.79708. Took 0.60 sec\n",
      "Epoch 22, Loss(train/val) 4.77691/4.79644. Took 0.60 sec\n",
      "Epoch 23, Loss(train/val) 4.77889/4.79709. Took 0.72 sec\n",
      "Epoch 24, Loss(train/val) 4.77488/4.79783. Took 0.69 sec\n",
      "Epoch 25, Loss(train/val) 4.77736/4.79446. Took 0.64 sec\n",
      "Epoch 26, Loss(train/val) 4.77363/4.79801. Took 0.58 sec\n",
      "Epoch 27, Loss(train/val) 4.77416/4.79533. Took 0.65 sec\n",
      "Epoch 28, Loss(train/val) 4.77157/4.79322. Took 0.55 sec\n",
      "Epoch 29, Loss(train/val) 4.77275/4.79167. Took 0.62 sec\n",
      "Epoch 30, Loss(train/val) 4.77214/4.79091. Took 0.63 sec\n",
      "Epoch 31, Loss(train/val) 4.76672/4.79486. Took 0.62 sec\n",
      "Epoch 32, Loss(train/val) 4.76964/4.79367. Took 0.60 sec\n",
      "Epoch 33, Loss(train/val) 4.77048/4.78908. Took 0.72 sec\n",
      "Epoch 34, Loss(train/val) 4.76908/4.78693. Took 0.64 sec\n"
     ]
    }
   ],
   "source": [
    "## 실행 파일\n",
    "args.data_list = os.listdir(\"C:\\\\taewon_project\\\\DTML_selfatt\\\\data\\\\kdd17\\\\ourpped\")\n",
    "\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'DTML_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\", \"avg_test_MCC\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        \n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        est = time.time()\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"DTML_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        \n",
    "        csv_read = stock_csv_read(data,args.x_frames,args.y_frames)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "        \n",
    "        ACC_cv = []\n",
    "        for i, data in enumerate(split_data_list):\n",
    "            args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "            os.makedirs(args.split_file_path)\n",
    "            # 0번째에 index 1번째에 stock 1개가 input으로 들어감\n",
    "            trainset = StockDataset(data[0])\n",
    "            valset = StockDataset(data[1])\n",
    "            testset = StockDataset(data[2])\n",
    "        \n",
    "\n",
    "            partition = {'train': trainset, 'val': valset, 'test': testset}\n",
    "\n",
    "\n",
    "            setting, result = experiment(partition, args)\n",
    "            eet = time.time()\n",
    "            entire_exp_time = eet - est\n",
    "\n",
    "            fig = plt.figure()\n",
    "            plt.plot(result['train_losses'])\n",
    "            plt.plot(result['val_losses'])\n",
    "            plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "            plt.xlabel('epoch', fontsize=15)\n",
    "            plt.ylabel('loss', fontsize=15)\n",
    "            plt.grid()\n",
    "            plt.savefig(args.split_file_path + '\\\\' + str(args.symbol) + '_fig' + '.png')\n",
    "            plt.close(fig)\n",
    "            ACC_cv.append(result['ACC'])\n",
    "            # csv파일에 기록하기\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"DTML\", args.symbol, entire_exp_time, acc_avg, acc_std, result['MCC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4bb3f-2033-4b97-8031-19370a89d75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a657e0acc2d88b9e680d20a67cd25584dc5d3bfc714ccbd8bd3cadf857b3f939"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tw_fin_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
